{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import nltk\n",
    "import pandas\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding, Dense\n",
    "from keras.layers import TimeDistributed, LSTM\n",
    "from keras.engine.input_layer import Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wget\n",
    "import zipfile\n",
    "import os, fnmatch\n",
    "import seaborn as sns\n",
    "import pickle as pkl\n",
    "import random\n",
    "import re\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract the dataset\n",
    "#def fetch_data(web_file, local_dir='.'):\n",
    "#    \"\"\"Download the `web_file`, assuming it is a web resource into the local_dir. \n",
    "#    If a file with the same filename already exists in the local directory, do not \n",
    "#    download it but return its path instead.\n",
    "#    Arguments:\n",
    "#        web_file: a web resource identifiable by a url (str)\n",
    "#        local_dir: a local directory to download the web_file into (str)\n",
    "#    Return: The local path to the file (str)\n",
    "#    \"\"\"\n",
    "#    file_name = local_dir + \"/\" + web_file.rsplit(\"/\",1)[-1]\n",
    "#    if os.path.exists(file_name):\n",
    "#        return file_name\n",
    "#    else:\n",
    "#        file_name = wget.download(web_file, out=local_dir)\n",
    "#        return file_name\n",
    "#data_filename = fetch_data('https://s3.amazonaws.com/pytorch-tutorial-assets/cornell_movie_dialogs_corpus.zip')\n",
    "#with zipfile.ZipFile(data_filename, 'r') as zip_ref:\n",
    "#    zip_ref.extractall('.\\data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the movie lines\n",
    "#movie_lines_features = [\"LineID\", \"Character\", \"Movie\", \"Name\", \"Line\"]\n",
    "#movie_lines = pd.read_csv('.\\\\data\\\\cornell movie-dialogs corpus\\\\movie_lines.txt', \n",
    "#                          engine = \"python\", \n",
    "#                          index_col = False,\n",
    "#                          sep=' \\+\\+\\+\\$\\+\\+\\+ ',\n",
    "#                          names = movie_lines_features)\n",
    "#\n",
    "## Using only the required columns, namely, \"LineID\" and \"Line\"\n",
    "#movie_lines = movie_lines[[\"LineID\", \"Line\"]]\n",
    "#\n",
    "## Strip the space from \"LineID\" for further usage and change the datatype of \"Line\"\n",
    "#movie_lines[\"LineID\"] = movie_lines[\"LineID\"].apply(str.strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movie_lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the conversations file\n",
    "#movie_conversations_features = [\"Character1\", \"Character2\", \"Movie\", \"Conversation\"]\n",
    "#movie_conversations = pd.read_csv('.\\\\data\\\\cornell movie-dialogs corpus\\\\movie_conversations.txt',\n",
    "#                                  sep = \"\\+\\+\\+\\$\\+\\+\\+\", \n",
    "#                                  engine = \"python\", \n",
    "#                                  index_col = False, \n",
    "#                                  names = movie_conversations_features)\n",
    "#\n",
    "# Again using the required feature, \"Conversation\"\n",
    "# movie_conversations = movie_conversations[\"Conversation\"]\n",
    "\n",
    "# Preprocessing and storing the conversation data. This takes too long to run, so we saved the result as a pickle\n",
    "# conversation = [[str(list(movie_lines.loc[movie_lines[\"LineID\"] == u.strip().strip(\"'\"), \"Line\"])[0]).strip() for u in c.strip().strip('[').strip(']').split(',')] for c in movie_conversations]\n",
    "\n",
    "#with open(\".\\\\data\\\\conversations.pkl\", \"wb\") as handle:\n",
    "    #pkl.dump(conversation, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"data\\conversations.pkl\", \"rb\") as handle:\n",
    "    conversation = pkl.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 30000\n",
    "\n",
    "indices = random.sample(range(len(conversation)), sample_size)\n",
    "\n",
    "conv_sample = []\n",
    "\n",
    "for i in indices:\n",
    "    conv = conversation[i]\n",
    "    conv_sample.append(conv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list of conversations, ready to use. These are the tasks we need to do: \n",
    "* Create pairs of questions and answers\n",
    "* Text Cleaning\n",
    "* Remove too large and too small utterances\n",
    "* Put \\<*BOS*> tag and \\<*EOS*> tag for decoder input\n",
    "* Make Vocabulary (VOCAB_SIZE)\n",
    "* Tokenize Bag of words to Bag of IDs\n",
    "* Padding (MAX_LEN)\n",
    "* Word Embedding (EMBEDDING_DIM)\n",
    "* Reshape the Data depends on neural network shape\n",
    "* Split Data for training and validation, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "min_line_length = 1\n",
    "max_line_length = 18\n",
    "VOCAB_SIZE= 10000\n",
    "HIDDEN_DIM=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the sentences into questions (inputs) and answers (targets)\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for conv in conv_sample:\n",
    "    for i in range(len(conv)-1):\n",
    "        questions.append(conv[i])\n",
    "        answers.append(conv[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, it's me, Mrs. Brenner. I thought you might like some tea.\n",
      "Oh, thank you.\n",
      "\n",
      "I don't want the fuckin' money! I'm not gonna give up everything I got for a lousy 50,000 dollars. It's <u>you</u>. You <u>Grace</u> or nothing. The whole thing... I want you to be my wife.... What do you way Grace?\n",
      "You sound just like Jake... I did see into the future, Virgil, but you weren't in it. Go back to your family. They love you.\n",
      "\n",
      "Talcott doesn't usually show up at the office 'till after his 18 holes. What are they nervous about?\n",
      "They're executives.  They're nervous about everything.\n",
      "\n",
      "Hey, how ya doin', cutie?\n",
      "Okay. How you doing?\n",
      "\n",
      "Okay. How you doing?\n",
      "Just great!  See ya around!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if we have loaded the data correctly\n",
    "limit = 0\n",
    "for i in range(limit, limit+5):\n",
    "    print(questions[i])\n",
    "    print(answers[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80636\n",
      "80636\n"
     ]
    }
   ],
   "source": [
    "# Compare lengths of questions and answers\n",
    "print(len(questions))\n",
    "print(len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    text = re.sub(r\"  \", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "    \n",
    "clean_answers = []    \n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no it is me mrs brenner i thought you might like some tea\n",
      "oh thank you\n",
      "\n",
      "i do not want the fucking money i am not gonna give up everything i got for a lousy 50000 dollars it is uyouu you ugraceu or nothing the whole thing i want you to be my wife what do you way grace\n",
      "you sound just like jake i did see into the future virgil but you were not in it go back to your family they love you\n",
      "\n",
      "talcott does not usually show up at the office untill after his 18 holes what are they nervous about\n",
      "they are executives they are nervous about everything\n",
      "\n",
      "hey how ya doing cutie\n",
      "okay how you doing\n",
      "\n",
      "okay how you doing\n",
      "just great see ya around\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at some of the data to ensure that it has been cleaned well.\n",
    "limit = 0\n",
    "for i in range(limit, limit+5):\n",
    "    print(clean_questions[i])\n",
    "    print(clean_answers[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the length of sentences\n",
    "lengths = []\n",
    "for question in clean_questions:\n",
    "    lengths.append(len(question.split()))\n",
    "for answer in clean_answers:\n",
    "    lengths.append(len(answer.split()))\n",
    "\n",
    "# Create a dataframe so that the values can be inspected\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>161272.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.884797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.285506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>555.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              counts\n",
       "count  161272.000000\n",
       "mean       10.884797\n",
       "std        12.285506\n",
       "min         0.000000\n",
       "25%         4.000000\n",
       "50%         7.000000\n",
       "75%        14.000000\n",
       "max       555.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove questions and answers that are shorter than <min_line_length> words and longer than <max_line_length> words.\n",
    "\n",
    "# Filter out the questions that are too short/long\n",
    "short_questions_temp = []\n",
    "short_answers_temp = []\n",
    "\n",
    "i = 0\n",
    "for question in clean_questions:\n",
    "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
    "        short_questions_temp.append(question)\n",
    "        short_answers_temp.append(clean_answers[i])\n",
    "    i += 1\n",
    "\n",
    "# Filter out the answers that are too short/long\n",
    "short_questions = []\n",
    "short_answers = []\n",
    "\n",
    "i = 0\n",
    "for answer in short_answers_temp:\n",
    "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
    "        short_answers.append(answer)\n",
    "        short_questions.append(short_questions_temp[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of questions: 47891\n",
      "# of answers: 47891\n",
      "% of data used: 59.39%\n"
     ]
    }
   ],
   "source": [
    "# Compare the number of lines we will use with the total number of lines.\n",
    "print(\"# of questions:\", len(short_questions))\n",
    "print(\"# of answers:\", len(short_answers))\n",
    "print(\"% of data used: {}%\".format(round(len(short_questions)/len(questions),4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put <BOS> and <EOS> tags on what will be the decoder input\n",
    "\n",
    "def tagger(decoder_input_sentence):\n",
    "    bos = \"<BOS> \"\n",
    "    eos = \" <EOS>\"\n",
    "    final_target = [bos + text + eos for text in decoder_input_sentence] \n",
    "    return final_target\n",
    "\n",
    "tagged_answers = tagger(short_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make vocabulary of arbitrary size\n",
    "\n",
    "def vocab_creator(text_lists, VOCAB_SIZE):\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "    tokenizer.fit_on_texts(text_lists)\n",
    "    dictionary = tokenizer.word_index\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for k, v in dictionary.items():\n",
    "        if v < VOCAB_SIZE:\n",
    "            word2idx[k] = v\n",
    "            idx2word[v] = k\n",
    "        if v >= VOCAB_SIZE-1:\n",
    "            continue\n",
    "          \n",
    "    return word2idx, idx2word\n",
    "\n",
    "\n",
    "word2idx, idx2word = vocab_creator(text_lists=short_questions+tagged_answers, VOCAB_SIZE=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos': 1,\n",
       " 'eos': 2,\n",
       " 'you': 3,\n",
       " 'i': 4,\n",
       " 'is': 5,\n",
       " 'the': 6,\n",
       " 'not': 7,\n",
       " 'it': 8,\n",
       " 'to': 9,\n",
       " 'a': 10,\n",
       " 'do': 11,\n",
       " 'that': 12,\n",
       " 'what': 13,\n",
       " 'are': 14,\n",
       " 'me': 15,\n",
       " 'have': 16,\n",
       " 'am': 17,\n",
       " 'we': 18,\n",
       " 'no': 19,\n",
       " 'of': 20,\n",
       " 'he': 21,\n",
       " 'in': 22,\n",
       " 'and': 23,\n",
       " 'will': 24,\n",
       " 'know': 25,\n",
       " 'this': 26,\n",
       " 'your': 27,\n",
       " 'for': 28,\n",
       " 'my': 29,\n",
       " 'was': 30,\n",
       " 'did': 31,\n",
       " 'on': 32,\n",
       " 'be': 33,\n",
       " 'just': 34,\n",
       " 'about': 35,\n",
       " 'how': 36,\n",
       " 'like': 37,\n",
       " 'would': 38,\n",
       " 'yes': 39,\n",
       " 'here': 40,\n",
       " 'they': 41,\n",
       " 'with': 42,\n",
       " 'get': 43,\n",
       " 'so': 44,\n",
       " 'she': 45,\n",
       " 'all': 46,\n",
       " 'but': 47,\n",
       " 'yeah': 48,\n",
       " 'why': 49,\n",
       " 'oh': 50,\n",
       " 'want': 51,\n",
       " 'right': 52,\n",
       " 'got': 53,\n",
       " 'him': 54,\n",
       " 'think': 55,\n",
       " 'well': 56,\n",
       " 'can': 57,\n",
       " 'go': 58,\n",
       " 'where': 59,\n",
       " 'out': 60,\n",
       " 'there': 61,\n",
       " 'up': 62,\n",
       " 'going': 63,\n",
       " 'good': 64,\n",
       " 'at': 65,\n",
       " 'one': 66,\n",
       " 'now': 67,\n",
       " 'see': 68,\n",
       " 'her': 69,\n",
       " 'if': 70,\n",
       " 'who': 71,\n",
       " 'come': 72,\n",
       " 'cannot': 73,\n",
       " 'say': 74,\n",
       " 'tell': 75,\n",
       " 'okay': 76,\n",
       " 'were': 77,\n",
       " 'could': 78,\n",
       " 'sure': 79,\n",
       " 'then': 80,\n",
       " 'from': 81,\n",
       " 'look': 82,\n",
       " 'take': 83,\n",
       " 'something': 84,\n",
       " 'mean': 85,\n",
       " 'time': 86,\n",
       " 'an': 87,\n",
       " 'been': 88,\n",
       " 'does': 89,\n",
       " 'too': 90,\n",
       " 'back': 91,\n",
       " 'never': 92,\n",
       " 'some': 93,\n",
       " 'us': 94,\n",
       " 'when': 95,\n",
       " 'really': 96,\n",
       " 'them': 97,\n",
       " 'man': 98,\n",
       " 'his': 99,\n",
       " 'or': 100,\n",
       " 'should': 101,\n",
       " 'mr': 102,\n",
       " 'doing': 103,\n",
       " 'as': 104,\n",
       " 'way': 105,\n",
       " 'down': 106,\n",
       " 'maybe': 107,\n",
       " 'sorry': 108,\n",
       " 'need': 109,\n",
       " 'said': 110,\n",
       " 'sir': 111,\n",
       " 'much': 112,\n",
       " 'had': 113,\n",
       " 'little': 114,\n",
       " 'nothing': 115,\n",
       " 'more': 116,\n",
       " 'any': 117,\n",
       " 'very': 118,\n",
       " 'make': 119,\n",
       " 'anything': 120,\n",
       " \"there's\": 121,\n",
       " 'thought': 122,\n",
       " 'hey': 123,\n",
       " 'let': 124,\n",
       " 'off': 125,\n",
       " 'talk': 126,\n",
       " 'gonna': 127,\n",
       " 'please': 128,\n",
       " 'call': 129,\n",
       " 'love': 130,\n",
       " 'over': 131,\n",
       " 'give': 132,\n",
       " 'only': 133,\n",
       " 'two': 134,\n",
       " 'thank': 135,\n",
       " 'told': 136,\n",
       " 'still': 137,\n",
       " 'thing': 138,\n",
       " \"let's\": 139,\n",
       " 'better': 140,\n",
       " 'has': 141,\n",
       " 'night': 142,\n",
       " 'people': 143,\n",
       " 'name': 144,\n",
       " 'god': 145,\n",
       " 'by': 146,\n",
       " 'must': 147,\n",
       " 'thanks': 148,\n",
       " 'our': 149,\n",
       " 'help': 150,\n",
       " 'again': 151,\n",
       " 'work': 152,\n",
       " 'fine': 153,\n",
       " 'huh': 154,\n",
       " 'shit': 155,\n",
       " 'last': 156,\n",
       " 'ever': 157,\n",
       " 'talking': 158,\n",
       " 'long': 159,\n",
       " 'fuck': 160,\n",
       " 'wait': 161,\n",
       " 'believe': 162,\n",
       " 'guess': 163,\n",
       " 'first': 164,\n",
       " 'find': 165,\n",
       " 'because': 166,\n",
       " 'home': 167,\n",
       " 'fucking': 168,\n",
       " 'wrong': 169,\n",
       " 'hell': 170,\n",
       " 'hear': 171,\n",
       " 'bad': 172,\n",
       " 'happened': 173,\n",
       " 'before': 174,\n",
       " 'course': 175,\n",
       " 'uh': 176,\n",
       " 'dead': 177,\n",
       " 'leave': 178,\n",
       " 'away': 179,\n",
       " 'old': 180,\n",
       " 'always': 181,\n",
       " 'around': 182,\n",
       " 'great': 183,\n",
       " 'ask': 184,\n",
       " 'money': 185,\n",
       " 'these': 186,\n",
       " 'things': 187,\n",
       " 'guy': 188,\n",
       " 'stop': 189,\n",
       " 'nice': 190,\n",
       " 'hi': 191,\n",
       " 'even': 192,\n",
       " 'remember': 193,\n",
       " 'getting': 194,\n",
       " 'put': 195,\n",
       " 'feel': 196,\n",
       " 'day': 197,\n",
       " 'those': 198,\n",
       " 'than': 199,\n",
       " 'keep': 200,\n",
       " 'other': 201,\n",
       " 'ai': 202,\n",
       " 'new': 203,\n",
       " 'hello': 204,\n",
       " 'kill': 205,\n",
       " 'gotta': 206,\n",
       " 'into': 207,\n",
       " 'car': 208,\n",
       " 'coming': 209,\n",
       " 'place': 210,\n",
       " 'after': 211,\n",
       " 'another': 212,\n",
       " 'life': 213,\n",
       " 'try': 214,\n",
       " 'mind': 215,\n",
       " 'girl': 216,\n",
       " 'stay': 217,\n",
       " 'years': 218,\n",
       " 'else': 219,\n",
       " 'enough': 220,\n",
       " 'big': 221,\n",
       " 'everything': 222,\n",
       " 'understand': 223,\n",
       " 'tomorrow': 224,\n",
       " 'kind': 225,\n",
       " 'ya': 226,\n",
       " 'three': 227,\n",
       " 'done': 228,\n",
       " 'may': 229,\n",
       " 'listen': 230,\n",
       " 'real': 231,\n",
       " 'lot': 232,\n",
       " 'looking': 233,\n",
       " 'tonight': 234,\n",
       " 'house': 235,\n",
       " 'heard': 236,\n",
       " 'yourself': 237,\n",
       " 'left': 238,\n",
       " \"who's\": 239,\n",
       " 'mother': 240,\n",
       " 'saw': 241,\n",
       " 'father': 242,\n",
       " 'dad': 243,\n",
       " 'friend': 244,\n",
       " 'boy': 245,\n",
       " 'pretty': 246,\n",
       " 'care': 247,\n",
       " 'yet': 248,\n",
       " 'wanted': 249,\n",
       " 'miss': 250,\n",
       " 'room': 251,\n",
       " 'might': 252,\n",
       " 'minute': 253,\n",
       " 'made': 254,\n",
       " 'seen': 255,\n",
       " 'matter': 256,\n",
       " 'baby': 257,\n",
       " 'morning': 258,\n",
       " 'five': 259,\n",
       " 'shut': 260,\n",
       " 'best': 261,\n",
       " 'mrs': 262,\n",
       " 'alright': 263,\n",
       " 'came': 264,\n",
       " 'play': 265,\n",
       " 'idea': 266,\n",
       " 'today': 267,\n",
       " 'late': 268,\n",
       " 'doctor': 269,\n",
       " 'killed': 270,\n",
       " 'through': 271,\n",
       " 'someone': 272,\n",
       " 'looks': 273,\n",
       " 'live': 274,\n",
       " 'ready': 275,\n",
       " 'show': 276,\n",
       " 'job': 277,\n",
       " 'none': 278,\n",
       " 'alone': 279,\n",
       " 'went': 280,\n",
       " 'jack': 281,\n",
       " 'afraid': 282,\n",
       " 'many': 283,\n",
       " 'guys': 284,\n",
       " 'same': 285,\n",
       " 'business': 286,\n",
       " 'already': 287,\n",
       " 'jesus': 288,\n",
       " 'trying': 289,\n",
       " 'wife': 290,\n",
       " 'ten': 291,\n",
       " 'saying': 292,\n",
       " \"'em\": 293,\n",
       " 'honey': 294,\n",
       " 'meet': 295,\n",
       " 'watch': 296,\n",
       " 'use': 297,\n",
       " 'being': 298,\n",
       " 'knew': 299,\n",
       " 'hold': 300,\n",
       " 'exactly': 301,\n",
       " 'problem': 302,\n",
       " 'together': 303,\n",
       " 'crazy': 304,\n",
       " 'probably': 305,\n",
       " 'their': 306,\n",
       " 'four': 307,\n",
       " 'drink': 308,\n",
       " 'damn': 309,\n",
       " 'own': 310,\n",
       " 'open': 311,\n",
       " 'john': 312,\n",
       " 'forget': 313,\n",
       " 'son': 314,\n",
       " 'hundred': 315,\n",
       " 'called': 316,\n",
       " 'ok': 317,\n",
       " 'hope': 318,\n",
       " 'die': 319,\n",
       " 'check': 320,\n",
       " 'stuff': 321,\n",
       " 'wanna': 322,\n",
       " 'later': 323,\n",
       " 'sleep': 324,\n",
       " 'which': 325,\n",
       " 'beautiful': 326,\n",
       " 'next': 327,\n",
       " 'says': 328,\n",
       " 'hurt': 329,\n",
       " 'found': 330,\n",
       " 'dr': 331,\n",
       " 'captain': 332,\n",
       " 'ago': 333,\n",
       " 'start': 334,\n",
       " 'working': 335,\n",
       " 'woman': 336,\n",
       " 'word': 337,\n",
       " 'mom': 338,\n",
       " 'used': 339,\n",
       " 'supposed': 340,\n",
       " 'minutes': 341,\n",
       " 'whole': 342,\n",
       " 'excuse': 343,\n",
       " 'gone': 344,\n",
       " 'worry': 345,\n",
       " 'lost': 346,\n",
       " 'asked': 347,\n",
       " 'every': 348,\n",
       " 'easy': 349,\n",
       " 'door': 350,\n",
       " 'myself': 351,\n",
       " 'once': 352,\n",
       " 'point': 353,\n",
       " 'friends': 354,\n",
       " 'deal': 355,\n",
       " 'thinking': 356,\n",
       " 'run': 357,\n",
       " 'married': 358,\n",
       " 'took': 359,\n",
       " 'happy': 360,\n",
       " 'mine': 361,\n",
       " 'anyway': 362,\n",
       " 'such': 363,\n",
       " 'read': 364,\n",
       " 'turn': 365,\n",
       " 'bring': 366,\n",
       " 'days': 367,\n",
       " 'eat': 368,\n",
       " 'anyone': 369,\n",
       " 'school': 370,\n",
       " 'sit': 371,\n",
       " 'without': 372,\n",
       " 'wants': 373,\n",
       " 'phone': 374,\n",
       " 'answer': 375,\n",
       " 'world': 376,\n",
       " 'hate': 377,\n",
       " 'until': 378,\n",
       " 'change': 379,\n",
       " 'somebody': 380,\n",
       " 'move': 381,\n",
       " 'actually': 382,\n",
       " 'hit': 383,\n",
       " 'case': 384,\n",
       " 'men': 385,\n",
       " 'second': 386,\n",
       " 'brother': 387,\n",
       " 'serious': 388,\n",
       " 'soon': 389,\n",
       " 'while': 390,\n",
       " 'kid': 391,\n",
       " 'wish': 392,\n",
       " 'close': 393,\n",
       " 'question': 394,\n",
       " 'both': 395,\n",
       " 'happen': 396,\n",
       " 'funny': 397,\n",
       " 'ah': 398,\n",
       " 'true': 399,\n",
       " 'trust': 400,\n",
       " 'week': 401,\n",
       " 'quite': 402,\n",
       " 'bet': 403,\n",
       " 'head': 404,\n",
       " 'yours': 405,\n",
       " 'dollars': 406,\n",
       " 'shot': 407,\n",
       " 'story': 408,\n",
       " 'far': 409,\n",
       " 'knows': 410,\n",
       " 'george': 411,\n",
       " 'whatever': 412,\n",
       " 'since': 413,\n",
       " 'suppose': 414,\n",
       " 'different': 415,\n",
       " 'hour': 416,\n",
       " 'hard': 417,\n",
       " 'nobody': 418,\n",
       " 'under': 419,\n",
       " 'ahead': 420,\n",
       " 'trouble': 421,\n",
       " 'makes': 422,\n",
       " 'gun': 423,\n",
       " 'taking': 424,\n",
       " 'number': 425,\n",
       " 'telling': 426,\n",
       " 'waiting': 427,\n",
       " 'walk': 428,\n",
       " 'promise': 429,\n",
       " 'half': 430,\n",
       " 'thousand': 431,\n",
       " 'times': 432,\n",
       " 'hand': 433,\n",
       " 'making': 434,\n",
       " 'dinner': 435,\n",
       " 'everybody': 436,\n",
       " 'end': 437,\n",
       " 'gave': 438,\n",
       " 'six': 439,\n",
       " 'bed': 440,\n",
       " 'few': 441,\n",
       " 'having': 442,\n",
       " 'almost': 443,\n",
       " 'drive': 444,\n",
       " 'goodbye': 445,\n",
       " 'absolutely': 446,\n",
       " 'met': 447,\n",
       " 'anymore': 448,\n",
       " 'most': 449,\n",
       " 'dear': 450,\n",
       " 'sick': 451,\n",
       " 'safe': 452,\n",
       " 'hours': 453,\n",
       " 'party': 454,\n",
       " 'pick': 455,\n",
       " 'family': 456,\n",
       " 'sometimes': 457,\n",
       " 'either': 458,\n",
       " 'truth': 459,\n",
       " 'kids': 460,\n",
       " 'inside': 461,\n",
       " 'year': 462,\n",
       " 'scared': 463,\n",
       " 'cool': 464,\n",
       " 'break': 465,\n",
       " 'means': 466,\n",
       " 'leaving': 467,\n",
       " 'everyone': 468,\n",
       " 'set': 469,\n",
       " 'part': 470,\n",
       " 'least': 471,\n",
       " 'couple': 472,\n",
       " 'pay': 473,\n",
       " 'wonderful': 474,\n",
       " 'tried': 475,\n",
       " 'harry': 476,\n",
       " 'anybody': 477,\n",
       " 'died': 478,\n",
       " 'shoot': 479,\n",
       " 'feeling': 480,\n",
       " \"ma'am\": 481,\n",
       " 'daddy': 482,\n",
       " 'asking': 483,\n",
       " 'cut': 484,\n",
       " 'buy': 485,\n",
       " 'hands': 486,\n",
       " 'chance': 487,\n",
       " 'sort': 488,\n",
       " \"c'mon\": 489,\n",
       " 'hurry': 490,\n",
       " 'water': 491,\n",
       " 'frank': 492,\n",
       " 'game': 493,\n",
       " 'bit': 494,\n",
       " 'hot': 495,\n",
       " 'sounds': 496,\n",
       " 'uhhuh': 497,\n",
       " 'nick': 498,\n",
       " 'rest': 499,\n",
       " 'news': 500,\n",
       " 'ass': 501,\n",
       " 'calling': 502,\n",
       " 'each': 503,\n",
       " 'police': 504,\n",
       " 'shall': 505,\n",
       " 'face': 506,\n",
       " 'kidding': 507,\n",
       " 'death': 508,\n",
       " 'though': 509,\n",
       " 'luck': 510,\n",
       " 'alive': 511,\n",
       " 'coffee': 512,\n",
       " 'eh': 513,\n",
       " 'christ': 514,\n",
       " 'black': 515,\n",
       " 'special': 516,\n",
       " 'lady': 517,\n",
       " 'eyes': 518,\n",
       " 'lucky': 519,\n",
       " 'bob': 520,\n",
       " 'light': 521,\n",
       " 'gets': 522,\n",
       " 'town': 523,\n",
       " 'sound': 524,\n",
       " 'sister': 525,\n",
       " 'eight': 526,\n",
       " 'ride': 527,\n",
       " 'twenty': 528,\n",
       " 'stand': 529,\n",
       " 'running': 530,\n",
       " 'important': 531,\n",
       " 'book': 532,\n",
       " 'young': 533,\n",
       " 'war': 534,\n",
       " 'president': 535,\n",
       " 'million': 536,\n",
       " 'cold': 537,\n",
       " 'stupid': 538,\n",
       " 'nine': 539,\n",
       " 'send': 540,\n",
       " 'mary': 541,\n",
       " 'mister': 542,\n",
       " 'office': 543,\n",
       " 'blood': 544,\n",
       " 'touch': 545,\n",
       " 'david': 546,\n",
       " 'bullshit': 547,\n",
       " 'husband': 548,\n",
       " 'person': 549,\n",
       " 'outside': 550,\n",
       " 'heart': 551,\n",
       " 'fun': 552,\n",
       " 'goes': 553,\n",
       " 'reason': 554,\n",
       " 'pull': 555,\n",
       " 'white': 556,\n",
       " 'buddy': 557,\n",
       " 'save': 558,\n",
       " 'glad': 559,\n",
       " 'date': 560,\n",
       " 'perhaps': 561,\n",
       " 'speak': 562,\n",
       " 'ben': 563,\n",
       " 'body': 564,\n",
       " 'fast': 565,\n",
       " 'playing': 566,\n",
       " 'charlie': 567,\n",
       " 'brought': 568,\n",
       " 'rather': 569,\n",
       " 'comes': 570,\n",
       " 'quit': 571,\n",
       " 'lie': 572,\n",
       " 'girls': 573,\n",
       " 'york': 574,\n",
       " 'busy': 575,\n",
       " 'goddamn': 576,\n",
       " 'accident': 577,\n",
       " 'jimmy': 578,\n",
       " 'side': 579,\n",
       " 'power': 580,\n",
       " 'fight': 581,\n",
       " 'weeks': 582,\n",
       " 'walter': 583,\n",
       " 'women': 584,\n",
       " 'kiss': 585,\n",
       " 'line': 586,\n",
       " 'tired': 587,\n",
       " 'hospital': 588,\n",
       " 'control': 589,\n",
       " 'sent': 590,\n",
       " 'mouth': 591,\n",
       " 'living': 592,\n",
       " 'lunch': 593,\n",
       " \"here's\": 594,\n",
       " 'drop': 595,\n",
       " 'somewhere': 596,\n",
       " 'against': 597,\n",
       " 'hair': 598,\n",
       " 'eddie': 599,\n",
       " 'behind': 600,\n",
       " 'lose': 601,\n",
       " 'likes': 602,\n",
       " 'dog': 603,\n",
       " 'terrible': 604,\n",
       " 'sweet': 605,\n",
       " 'dream': 606,\n",
       " 'paul': 607,\n",
       " 'worried': 608,\n",
       " 'hotel': 609,\n",
       " 'james': 610,\n",
       " 'seem': 611,\n",
       " 'beat': 612,\n",
       " 'bitch': 613,\n",
       " 'quiet': 614,\n",
       " 'bill': 615,\n",
       " 'happens': 616,\n",
       " 'darling': 617,\n",
       " 'red': 618,\n",
       " 'along': 619,\n",
       " 'full': 620,\n",
       " 'catch': 621,\n",
       " 'fifty': 622,\n",
       " 'michael': 623,\n",
       " 'picture': 624,\n",
       " 'interested': 625,\n",
       " 'worse': 626,\n",
       " 'fire': 627,\n",
       " 'food': 628,\n",
       " 'movie': 629,\n",
       " 'music': 630,\n",
       " 'watching': 631,\n",
       " 'evening': 632,\n",
       " 'mistake': 633,\n",
       " 'moving': 634,\n",
       " 'hungry': 635,\n",
       " 'order': 636,\n",
       " 'taken': 637,\n",
       " 'needs': 638,\n",
       " 'air': 639,\n",
       " 'children': 640,\n",
       " 'plan': 641,\n",
       " 'expect': 642,\n",
       " 'max': 643,\n",
       " 'hang': 644,\n",
       " 'joe': 645,\n",
       " 'welcome': 646,\n",
       " 'sex': 647,\n",
       " 'favor': 648,\n",
       " 'child': 649,\n",
       " 'tom': 650,\n",
       " 'bother': 651,\n",
       " 'except': 652,\n",
       " 'mad': 653,\n",
       " 'happening': 654,\n",
       " 'sense': 655,\n",
       " 'possible': 656,\n",
       " 'feet': 657,\n",
       " 'key': 658,\n",
       " 'private': 659,\n",
       " 'certainly': 660,\n",
       " 'figure': 661,\n",
       " 'uncle': 662,\n",
       " 'finish': 663,\n",
       " 'front': 664,\n",
       " 'piece': 665,\n",
       " 'sam': 666,\n",
       " 'aye': 667,\n",
       " 'tv': 668,\n",
       " 'strange': 669,\n",
       " 'meant': 670,\n",
       " 'seems': 671,\n",
       " 'moment': 672,\n",
       " 'worth': 673,\n",
       " 'between': 674,\n",
       " 'afternoon': 675,\n",
       " 'plane': 676,\n",
       " 'straight': 677,\n",
       " 'anywhere': 678,\n",
       " 'missed': 679,\n",
       " 'till': 680,\n",
       " 'parents': 681,\n",
       " 'nope': 682,\n",
       " 'free': 683,\n",
       " 'high': 684,\n",
       " 'also': 685,\n",
       " 'perfect': 686,\n",
       " 'ship': 687,\n",
       " 'worked': 688,\n",
       " 'clear': 689,\n",
       " 'poor': 690,\n",
       " 'questions': 691,\n",
       " 'write': 692,\n",
       " 'explain': 693,\n",
       " 'thinks': 694,\n",
       " 'seven': 695,\n",
       " 'ice': 696,\n",
       " 'bucks': 697,\n",
       " 'peter': 698,\n",
       " 'nervous': 699,\n",
       " 'bastard': 700,\n",
       " 'looked': 701,\n",
       " 'changed': 702,\n",
       " 'miles': 703,\n",
       " 'felt': 704,\n",
       " 'eye': 705,\n",
       " 'staying': 706,\n",
       " 'boss': 707,\n",
       " 'lives': 708,\n",
       " 'clean': 709,\n",
       " 'ray': 710,\n",
       " 'smart': 711,\n",
       " 'outta': 712,\n",
       " 'jake': 713,\n",
       " 'sign': 714,\n",
       " 'started': 715,\n",
       " 'lord': 716,\n",
       " 'learn': 717,\n",
       " 'city': 718,\n",
       " 'holy': 719,\n",
       " 'broke': 720,\n",
       " 'meeting': 721,\n",
       " 'wonder': 722,\n",
       " 'surprise': 723,\n",
       " 'boys': 724,\n",
       " 'careful': 725,\n",
       " 'blue': 726,\n",
       " 'choice': 727,\n",
       " 'seeing': 728,\n",
       " 'cop': 729,\n",
       " 'loved': 730,\n",
       " 'king': 731,\n",
       " 'fucked': 732,\n",
       " 'asleep': 733,\n",
       " 'yah': 734,\n",
       " 'throw': 735,\n",
       " 'asshole': 736,\n",
       " 'neither': 737,\n",
       " 'works': 738,\n",
       " 'bathroom': 739,\n",
       " 'apartment': 740,\n",
       " 'wow': 741,\n",
       " 'thirty': 742,\n",
       " 'yesterday': 743,\n",
       " 'follow': 744,\n",
       " 'relax': 745,\n",
       " 'sing': 746,\n",
       " 'pardon': 747,\n",
       " 'handle': 748,\n",
       " 'month': 749,\n",
       " 'upset': 750,\n",
       " 'country': 751,\n",
       " 'ones': 752,\n",
       " 'street': 753,\n",
       " 'forgot': 754,\n",
       " 'known': 755,\n",
       " 'suit': 756,\n",
       " 'dick': 757,\n",
       " 'forgive': 758,\n",
       " 'company': 759,\n",
       " 'blow': 760,\n",
       " 'sitting': 761,\n",
       " 'fifteen': 762,\n",
       " 'liked': 763,\n",
       " 'jim': 764,\n",
       " 'count': 765,\n",
       " 'lying': 766,\n",
       " 'daughter': 767,\n",
       " 'wearing': 768,\n",
       " 'others': 769,\n",
       " 'johnny': 770,\n",
       " 'general': 771,\n",
       " 'kinda': 772,\n",
       " 'nah': 773,\n",
       " \"'cause\": 774,\n",
       " 'eve': 775,\n",
       " 'caught': 776,\n",
       " \"name's\": 777,\n",
       " 'pleasure': 778,\n",
       " 'carry': 779,\n",
       " 'its': 780,\n",
       " 'major': 781,\n",
       " 'mike': 782,\n",
       " 'talked': 783,\n",
       " 'rose': 784,\n",
       " 'longer': 785,\n",
       " 'cause': 786,\n",
       " 'dance': 787,\n",
       " 'words': 788,\n",
       " 'class': 789,\n",
       " 'de': 790,\n",
       " 'grand': 791,\n",
       " 'voice': 792,\n",
       " 'act': 793,\n",
       " 'christmas': 794,\n",
       " 'chief': 795,\n",
       " 'months': 796,\n",
       " 'louis': 797,\n",
       " 'trip': 798,\n",
       " 'rich': 799,\n",
       " 'gimme': 800,\n",
       " 'dangerous': 801,\n",
       " 'cash': 802,\n",
       " 'dark': 803,\n",
       " 'fall': 804,\n",
       " 'wear': 805,\n",
       " 'drunk': 806,\n",
       " 'boat': 807,\n",
       " 'fact': 808,\n",
       " 'slow': 809,\n",
       " 'twelve': 810,\n",
       " 'killing': 811,\n",
       " \"o'clock\": 812,\n",
       " 'marry': 813,\n",
       " 'twice': 814,\n",
       " 'interesting': 815,\n",
       " 'store': 816,\n",
       " 'college': 817,\n",
       " 'early': 818,\n",
       " 'top': 819,\n",
       " 'difference': 820,\n",
       " 'um': 821,\n",
       " 'ha': 822,\n",
       " 'green': 823,\n",
       " 'owe': 824,\n",
       " 'plenty': 825,\n",
       " 'earth': 826,\n",
       " 'english': 827,\n",
       " 'win': 828,\n",
       " 'american': 829,\n",
       " 'ought': 830,\n",
       " 'past': 831,\n",
       " 'small': 832,\n",
       " 'cops': 833,\n",
       " 'personal': 834,\n",
       " 'state': 835,\n",
       " 'weird': 836,\n",
       " 'often': 837,\n",
       " 'bye': 838,\n",
       " 'tough': 839,\n",
       " 'movies': 840,\n",
       " 'dude': 841,\n",
       " 'lieutenant': 842,\n",
       " 'officer': 843,\n",
       " 'swear': 844,\n",
       " 'fair': 845,\n",
       " 'paid': 846,\n",
       " 'fault': 847,\n",
       " 'whose': 848,\n",
       " 'dying': 849,\n",
       " 'ring': 850,\n",
       " 'join': 851,\n",
       " 'joke': 852,\n",
       " 'less': 853,\n",
       " 'unless': 854,\n",
       " 'wake': 855,\n",
       " 'goodnight': 856,\n",
       " 'cat': 857,\n",
       " 'wrote': 858,\n",
       " 'bridge': 859,\n",
       " 'beer': 860,\n",
       " 'killer': 861,\n",
       " 'calls': 862,\n",
       " 'sake': 863,\n",
       " 'master': 864,\n",
       " 'able': 865,\n",
       " 'ii': 866,\n",
       " 'listening': 867,\n",
       " 'simple': 868,\n",
       " 'bank': 869,\n",
       " 'ï¿½': 870,\n",
       " 'colonel': 871,\n",
       " 'finished': 872,\n",
       " 'born': 873,\n",
       " 'correct': 874,\n",
       " 'information': 875,\n",
       " 'short': 876,\n",
       " 'mama': 877,\n",
       " 'agent': 878,\n",
       " 'given': 879,\n",
       " 'driving': 880,\n",
       " 'california': 881,\n",
       " 'van': 882,\n",
       " 'cost': 883,\n",
       " 'sleeping': 884,\n",
       " 'address': 885,\n",
       " 'boyfriend': 886,\n",
       " 'mangs': 887,\n",
       " 'calm': 888,\n",
       " 'pain': 889,\n",
       " 'attack': 890,\n",
       " 'idiot': 891,\n",
       " 'pal': 892,\n",
       " 'doubt': 893,\n",
       " 'himself': 894,\n",
       " 'norman': 895,\n",
       " 'road': 896,\n",
       " 'silly': 897,\n",
       " 'usually': 898,\n",
       " 'window': 899,\n",
       " 'won': 900,\n",
       " 'guns': 901,\n",
       " 'art': 902,\n",
       " 'sid': 903,\n",
       " 'radio': 904,\n",
       " 'smell': 905,\n",
       " 'clothes': 906,\n",
       " 'secret': 907,\n",
       " 'totally': 908,\n",
       " 'arm': 909,\n",
       " 'lawyer': 910,\n",
       " 'picked': 911,\n",
       " 'girlfriend': 912,\n",
       " 'depends': 913,\n",
       " 'ted': 914,\n",
       " 'law': 915,\n",
       " 'yep': 916,\n",
       " 'record': 917,\n",
       " 'fool': 918,\n",
       " 'holding': 919,\n",
       " 'club': 920,\n",
       " 'upstairs': 921,\n",
       " 'double': 922,\n",
       " 'become': 923,\n",
       " 'fly': 924,\n",
       " 'quick': 925,\n",
       " 'lock': 926,\n",
       " 'figured': 927,\n",
       " 'harold': 928,\n",
       " 'strong': 929,\n",
       " 'sell': 930,\n",
       " 'patrick': 931,\n",
       " 'problems': 932,\n",
       " 'sad': 933,\n",
       " 'fired': 934,\n",
       " 'named': 935,\n",
       " 'list': 936,\n",
       " 'station': 937,\n",
       " 'near': 938,\n",
       " 'realize': 939,\n",
       " 'message': 940,\n",
       " 'honest': 941,\n",
       " 'fish': 942,\n",
       " 'dress': 943,\n",
       " 'moved': 944,\n",
       " 'evil': 945,\n",
       " 'takes': 946,\n",
       " 'murder': 947,\n",
       " 'honor': 948,\n",
       " 'lots': 949,\n",
       " 'following': 950,\n",
       " 'report': 951,\n",
       " 'fat': 952,\n",
       " 'standing': 953,\n",
       " 'age': 954,\n",
       " \"nobody's\": 955,\n",
       " 'apart': 956,\n",
       " 'jason': 957,\n",
       " 'fell': 958,\n",
       " 'missing': 959,\n",
       " 'thomas': 960,\n",
       " 'sun': 961,\n",
       " 'liar': 962,\n",
       " 'nuts': 963,\n",
       " 'bobby': 964,\n",
       " 'birthday': 965,\n",
       " 'third': 966,\n",
       " 'turned': 967,\n",
       " 'involved': 968,\n",
       " 'professor': 969,\n",
       " 'french': 970,\n",
       " 'north': 971,\n",
       " 'floor': 972,\n",
       " 'sweetheart': 973,\n",
       " 'dunno': 974,\n",
       " \"one's\": 975,\n",
       " 'ang': 976,\n",
       " \"something's\": 977,\n",
       " 'letter': 978,\n",
       " 'stopped': 979,\n",
       " 'besides': 980,\n",
       " 'la': 981,\n",
       " 'writing': 982,\n",
       " 'naw': 983,\n",
       " 'river': 984,\n",
       " 'billy': 985,\n",
       " 'tape': 986,\n",
       " 'computer': 987,\n",
       " 'loves': 988,\n",
       " 'file': 989,\n",
       " 'security': 990,\n",
       " 'madame': 991,\n",
       " 'ma': 992,\n",
       " 'promised': 993,\n",
       " 'definitely': 994,\n",
       " 'south': 995,\n",
       " 'nose': 996,\n",
       " 'dumb': 997,\n",
       " 'knock': 998,\n",
       " 'bar': 999,\n",
       " 'jail': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'bos',\n",
       " 2: 'eos',\n",
       " 3: 'you',\n",
       " 4: 'i',\n",
       " 5: 'is',\n",
       " 6: 'the',\n",
       " 7: 'not',\n",
       " 8: 'it',\n",
       " 9: 'to',\n",
       " 10: 'a',\n",
       " 11: 'do',\n",
       " 12: 'that',\n",
       " 13: 'what',\n",
       " 14: 'are',\n",
       " 15: 'me',\n",
       " 16: 'have',\n",
       " 17: 'am',\n",
       " 18: 'we',\n",
       " 19: 'no',\n",
       " 20: 'of',\n",
       " 21: 'he',\n",
       " 22: 'in',\n",
       " 23: 'and',\n",
       " 24: 'will',\n",
       " 25: 'know',\n",
       " 26: 'this',\n",
       " 27: 'your',\n",
       " 28: 'for',\n",
       " 29: 'my',\n",
       " 30: 'was',\n",
       " 31: 'did',\n",
       " 32: 'on',\n",
       " 33: 'be',\n",
       " 34: 'just',\n",
       " 35: 'about',\n",
       " 36: 'how',\n",
       " 37: 'like',\n",
       " 38: 'would',\n",
       " 39: 'yes',\n",
       " 40: 'here',\n",
       " 41: 'they',\n",
       " 42: 'with',\n",
       " 43: 'get',\n",
       " 44: 'so',\n",
       " 45: 'she',\n",
       " 46: 'all',\n",
       " 47: 'but',\n",
       " 48: 'yeah',\n",
       " 49: 'why',\n",
       " 50: 'oh',\n",
       " 51: 'want',\n",
       " 52: 'right',\n",
       " 53: 'got',\n",
       " 54: 'him',\n",
       " 55: 'think',\n",
       " 56: 'well',\n",
       " 57: 'can',\n",
       " 58: 'go',\n",
       " 59: 'where',\n",
       " 60: 'out',\n",
       " 61: 'there',\n",
       " 62: 'up',\n",
       " 63: 'going',\n",
       " 64: 'good',\n",
       " 65: 'at',\n",
       " 66: 'one',\n",
       " 67: 'now',\n",
       " 68: 'see',\n",
       " 69: 'her',\n",
       " 70: 'if',\n",
       " 71: 'who',\n",
       " 72: 'come',\n",
       " 73: 'cannot',\n",
       " 74: 'say',\n",
       " 75: 'tell',\n",
       " 76: 'okay',\n",
       " 77: 'were',\n",
       " 78: 'could',\n",
       " 79: 'sure',\n",
       " 80: 'then',\n",
       " 81: 'from',\n",
       " 82: 'look',\n",
       " 83: 'take',\n",
       " 84: 'something',\n",
       " 85: 'mean',\n",
       " 86: 'time',\n",
       " 87: 'an',\n",
       " 88: 'been',\n",
       " 89: 'does',\n",
       " 90: 'too',\n",
       " 91: 'back',\n",
       " 92: 'never',\n",
       " 93: 'some',\n",
       " 94: 'us',\n",
       " 95: 'when',\n",
       " 96: 'really',\n",
       " 97: 'them',\n",
       " 98: 'man',\n",
       " 99: 'his',\n",
       " 100: 'or',\n",
       " 101: 'should',\n",
       " 102: 'mr',\n",
       " 103: 'doing',\n",
       " 104: 'as',\n",
       " 105: 'way',\n",
       " 106: 'down',\n",
       " 107: 'maybe',\n",
       " 108: 'sorry',\n",
       " 109: 'need',\n",
       " 110: 'said',\n",
       " 111: 'sir',\n",
       " 112: 'much',\n",
       " 113: 'had',\n",
       " 114: 'little',\n",
       " 115: 'nothing',\n",
       " 116: 'more',\n",
       " 117: 'any',\n",
       " 118: 'very',\n",
       " 119: 'make',\n",
       " 120: 'anything',\n",
       " 121: \"there's\",\n",
       " 122: 'thought',\n",
       " 123: 'hey',\n",
       " 124: 'let',\n",
       " 125: 'off',\n",
       " 126: 'talk',\n",
       " 127: 'gonna',\n",
       " 128: 'please',\n",
       " 129: 'call',\n",
       " 130: 'love',\n",
       " 131: 'over',\n",
       " 132: 'give',\n",
       " 133: 'only',\n",
       " 134: 'two',\n",
       " 135: 'thank',\n",
       " 136: 'told',\n",
       " 137: 'still',\n",
       " 138: 'thing',\n",
       " 139: \"let's\",\n",
       " 140: 'better',\n",
       " 141: 'has',\n",
       " 142: 'night',\n",
       " 143: 'people',\n",
       " 144: 'name',\n",
       " 145: 'god',\n",
       " 146: 'by',\n",
       " 147: 'must',\n",
       " 148: 'thanks',\n",
       " 149: 'our',\n",
       " 150: 'help',\n",
       " 151: 'again',\n",
       " 152: 'work',\n",
       " 153: 'fine',\n",
       " 154: 'huh',\n",
       " 155: 'shit',\n",
       " 156: 'last',\n",
       " 157: 'ever',\n",
       " 158: 'talking',\n",
       " 159: 'long',\n",
       " 160: 'fuck',\n",
       " 161: 'wait',\n",
       " 162: 'believe',\n",
       " 163: 'guess',\n",
       " 164: 'first',\n",
       " 165: 'find',\n",
       " 166: 'because',\n",
       " 167: 'home',\n",
       " 168: 'fucking',\n",
       " 169: 'wrong',\n",
       " 170: 'hell',\n",
       " 171: 'hear',\n",
       " 172: 'bad',\n",
       " 173: 'happened',\n",
       " 174: 'before',\n",
       " 175: 'course',\n",
       " 176: 'uh',\n",
       " 177: 'dead',\n",
       " 178: 'leave',\n",
       " 179: 'away',\n",
       " 180: 'old',\n",
       " 181: 'always',\n",
       " 182: 'around',\n",
       " 183: 'great',\n",
       " 184: 'ask',\n",
       " 185: 'money',\n",
       " 186: 'these',\n",
       " 187: 'things',\n",
       " 188: 'guy',\n",
       " 189: 'stop',\n",
       " 190: 'nice',\n",
       " 191: 'hi',\n",
       " 192: 'even',\n",
       " 193: 'remember',\n",
       " 194: 'getting',\n",
       " 195: 'put',\n",
       " 196: 'feel',\n",
       " 197: 'day',\n",
       " 198: 'those',\n",
       " 199: 'than',\n",
       " 200: 'keep',\n",
       " 201: 'other',\n",
       " 202: 'ai',\n",
       " 203: 'new',\n",
       " 204: 'hello',\n",
       " 205: 'kill',\n",
       " 206: 'gotta',\n",
       " 207: 'into',\n",
       " 208: 'car',\n",
       " 209: 'coming',\n",
       " 210: 'place',\n",
       " 211: 'after',\n",
       " 212: 'another',\n",
       " 213: 'life',\n",
       " 214: 'try',\n",
       " 215: 'mind',\n",
       " 216: 'girl',\n",
       " 217: 'stay',\n",
       " 218: 'years',\n",
       " 219: 'else',\n",
       " 220: 'enough',\n",
       " 221: 'big',\n",
       " 222: 'everything',\n",
       " 223: 'understand',\n",
       " 224: 'tomorrow',\n",
       " 225: 'kind',\n",
       " 226: 'ya',\n",
       " 227: 'three',\n",
       " 228: 'done',\n",
       " 229: 'may',\n",
       " 230: 'listen',\n",
       " 231: 'real',\n",
       " 232: 'lot',\n",
       " 233: 'looking',\n",
       " 234: 'tonight',\n",
       " 235: 'house',\n",
       " 236: 'heard',\n",
       " 237: 'yourself',\n",
       " 238: 'left',\n",
       " 239: \"who's\",\n",
       " 240: 'mother',\n",
       " 241: 'saw',\n",
       " 242: 'father',\n",
       " 243: 'dad',\n",
       " 244: 'friend',\n",
       " 245: 'boy',\n",
       " 246: 'pretty',\n",
       " 247: 'care',\n",
       " 248: 'yet',\n",
       " 249: 'wanted',\n",
       " 250: 'miss',\n",
       " 251: 'room',\n",
       " 252: 'might',\n",
       " 253: 'minute',\n",
       " 254: 'made',\n",
       " 255: 'seen',\n",
       " 256: 'matter',\n",
       " 257: 'baby',\n",
       " 258: 'morning',\n",
       " 259: 'five',\n",
       " 260: 'shut',\n",
       " 261: 'best',\n",
       " 262: 'mrs',\n",
       " 263: 'alright',\n",
       " 264: 'came',\n",
       " 265: 'play',\n",
       " 266: 'idea',\n",
       " 267: 'today',\n",
       " 268: 'late',\n",
       " 269: 'doctor',\n",
       " 270: 'killed',\n",
       " 271: 'through',\n",
       " 272: 'someone',\n",
       " 273: 'looks',\n",
       " 274: 'live',\n",
       " 275: 'ready',\n",
       " 276: 'show',\n",
       " 277: 'job',\n",
       " 278: 'none',\n",
       " 279: 'alone',\n",
       " 280: 'went',\n",
       " 281: 'jack',\n",
       " 282: 'afraid',\n",
       " 283: 'many',\n",
       " 284: 'guys',\n",
       " 285: 'same',\n",
       " 286: 'business',\n",
       " 287: 'already',\n",
       " 288: 'jesus',\n",
       " 289: 'trying',\n",
       " 290: 'wife',\n",
       " 291: 'ten',\n",
       " 292: 'saying',\n",
       " 293: \"'em\",\n",
       " 294: 'honey',\n",
       " 295: 'meet',\n",
       " 296: 'watch',\n",
       " 297: 'use',\n",
       " 298: 'being',\n",
       " 299: 'knew',\n",
       " 300: 'hold',\n",
       " 301: 'exactly',\n",
       " 302: 'problem',\n",
       " 303: 'together',\n",
       " 304: 'crazy',\n",
       " 305: 'probably',\n",
       " 306: 'their',\n",
       " 307: 'four',\n",
       " 308: 'drink',\n",
       " 309: 'damn',\n",
       " 310: 'own',\n",
       " 311: 'open',\n",
       " 312: 'john',\n",
       " 313: 'forget',\n",
       " 314: 'son',\n",
       " 315: 'hundred',\n",
       " 316: 'called',\n",
       " 317: 'ok',\n",
       " 318: 'hope',\n",
       " 319: 'die',\n",
       " 320: 'check',\n",
       " 321: 'stuff',\n",
       " 322: 'wanna',\n",
       " 323: 'later',\n",
       " 324: 'sleep',\n",
       " 325: 'which',\n",
       " 326: 'beautiful',\n",
       " 327: 'next',\n",
       " 328: 'says',\n",
       " 329: 'hurt',\n",
       " 330: 'found',\n",
       " 331: 'dr',\n",
       " 332: 'captain',\n",
       " 333: 'ago',\n",
       " 334: 'start',\n",
       " 335: 'working',\n",
       " 336: 'woman',\n",
       " 337: 'word',\n",
       " 338: 'mom',\n",
       " 339: 'used',\n",
       " 340: 'supposed',\n",
       " 341: 'minutes',\n",
       " 342: 'whole',\n",
       " 343: 'excuse',\n",
       " 344: 'gone',\n",
       " 345: 'worry',\n",
       " 346: 'lost',\n",
       " 347: 'asked',\n",
       " 348: 'every',\n",
       " 349: 'easy',\n",
       " 350: 'door',\n",
       " 351: 'myself',\n",
       " 352: 'once',\n",
       " 353: 'point',\n",
       " 354: 'friends',\n",
       " 355: 'deal',\n",
       " 356: 'thinking',\n",
       " 357: 'run',\n",
       " 358: 'married',\n",
       " 359: 'took',\n",
       " 360: 'happy',\n",
       " 361: 'mine',\n",
       " 362: 'anyway',\n",
       " 363: 'such',\n",
       " 364: 'read',\n",
       " 365: 'turn',\n",
       " 366: 'bring',\n",
       " 367: 'days',\n",
       " 368: 'eat',\n",
       " 369: 'anyone',\n",
       " 370: 'school',\n",
       " 371: 'sit',\n",
       " 372: 'without',\n",
       " 373: 'wants',\n",
       " 374: 'phone',\n",
       " 375: 'answer',\n",
       " 376: 'world',\n",
       " 377: 'hate',\n",
       " 378: 'until',\n",
       " 379: 'change',\n",
       " 380: 'somebody',\n",
       " 381: 'move',\n",
       " 382: 'actually',\n",
       " 383: 'hit',\n",
       " 384: 'case',\n",
       " 385: 'men',\n",
       " 386: 'second',\n",
       " 387: 'brother',\n",
       " 388: 'serious',\n",
       " 389: 'soon',\n",
       " 390: 'while',\n",
       " 391: 'kid',\n",
       " 392: 'wish',\n",
       " 393: 'close',\n",
       " 394: 'question',\n",
       " 395: 'both',\n",
       " 396: 'happen',\n",
       " 397: 'funny',\n",
       " 398: 'ah',\n",
       " 399: 'true',\n",
       " 400: 'trust',\n",
       " 401: 'week',\n",
       " 402: 'quite',\n",
       " 403: 'bet',\n",
       " 404: 'head',\n",
       " 405: 'yours',\n",
       " 406: 'dollars',\n",
       " 407: 'shot',\n",
       " 408: 'story',\n",
       " 409: 'far',\n",
       " 410: 'knows',\n",
       " 411: 'george',\n",
       " 412: 'whatever',\n",
       " 413: 'since',\n",
       " 414: 'suppose',\n",
       " 415: 'different',\n",
       " 416: 'hour',\n",
       " 417: 'hard',\n",
       " 418: 'nobody',\n",
       " 419: 'under',\n",
       " 420: 'ahead',\n",
       " 421: 'trouble',\n",
       " 422: 'makes',\n",
       " 423: 'gun',\n",
       " 424: 'taking',\n",
       " 425: 'number',\n",
       " 426: 'telling',\n",
       " 427: 'waiting',\n",
       " 428: 'walk',\n",
       " 429: 'promise',\n",
       " 430: 'half',\n",
       " 431: 'thousand',\n",
       " 432: 'times',\n",
       " 433: 'hand',\n",
       " 434: 'making',\n",
       " 435: 'dinner',\n",
       " 436: 'everybody',\n",
       " 437: 'end',\n",
       " 438: 'gave',\n",
       " 439: 'six',\n",
       " 440: 'bed',\n",
       " 441: 'few',\n",
       " 442: 'having',\n",
       " 443: 'almost',\n",
       " 444: 'drive',\n",
       " 445: 'goodbye',\n",
       " 446: 'absolutely',\n",
       " 447: 'met',\n",
       " 448: 'anymore',\n",
       " 449: 'most',\n",
       " 450: 'dear',\n",
       " 451: 'sick',\n",
       " 452: 'safe',\n",
       " 453: 'hours',\n",
       " 454: 'party',\n",
       " 455: 'pick',\n",
       " 456: 'family',\n",
       " 457: 'sometimes',\n",
       " 458: 'either',\n",
       " 459: 'truth',\n",
       " 460: 'kids',\n",
       " 461: 'inside',\n",
       " 462: 'year',\n",
       " 463: 'scared',\n",
       " 464: 'cool',\n",
       " 465: 'break',\n",
       " 466: 'means',\n",
       " 467: 'leaving',\n",
       " 468: 'everyone',\n",
       " 469: 'set',\n",
       " 470: 'part',\n",
       " 471: 'least',\n",
       " 472: 'couple',\n",
       " 473: 'pay',\n",
       " 474: 'wonderful',\n",
       " 475: 'tried',\n",
       " 476: 'harry',\n",
       " 477: 'anybody',\n",
       " 478: 'died',\n",
       " 479: 'shoot',\n",
       " 480: 'feeling',\n",
       " 481: \"ma'am\",\n",
       " 482: 'daddy',\n",
       " 483: 'asking',\n",
       " 484: 'cut',\n",
       " 485: 'buy',\n",
       " 486: 'hands',\n",
       " 487: 'chance',\n",
       " 488: 'sort',\n",
       " 489: \"c'mon\",\n",
       " 490: 'hurry',\n",
       " 491: 'water',\n",
       " 492: 'frank',\n",
       " 493: 'game',\n",
       " 494: 'bit',\n",
       " 495: 'hot',\n",
       " 496: 'sounds',\n",
       " 497: 'uhhuh',\n",
       " 498: 'nick',\n",
       " 499: 'rest',\n",
       " 500: 'news',\n",
       " 501: 'ass',\n",
       " 502: 'calling',\n",
       " 503: 'each',\n",
       " 504: 'police',\n",
       " 505: 'shall',\n",
       " 506: 'face',\n",
       " 507: 'kidding',\n",
       " 508: 'death',\n",
       " 509: 'though',\n",
       " 510: 'luck',\n",
       " 511: 'alive',\n",
       " 512: 'coffee',\n",
       " 513: 'eh',\n",
       " 514: 'christ',\n",
       " 515: 'black',\n",
       " 516: 'special',\n",
       " 517: 'lady',\n",
       " 518: 'eyes',\n",
       " 519: 'lucky',\n",
       " 520: 'bob',\n",
       " 521: 'light',\n",
       " 522: 'gets',\n",
       " 523: 'town',\n",
       " 524: 'sound',\n",
       " 525: 'sister',\n",
       " 526: 'eight',\n",
       " 527: 'ride',\n",
       " 528: 'twenty',\n",
       " 529: 'stand',\n",
       " 530: 'running',\n",
       " 531: 'important',\n",
       " 532: 'book',\n",
       " 533: 'young',\n",
       " 534: 'war',\n",
       " 535: 'president',\n",
       " 536: 'million',\n",
       " 537: 'cold',\n",
       " 538: 'stupid',\n",
       " 539: 'nine',\n",
       " 540: 'send',\n",
       " 541: 'mary',\n",
       " 542: 'mister',\n",
       " 543: 'office',\n",
       " 544: 'blood',\n",
       " 545: 'touch',\n",
       " 546: 'david',\n",
       " 547: 'bullshit',\n",
       " 548: 'husband',\n",
       " 549: 'person',\n",
       " 550: 'outside',\n",
       " 551: 'heart',\n",
       " 552: 'fun',\n",
       " 553: 'goes',\n",
       " 554: 'reason',\n",
       " 555: 'pull',\n",
       " 556: 'white',\n",
       " 557: 'buddy',\n",
       " 558: 'save',\n",
       " 559: 'glad',\n",
       " 560: 'date',\n",
       " 561: 'perhaps',\n",
       " 562: 'speak',\n",
       " 563: 'ben',\n",
       " 564: 'body',\n",
       " 565: 'fast',\n",
       " 566: 'playing',\n",
       " 567: 'charlie',\n",
       " 568: 'brought',\n",
       " 569: 'rather',\n",
       " 570: 'comes',\n",
       " 571: 'quit',\n",
       " 572: 'lie',\n",
       " 573: 'girls',\n",
       " 574: 'york',\n",
       " 575: 'busy',\n",
       " 576: 'goddamn',\n",
       " 577: 'accident',\n",
       " 578: 'jimmy',\n",
       " 579: 'side',\n",
       " 580: 'power',\n",
       " 581: 'fight',\n",
       " 582: 'weeks',\n",
       " 583: 'walter',\n",
       " 584: 'women',\n",
       " 585: 'kiss',\n",
       " 586: 'line',\n",
       " 587: 'tired',\n",
       " 588: 'hospital',\n",
       " 589: 'control',\n",
       " 590: 'sent',\n",
       " 591: 'mouth',\n",
       " 592: 'living',\n",
       " 593: 'lunch',\n",
       " 594: \"here's\",\n",
       " 595: 'drop',\n",
       " 596: 'somewhere',\n",
       " 597: 'against',\n",
       " 598: 'hair',\n",
       " 599: 'eddie',\n",
       " 600: 'behind',\n",
       " 601: 'lose',\n",
       " 602: 'likes',\n",
       " 603: 'dog',\n",
       " 604: 'terrible',\n",
       " 605: 'sweet',\n",
       " 606: 'dream',\n",
       " 607: 'paul',\n",
       " 608: 'worried',\n",
       " 609: 'hotel',\n",
       " 610: 'james',\n",
       " 611: 'seem',\n",
       " 612: 'beat',\n",
       " 613: 'bitch',\n",
       " 614: 'quiet',\n",
       " 615: 'bill',\n",
       " 616: 'happens',\n",
       " 617: 'darling',\n",
       " 618: 'red',\n",
       " 619: 'along',\n",
       " 620: 'full',\n",
       " 621: 'catch',\n",
       " 622: 'fifty',\n",
       " 623: 'michael',\n",
       " 624: 'picture',\n",
       " 625: 'interested',\n",
       " 626: 'worse',\n",
       " 627: 'fire',\n",
       " 628: 'food',\n",
       " 629: 'movie',\n",
       " 630: 'music',\n",
       " 631: 'watching',\n",
       " 632: 'evening',\n",
       " 633: 'mistake',\n",
       " 634: 'moving',\n",
       " 635: 'hungry',\n",
       " 636: 'order',\n",
       " 637: 'taken',\n",
       " 638: 'needs',\n",
       " 639: 'air',\n",
       " 640: 'children',\n",
       " 641: 'plan',\n",
       " 642: 'expect',\n",
       " 643: 'max',\n",
       " 644: 'hang',\n",
       " 645: 'joe',\n",
       " 646: 'welcome',\n",
       " 647: 'sex',\n",
       " 648: 'favor',\n",
       " 649: 'child',\n",
       " 650: 'tom',\n",
       " 651: 'bother',\n",
       " 652: 'except',\n",
       " 653: 'mad',\n",
       " 654: 'happening',\n",
       " 655: 'sense',\n",
       " 656: 'possible',\n",
       " 657: 'feet',\n",
       " 658: 'key',\n",
       " 659: 'private',\n",
       " 660: 'certainly',\n",
       " 661: 'figure',\n",
       " 662: 'uncle',\n",
       " 663: 'finish',\n",
       " 664: 'front',\n",
       " 665: 'piece',\n",
       " 666: 'sam',\n",
       " 667: 'aye',\n",
       " 668: 'tv',\n",
       " 669: 'strange',\n",
       " 670: 'meant',\n",
       " 671: 'seems',\n",
       " 672: 'moment',\n",
       " 673: 'worth',\n",
       " 674: 'between',\n",
       " 675: 'afternoon',\n",
       " 676: 'plane',\n",
       " 677: 'straight',\n",
       " 678: 'anywhere',\n",
       " 679: 'missed',\n",
       " 680: 'till',\n",
       " 681: 'parents',\n",
       " 682: 'nope',\n",
       " 683: 'free',\n",
       " 684: 'high',\n",
       " 685: 'also',\n",
       " 686: 'perfect',\n",
       " 687: 'ship',\n",
       " 688: 'worked',\n",
       " 689: 'clear',\n",
       " 690: 'poor',\n",
       " 691: 'questions',\n",
       " 692: 'write',\n",
       " 693: 'explain',\n",
       " 694: 'thinks',\n",
       " 695: 'seven',\n",
       " 696: 'ice',\n",
       " 697: 'bucks',\n",
       " 698: 'peter',\n",
       " 699: 'nervous',\n",
       " 700: 'bastard',\n",
       " 701: 'looked',\n",
       " 702: 'changed',\n",
       " 703: 'miles',\n",
       " 704: 'felt',\n",
       " 705: 'eye',\n",
       " 706: 'staying',\n",
       " 707: 'boss',\n",
       " 708: 'lives',\n",
       " 709: 'clean',\n",
       " 710: 'ray',\n",
       " 711: 'smart',\n",
       " 712: 'outta',\n",
       " 713: 'jake',\n",
       " 714: 'sign',\n",
       " 715: 'started',\n",
       " 716: 'lord',\n",
       " 717: 'learn',\n",
       " 718: 'city',\n",
       " 719: 'holy',\n",
       " 720: 'broke',\n",
       " 721: 'meeting',\n",
       " 722: 'wonder',\n",
       " 723: 'surprise',\n",
       " 724: 'boys',\n",
       " 725: 'careful',\n",
       " 726: 'blue',\n",
       " 727: 'choice',\n",
       " 728: 'seeing',\n",
       " 729: 'cop',\n",
       " 730: 'loved',\n",
       " 731: 'king',\n",
       " 732: 'fucked',\n",
       " 733: 'asleep',\n",
       " 734: 'yah',\n",
       " 735: 'throw',\n",
       " 736: 'asshole',\n",
       " 737: 'neither',\n",
       " 738: 'works',\n",
       " 739: 'bathroom',\n",
       " 740: 'apartment',\n",
       " 741: 'wow',\n",
       " 742: 'thirty',\n",
       " 743: 'yesterday',\n",
       " 744: 'follow',\n",
       " 745: 'relax',\n",
       " 746: 'sing',\n",
       " 747: 'pardon',\n",
       " 748: 'handle',\n",
       " 749: 'month',\n",
       " 750: 'upset',\n",
       " 751: 'country',\n",
       " 752: 'ones',\n",
       " 753: 'street',\n",
       " 754: 'forgot',\n",
       " 755: 'known',\n",
       " 756: 'suit',\n",
       " 757: 'dick',\n",
       " 758: 'forgive',\n",
       " 759: 'company',\n",
       " 760: 'blow',\n",
       " 761: 'sitting',\n",
       " 762: 'fifteen',\n",
       " 763: 'liked',\n",
       " 764: 'jim',\n",
       " 765: 'count',\n",
       " 766: 'lying',\n",
       " 767: 'daughter',\n",
       " 768: 'wearing',\n",
       " 769: 'others',\n",
       " 770: 'johnny',\n",
       " 771: 'general',\n",
       " 772: 'kinda',\n",
       " 773: 'nah',\n",
       " 774: \"'cause\",\n",
       " 775: 'eve',\n",
       " 776: 'caught',\n",
       " 777: \"name's\",\n",
       " 778: 'pleasure',\n",
       " 779: 'carry',\n",
       " 780: 'its',\n",
       " 781: 'major',\n",
       " 782: 'mike',\n",
       " 783: 'talked',\n",
       " 784: 'rose',\n",
       " 785: 'longer',\n",
       " 786: 'cause',\n",
       " 787: 'dance',\n",
       " 788: 'words',\n",
       " 789: 'class',\n",
       " 790: 'de',\n",
       " 791: 'grand',\n",
       " 792: 'voice',\n",
       " 793: 'act',\n",
       " 794: 'christmas',\n",
       " 795: 'chief',\n",
       " 796: 'months',\n",
       " 797: 'louis',\n",
       " 798: 'trip',\n",
       " 799: 'rich',\n",
       " 800: 'gimme',\n",
       " 801: 'dangerous',\n",
       " 802: 'cash',\n",
       " 803: 'dark',\n",
       " 804: 'fall',\n",
       " 805: 'wear',\n",
       " 806: 'drunk',\n",
       " 807: 'boat',\n",
       " 808: 'fact',\n",
       " 809: 'slow',\n",
       " 810: 'twelve',\n",
       " 811: 'killing',\n",
       " 812: \"o'clock\",\n",
       " 813: 'marry',\n",
       " 814: 'twice',\n",
       " 815: 'interesting',\n",
       " 816: 'store',\n",
       " 817: 'college',\n",
       " 818: 'early',\n",
       " 819: 'top',\n",
       " 820: 'difference',\n",
       " 821: 'um',\n",
       " 822: 'ha',\n",
       " 823: 'green',\n",
       " 824: 'owe',\n",
       " 825: 'plenty',\n",
       " 826: 'earth',\n",
       " 827: 'english',\n",
       " 828: 'win',\n",
       " 829: 'american',\n",
       " 830: 'ought',\n",
       " 831: 'past',\n",
       " 832: 'small',\n",
       " 833: 'cops',\n",
       " 834: 'personal',\n",
       " 835: 'state',\n",
       " 836: 'weird',\n",
       " 837: 'often',\n",
       " 838: 'bye',\n",
       " 839: 'tough',\n",
       " 840: 'movies',\n",
       " 841: 'dude',\n",
       " 842: 'lieutenant',\n",
       " 843: 'officer',\n",
       " 844: 'swear',\n",
       " 845: 'fair',\n",
       " 846: 'paid',\n",
       " 847: 'fault',\n",
       " 848: 'whose',\n",
       " 849: 'dying',\n",
       " 850: 'ring',\n",
       " 851: 'join',\n",
       " 852: 'joke',\n",
       " 853: 'less',\n",
       " 854: 'unless',\n",
       " 855: 'wake',\n",
       " 856: 'goodnight',\n",
       " 857: 'cat',\n",
       " 858: 'wrote',\n",
       " 859: 'bridge',\n",
       " 860: 'beer',\n",
       " 861: 'killer',\n",
       " 862: 'calls',\n",
       " 863: 'sake',\n",
       " 864: 'master',\n",
       " 865: 'able',\n",
       " 866: 'ii',\n",
       " 867: 'listening',\n",
       " 868: 'simple',\n",
       " 869: 'bank',\n",
       " 870: 'ï¿½',\n",
       " 871: 'colonel',\n",
       " 872: 'finished',\n",
       " 873: 'born',\n",
       " 874: 'correct',\n",
       " 875: 'information',\n",
       " 876: 'short',\n",
       " 877: 'mama',\n",
       " 878: 'agent',\n",
       " 879: 'given',\n",
       " 880: 'driving',\n",
       " 881: 'california',\n",
       " 882: 'van',\n",
       " 883: 'cost',\n",
       " 884: 'sleeping',\n",
       " 885: 'address',\n",
       " 886: 'boyfriend',\n",
       " 887: 'mangs',\n",
       " 888: 'calm',\n",
       " 889: 'pain',\n",
       " 890: 'attack',\n",
       " 891: 'idiot',\n",
       " 892: 'pal',\n",
       " 893: 'doubt',\n",
       " 894: 'himself',\n",
       " 895: 'norman',\n",
       " 896: 'road',\n",
       " 897: 'silly',\n",
       " 898: 'usually',\n",
       " 899: 'window',\n",
       " 900: 'won',\n",
       " 901: 'guns',\n",
       " 902: 'art',\n",
       " 903: 'sid',\n",
       " 904: 'radio',\n",
       " 905: 'smell',\n",
       " 906: 'clothes',\n",
       " 907: 'secret',\n",
       " 908: 'totally',\n",
       " 909: 'arm',\n",
       " 910: 'lawyer',\n",
       " 911: 'picked',\n",
       " 912: 'girlfriend',\n",
       " 913: 'depends',\n",
       " 914: 'ted',\n",
       " 915: 'law',\n",
       " 916: 'yep',\n",
       " 917: 'record',\n",
       " 918: 'fool',\n",
       " 919: 'holding',\n",
       " 920: 'club',\n",
       " 921: 'upstairs',\n",
       " 922: 'double',\n",
       " 923: 'become',\n",
       " 924: 'fly',\n",
       " 925: 'quick',\n",
       " 926: 'lock',\n",
       " 927: 'figured',\n",
       " 928: 'harold',\n",
       " 929: 'strong',\n",
       " 930: 'sell',\n",
       " 931: 'patrick',\n",
       " 932: 'problems',\n",
       " 933: 'sad',\n",
       " 934: 'fired',\n",
       " 935: 'named',\n",
       " 936: 'list',\n",
       " 937: 'station',\n",
       " 938: 'near',\n",
       " 939: 'realize',\n",
       " 940: 'message',\n",
       " 941: 'honest',\n",
       " 942: 'fish',\n",
       " 943: 'dress',\n",
       " 944: 'moved',\n",
       " 945: 'evil',\n",
       " 946: 'takes',\n",
       " 947: 'murder',\n",
       " 948: 'honor',\n",
       " 949: 'lots',\n",
       " 950: 'following',\n",
       " 951: 'report',\n",
       " 952: 'fat',\n",
       " 953: 'standing',\n",
       " 954: 'age',\n",
       " 955: \"nobody's\",\n",
       " 956: 'apart',\n",
       " 957: 'jason',\n",
       " 958: 'fell',\n",
       " 959: 'missing',\n",
       " 960: 'thomas',\n",
       " 961: 'sun',\n",
       " 962: 'liar',\n",
       " 963: 'nuts',\n",
       " 964: 'bobby',\n",
       " 965: 'birthday',\n",
       " 966: 'third',\n",
       " 967: 'turned',\n",
       " 968: 'involved',\n",
       " 969: 'professor',\n",
       " 970: 'french',\n",
       " 971: 'north',\n",
       " 972: 'floor',\n",
       " 973: 'sweetheart',\n",
       " 974: 'dunno',\n",
       " 975: \"one's\",\n",
       " 976: 'ang',\n",
       " 977: \"something's\",\n",
       " 978: 'letter',\n",
       " 979: 'stopped',\n",
       " 980: 'besides',\n",
       " 981: 'la',\n",
       " 982: 'writing',\n",
       " 983: 'naw',\n",
       " 984: 'river',\n",
       " 985: 'billy',\n",
       " 986: 'tape',\n",
       " 987: 'computer',\n",
       " 988: 'loves',\n",
       " 989: 'file',\n",
       " 990: 'security',\n",
       " 991: 'madame',\n",
       " 992: 'ma',\n",
       " 993: 'promised',\n",
       " 994: 'definitely',\n",
       " 995: 'south',\n",
       " 996: 'nose',\n",
       " 997: 'dumb',\n",
       " 998: 'knock',\n",
       " 999: 'bar',\n",
       " 1000: 'jail',\n",
       " ...}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "glove_data = './data/glove.6B.50d.txt'\n",
    "f = open(glove_data, encoding = 'utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    value = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = value\n",
    "f.close()\n",
    " \n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add two more dimensions to the embeddings\n",
    "new_vector_length = 52\n",
    "\n",
    "for key, word_vector in embeddings_index.items():\n",
    "    zero_append_length = new_vector_length - word_vector.shape[0]\n",
    "    embeddings_index[key] = np.append(word_vector, np.zeros(zero_append_length))\n",
    "\n",
    "# create an embedding for bos and eos tags\n",
    "\n",
    "bos_ind = np.zeros(new_vector_length)\n",
    "bos_ind[-2] += 1\n",
    "eos_ind = np.zeros(new_vector_length)\n",
    "eos_ind[-1] += 1\n",
    "embeddings_index['bos'] = bos_ind\n",
    "embeddings_index['eos'] = eos_ind\n",
    "\n",
    "# create an embedding for the padder \n",
    "idx2word[0]='padr'\n",
    "word2idx['padr']= 0\n",
    "embeddings_index['padr']= np.zeros(new_vector_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2seq(encoder_text, decoder_text, VOCAB_SIZE):\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "    tokenizer.fit_on_texts(encoder_text+decoder_text)\n",
    "    encoder_sequences = tokenizer.texts_to_sequences(encoder_text)\n",
    "    decoder_sequences = tokenizer.texts_to_sequences(decoder_text)\n",
    "  \n",
    "    return encoder_sequences, decoder_sequences\n",
    "\n",
    "encoder_sequences, decoder_sequences = text2seq(short_questions, tagged_answers, VOCAB_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_sequences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_sequences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the padding is 2 characters longer than max_line_length to account for the <BOS> and <EOS> tags\n",
    "\n",
    "def padding(encoder_sequences, decoder_sequences, MAX_LEN):\n",
    "  \n",
    "    encoder_input_data = pad_sequences(encoder_sequences, maxlen=MAX_LEN, dtype='int32', padding='post', truncating='post')\n",
    "    decoder_input_data = pad_sequences(decoder_sequences, maxlen=MAX_LEN, dtype='int32', padding='post', truncating='post')\n",
    "    decoder_output_data = pad_sequences(decoder_sequences, maxlen=MAX_LEN+1, dtype='int32', padding='post', truncating='post')\n",
    "    return encoder_input_data, decoder_input_data, decoder_output_data\n",
    "\n",
    "encoder_padded_seq, decoder_padded_seq, decoder_padded_seq_1 = padding(encoder_sequences, decoder_sequences, MAX_LEN=max_line_length+2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47891, 16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_padded_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_padded_seq_1[:,1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_padded_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(encoder_padded_seq)\n",
    "\n",
    "def to_embedding(data, num_samples, max_line_length, new_vector_length):\n",
    "    '''\n",
    "    converts padded sequence data to embeddings\n",
    "    '''\n",
    "    embedded = np.zeros((num_samples,max_line_length+2,new_vector_length))\n",
    "    k = 0\n",
    "    for seq in encoder_padded_seq:\n",
    "        seq_array = np.zeros((max_line_length+2,new_vector_length))\n",
    "        for i, index in enumerate(seq):\n",
    "            word = idx2word[index]\n",
    "            try:\n",
    "                w_embedding = embeddings_index[word]\n",
    "            except KeyError:\n",
    "                w_embedding = embeddings_index['padr']\n",
    "            seq_array[i] += w_embedding\n",
    "        embedded[k] += seq_array\n",
    "        k+=1\n",
    "    return embedded\n",
    "        \n",
    "encoder_input_data = to_embedding(encoder_padded_seq, num_samples, max_line_length, new_vector_length)\n",
    "decoder_input_data = to_embedding(decoder_padded_seq, num_samples, max_line_length, new_vector_length)\n",
    "decoder_output_data = to_embedding(decoder_padded_seq_1[:,1:], num_samples, max_line_length, new_vector_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47891, 16, 52)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47891, 16, 52)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47891, 16, 52)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_splitter(encoder_input_data, decoder_input_data, decoder_output_data, test_size1=0.2, test_size2=0.3):\n",
    "  \n",
    "    en_train, en_test, de_train, de_test, out_train, out_test = train_test_split(encoder_input_data, decoder_input_data, decoder_output_data, test_size=test_size1)\n",
    "    en_train, en_val, de_train, de_val, out_train, out_val = train_test_split(en_train, de_train, out_train, test_size=test_size2)\n",
    "  \n",
    "    return en_train, en_val, en_test, de_train, de_val, de_test, out_train, out_val, out_test\n",
    "\n",
    "en_train, en_val, en_test, de_train, de_val, de_test, out_train, out_val, out_test = data_splitter(encoder_input_data, decoder_input_data, decoder_output_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model_builder(HIDDEN_DIM=300):\n",
    "    \n",
    "    encoder_inputs = Input(shape=(max_line_length+2,new_vector_length,), dtype='float32',)\n",
    "    encoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_LSTM(encoder_inputs)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(max_line_length+2,new_vector_length,), dtype='float32',)\n",
    "    decoder_LSTM = LSTM(HIDDEN_DIM, return_state=True, return_sequences=True)\n",
    "    decoder_outputs, _, _ = decoder_LSTM(decoder_inputs, initial_state=[state_h, state_c])\n",
    "    \n",
    "    # dense_layer = Dense(VOCAB_SIZE, activation='softmax')\n",
    "    outputs = TimeDistributed(Dense(new_vector_length, activation='softmax'))(decoder_outputs)\n",
    "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16, 52)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 16, 52)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   [(None, 300), (None, 423600      input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  [(None, 16, 300), (N 423600      input_10[0][0]                   \n",
      "                                                                 lstm_9[0][1]                     \n",
      "                                                                 lstm_9[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 16, 52)       15652       lstm_10[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 862,852\n",
      "Trainable params: 862,852\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = seq2seq_model_builder(HIDDEN_DIM=300)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\georg\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\georg\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 26818 samples, validate on 11494 samples\n",
      "Epoch 1/40\n",
      "26818/26818 [==============================] - 40s 1ms/step - loss: -29.0745 - accuracy: 0.3238 - val_loss: -31.4993 - val_accuracy: 0.3246\n",
      "Epoch 2/40\n",
      "26818/26818 [==============================] - 39s 1ms/step - loss: -33.0387 - accuracy: 0.3251 - val_loss: -33.9913 - val_accuracy: 0.3199\n",
      "Epoch 3/40\n",
      "26818/26818 [==============================] - 39s 1ms/step - loss: -35.5735 - accuracy: 0.3117 - val_loss: -36.6351 - val_accuracy: 0.3026\n",
      "Epoch 4/40\n",
      "26818/26818 [==============================] - 39s 1ms/step - loss: -38.0948 - accuracy: 0.2996 - val_loss: -38.9352 - val_accuracy: 0.2924\n",
      "Epoch 5/40\n",
      "26818/26818 [==============================] - 39s 1ms/step - loss: -40.2969 - accuracy: 0.2928 - val_loss: -40.6962 - val_accuracy: 0.2873\n",
      "Epoch 6/40\n",
      "26818/26818 [==============================] - 39s 1ms/step - loss: -41.4242 - accuracy: 0.2910 - val_loss: -41.6904 - val_accuracy: 0.2869\n",
      "Epoch 7/40\n",
      "26818/26818 [==============================] - 39s 1ms/step - loss: -42.4020 - accuracy: 0.2905 - val_loss: -42.5734 - val_accuracy: 0.2894\n",
      "Epoch 8/40\n",
      "26818/26818 [==============================] - 39s 1ms/step - loss: -43.1074 - accuracy: 0.2921 - val_loss: -43.0623 - val_accuracy: 0.2907\n",
      "Epoch 9/40\n",
      "26818/26818 [==============================] - 39s 1ms/step - loss: -43.5410 - accuracy: 0.2933 - val_loss: -43.4583 - val_accuracy: 0.2944\n",
      "Epoch 10/40\n",
      "26818/26818 [==============================] - 39s 1ms/step - loss: -43.9088 - accuracy: 0.2944 - val_loss: -43.7890 - val_accuracy: 0.2936\n",
      "Epoch 11/40\n",
      "26818/26818 [==============================] - 39s 1ms/step - loss: -44.2017 - accuracy: 0.2961 - val_loss: -44.0544 - val_accuracy: 0.2965\n",
      "Epoch 12/40\n",
      "26818/26818 [==============================] - 39s 1ms/step - loss: -44.4596 - accuracy: 0.2978 - val_loss: -44.2975 - val_accuracy: 0.2957\n",
      "Epoch 13/40\n",
      "26818/26818 [==============================] - 39s 1ms/step - loss: -44.6910 - accuracy: 0.2993 - val_loss: -44.5116 - val_accuracy: 0.2993\n",
      "Epoch 14/40\n",
      "26818/26818 [==============================] - 40s 2ms/step - loss: -44.8439 - accuracy: 0.2995 - val_loss: -44.6894 - val_accuracy: 0.2991\n",
      "Epoch 15/40\n",
      "26818/26818 [==============================] - 41s 2ms/step - loss: -45.0634 - accuracy: 0.3011 - val_loss: -44.8605 - val_accuracy: 0.2991\n",
      "Epoch 16/40\n",
      "26818/26818 [==============================] - 40s 1ms/step - loss: -45.2282 - accuracy: 0.3021 - val_loss: -45.0159 - val_accuracy: 0.3009\n",
      "Epoch 17/40\n",
      "26818/26818 [==============================] - 39s 1ms/step - loss: -45.3758 - accuracy: 0.3028 - val_loss: -45.1514 - val_accuracy: 0.3025\n",
      "Epoch 18/40\n",
      "26818/26818 [==============================] - 39s 1ms/step - loss: -45.5087 - accuracy: 0.3041 - val_loss: -45.2765 - val_accuracy: 0.3040\n",
      "Epoch 19/40\n",
      "26818/26818 [==============================] - 39s 1ms/step - loss: -45.6318 - accuracy: 0.3047 - val_loss: -45.3925 - val_accuracy: 0.3036\n",
      "Epoch 20/40\n",
      "26818/26818 [==============================] - 39s 1ms/step - loss: -45.7452 - accuracy: 0.3053 - val_loss: -45.5046 - val_accuracy: 0.3040\n",
      "Epoch 21/40\n",
      "26818/26818 [==============================] - 40s 1ms/step - loss: -45.8487 - accuracy: 0.3059 - val_loss: -45.5985 - val_accuracy: 0.3049\n",
      "Epoch 22/40\n",
      "26818/26818 [==============================] - 39s 1ms/step - loss: -45.9444 - accuracy: 0.3067 - val_loss: -45.6909 - val_accuracy: 0.3045\n",
      "Epoch 23/40\n",
      "26818/26818 [==============================] - 40s 1ms/step - loss: -46.0340 - accuracy: 0.3069 - val_loss: -45.7800 - val_accuracy: 0.3050\n",
      "Epoch 24/40\n",
      "26818/26818 [==============================] - 40s 1ms/step - loss: -46.1202 - accuracy: 0.3077 - val_loss: -45.8619 - val_accuracy: 0.3064\n",
      "Epoch 25/40\n",
      "26818/26818 [==============================] - 40s 1ms/step - loss: -46.2284 - accuracy: 0.3084 - val_loss: -45.9913 - val_accuracy: 0.3051\n",
      "Epoch 26/40\n",
      "26818/26818 [==============================] - 40s 1ms/step - loss: -46.3559 - accuracy: 0.3087 - val_loss: -46.1065 - val_accuracy: 0.3083\n",
      "Epoch 27/40\n",
      "26818/26818 [==============================] - 41s 2ms/step - loss: -46.4645 - accuracy: 0.3095 - val_loss: -46.2098 - val_accuracy: 0.3058\n",
      "Epoch 28/40\n",
      "26818/26818 [==============================] - 41s 2ms/step - loss: -46.5567 - accuracy: 0.3100 - val_loss: -46.2934 - val_accuracy: 0.3085\n",
      "Epoch 29/40\n",
      "26818/26818 [==============================] - 40s 1ms/step - loss: -46.6355 - accuracy: 0.3108 - val_loss: -46.3656 - val_accuracy: 0.3081\n",
      "Epoch 30/40\n",
      "26818/26818 [==============================] - 40s 1ms/step - loss: -46.7088 - accuracy: 0.3115 - val_loss: -46.4378 - val_accuracy: 0.3088\n",
      "Epoch 31/40\n",
      "26818/26818 [==============================] - 40s 2ms/step - loss: -46.7788 - accuracy: 0.3120 - val_loss: -46.5072 - val_accuracy: 0.3077\n",
      "Epoch 32/40\n",
      "26818/26818 [==============================] - 39s 1ms/step - loss: -46.8445 - accuracy: 0.3131 - val_loss: -46.5689 - val_accuracy: 0.3095\n",
      "Epoch 33/40\n",
      "26818/26818 [==============================] - 40s 1ms/step - loss: -46.9084 - accuracy: 0.3131 - val_loss: -46.6319 - val_accuracy: 0.3100\n",
      "Epoch 34/40\n",
      "26818/26818 [==============================] - 40s 1ms/step - loss: -46.9693 - accuracy: 0.3139 - val_loss: -46.6928 - val_accuracy: 0.3099\n",
      "Epoch 35/40\n",
      "26818/26818 [==============================] - 39s 1ms/step - loss: -47.0282 - accuracy: 0.3147 - val_loss: -46.7515 - val_accuracy: 0.3122\n",
      "Epoch 36/40\n",
      "26818/26818 [==============================] - 39s 1ms/step - loss: -47.0845 - accuracy: 0.3152 - val_loss: -46.8069 - val_accuracy: 0.3111\n",
      "Epoch 37/40\n",
      "26818/26818 [==============================] - 40s 1ms/step - loss: -47.1405 - accuracy: 0.3160 - val_loss: -46.8603 - val_accuracy: 0.3125\n",
      "Epoch 38/40\n",
      "26818/26818 [==============================] - 39s 1ms/step - loss: -47.1932 - accuracy: 0.3170 - val_loss: -46.9161 - val_accuracy: 0.3133\n",
      "Epoch 39/40\n",
      "26818/26818 [==============================] - 39s 1ms/step - loss: -47.2459 - accuracy: 0.3174 - val_loss: -46.9689 - val_accuracy: 0.3135\n",
      "Epoch 40/40\n",
      "26818/26818 [==============================] - 40s 1ms/step - loss: -47.5465 - accuracy: 0.3175 - val_loss: -47.4199 - val_accuracy: 0.3145\n",
      "Test loss: -47.618202138586746\n",
      "Test accuracy: 0.31449002027511597\n",
      "Wall time: 26min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 128\n",
    "epochs = 40\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit([en_train, de_train],out_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=([en_val, de_val],out_val))\n",
    "score = model.evaluate([en_test, de_test],out_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1.])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['eos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(HIDDEN_DIM,))\n",
    "decoder_state_input_c = Input(shape=(HIDDEN_DIM,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for the frequency of the vocabulary\n",
    "vocab = {}\n",
    "for question in short_questions:\n",
    "    for word in question.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "            \n",
    "for answer in short_answers:\n",
    "    for word in answer.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "            \n",
    "# Remove rare words from the vocabulary.\n",
    "# We will aim to replace fewer than 5% of words with <UNK>\n",
    "# You will see this ratio soon.\n",
    "threshold = 10\n",
    "count = 0\n",
    "for k,v in vocab.items():\n",
    "    if v >= threshold:\n",
    "        count += 1\n",
    "\n",
    "print(\"Size of total vocab:\", len(vocab))\n",
    "print(\"Size of vocab we will use:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we want to use a different vocabulary sizes for the source and target text, \n",
    "# we can set different threshold values.\n",
    "# Nonetheless, we will create dictionaries to provide a unique integer for each word.\n",
    "questions_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        questions_vocab_to_int[word] = word_num\n",
    "        word_num += 1\n",
    "        \n",
    "answers_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        answers_vocab_to_int[word] = word_num\n",
    "        word_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the unique tokens to the vocabulary dictionaries.\n",
    "codes = ['<PAD>','<EOS>','<UNK>','<GO>']\n",
    "\n",
    "for code in codes:\n",
    "    questions_vocab_to_int[code] = len(questions_vocab_to_int)+1\n",
    "    \n",
    "for code in codes:\n",
    "    answers_vocab_to_int[code] = len(answers_vocab_to_int)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries to map the unique integers to their respective words.\n",
    "# i.e. an inverse dictionary for vocab_to_int.\n",
    "questions_int_to_vocab = {v_i: v for v, v_i in questions_vocab_to_int.items()}\n",
    "answers_int_to_vocab = {v_i: v for v, v_i in answers_vocab_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the length of the dictionaries.\n",
    "print(len(questions_vocab_to_int))\n",
    "print(len(questions_int_to_vocab))\n",
    "print(len(answers_vocab_to_int))\n",
    "print(len(answers_int_to_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the end of sentence token to the end of every answer.\n",
    "for i in range(len(short_answers)):\n",
    "    short_answers[i] += ' <EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text to integers. \n",
    "# Replace any words that are not in the respective vocabulary with <UNK> \n",
    "questions_int = []\n",
    "for question in short_questions:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in questions_vocab_to_int:\n",
    "            ints.append(questions_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(questions_vocab_to_int[word])\n",
    "    questions_int.append(ints)\n",
    "    \n",
    "answers_int = []\n",
    "for answer in short_answers:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in answers_vocab_to_int:\n",
    "            ints.append(answers_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(answers_vocab_to_int[word])\n",
    "    answers_int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the lengths\n",
    "print(len(questions_int))\n",
    "print(len(answers_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate what percentage of all words have been replaced with <UNK>\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "for question in questions_int:\n",
    "    for word in question:\n",
    "        if word == questions_vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "        word_count += 1\n",
    "    \n",
    "for answer in answers_int:\n",
    "    for word in answer:\n",
    "        if word == answers_vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "        word_count += 1\n",
    "    \n",
    "unk_ratio = round(unk_count/word_count,4)*100\n",
    "    \n",
    "print(\"Total number of words:\", word_count)\n",
    "print(\"Number of times <UNK> is used:\", unk_count)\n",
    "print(\"Percent of words that are <UNK>: {}%\".format(round(unk_ratio,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort questions and answers by the length of questions.\n",
    "# This will reduce the amount of padding during training\n",
    "# Which should speed up training and help to reduce the loss\n",
    "\n",
    "sorted_questions = []\n",
    "sorted_answers = []\n",
    "\n",
    "for length in range(1, max_line_length+1):\n",
    "    for i in enumerate(questions_int):\n",
    "        if len(i[1]) == length:\n",
    "            sorted_questions.append(questions_int[i[0]])\n",
    "            sorted_answers.append(answers_int[i[0]])\n",
    "\n",
    "print(len(sorted_questions))\n",
    "print(len(sorted_answers))\n",
    "print()\n",
    "for i in range(3):\n",
    "    print(sorted_questions[i])\n",
    "    print(sorted_answers[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    '''Create palceholders for inputs to the model'''\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "    '''Create the encoding layer'''\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    _, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = enc_cell,\n",
    "                                                   cell_bw = enc_cell,\n",
    "                                                   sequence_length = sequence_length,\n",
    "                                                   inputs = rnn_inputs, \n",
    "                                                   dtype=tf.float32)\n",
    "    return enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope,\n",
    "                         output_fn, keep_prob, batch_size):\n",
    "    '''Decode the training data'''\n",
    "    \n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = \\\n",
    "            tf.contrib.seq2seq.prepare_attention(attention_states,\n",
    "                                                 attention_option=\"bahdanau\",\n",
    "                                                 num_units=dec_cell.output_size)\n",
    "    \n",
    "    train_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
    "                                                                     att_keys,\n",
    "                                                                     att_vals,\n",
    "                                                                     att_score_fn,\n",
    "                                                                     att_construct_fn,\n",
    "                                                                     name = \"attn_dec_train\")\n",
    "    train_pred, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, \n",
    "                                                              train_decoder_fn, \n",
    "                                                              dec_embed_input, \n",
    "                                                              sequence_length, \n",
    "                                                              scope=decoding_scope)\n",
    "    train_pred_drop = tf.nn.dropout(train_pred, keep_prob)\n",
    "    return output_fn(train_pred_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id,\n",
    "                         maximum_length, vocab_size, decoding_scope, output_fn, keep_prob, batch_size):\n",
    "    '''Decode the prediction data'''\n",
    "    \n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = \\\n",
    "            tf.contrib.seq2seq.prepare_attention(attention_states,\n",
    "                                                 attention_option=\"bahdanau\",\n",
    "                                                 num_units=dec_cell.output_size)\n",
    "    \n",
    "    infer_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_inference(output_fn, \n",
    "                                                                         encoder_state[0], \n",
    "                                                                         att_keys, \n",
    "                                                                         att_vals, \n",
    "                                                                         att_score_fn, \n",
    "                                                                         att_construct_fn, \n",
    "                                                                         dec_embeddings,\n",
    "                                                                         start_of_sequence_id, \n",
    "                                                                         end_of_sequence_id, \n",
    "                                                                         maximum_length, \n",
    "                                                                         vocab_size, \n",
    "                                                                         name = \"attn_dec_inf\")\n",
    "    infer_logits, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, \n",
    "                                                                infer_decoder_fn, \n",
    "                                                                scope=decoding_scope)\n",
    "    \n",
    "    return infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size,\n",
    "                   num_layers, vocab_to_int, keep_prob, batch_size):\n",
    "    '''Create the decoding cell and input the parameters for the training and inference decoding layers'''\n",
    "    \n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "        dec_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "        \n",
    "        weights = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        output_fn = lambda x: tf.contrib.layers.fully_connected(x, \n",
    "                                                                vocab_size, \n",
    "                                                                None, \n",
    "                                                                scope=decoding_scope,\n",
    "                                                                weights_initializer = weights,\n",
    "                                                                biases_initializer = biases)\n",
    "\n",
    "        train_logits = decoding_layer_train(encoder_state, \n",
    "                                            dec_cell, \n",
    "                                            dec_embed_input, \n",
    "                                            sequence_length, \n",
    "                                            decoding_scope, \n",
    "                                            output_fn, \n",
    "                                            keep_prob, \n",
    "                                            batch_size)\n",
    "        decoding_scope.reuse_variables()\n",
    "        infer_logits = decoding_layer_infer(encoder_state, \n",
    "                                            dec_cell, \n",
    "                                            dec_embeddings, \n",
    "                                            vocab_to_int['<GO>'],\n",
    "                                            vocab_to_int['<EOS>'], \n",
    "                                            sequence_length - 1, \n",
    "                                            vocab_size,\n",
    "                                            decoding_scope, \n",
    "                                            output_fn, keep_prob, \n",
    "                                            batch_size)\n",
    "\n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size, sequence_length, answers_vocab_size, \n",
    "                  questions_vocab_size, enc_embedding_size, dec_embedding_size, rnn_size, num_layers, \n",
    "                  questions_vocab_to_int):\n",
    "    \n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, \n",
    "                                                       answers_vocab_size+1, \n",
    "                                                       enc_embedding_size,\n",
    "                                                       initializer = tf.random_uniform_initializer(0,1))\n",
    "    enc_state = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "\n",
    "    dec_input = process_encoding_input(target_data, questions_vocab_to_int, batch_size)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([questions_vocab_size+1, dec_embedding_size], 0, 1))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    train_logits, infer_logits = decoding_layer(dec_embed_input, \n",
    "                                                dec_embeddings, \n",
    "                                                enc_state, \n",
    "                                                questions_vocab_size, \n",
    "                                                sequence_length, \n",
    "                                                rnn_size, \n",
    "                                                num_layers, \n",
    "                                                questions_vocab_to_int, \n",
    "                                                keep_prob, \n",
    "                                                batch_size)\n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "rnn_size = 512\n",
    "num_layers = 2\n",
    "encoding_embedding_size = 512\n",
    "decoding_embedding_size = 512\n",
    "learning_rate = 0.005\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.0001\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph to ensure that it is ready for training\n",
    "tf.reset_default_graph()\n",
    "# Start the session\n",
    "sess = tf.InteractiveSession()\n",
    "    \n",
    "# Load the model inputs    \n",
    "input_data, targets, lr, keep_prob = model_inputs()\n",
    "# Sequence length will be the max line length for each batch\n",
    "sequence_length = tf.placeholder_with_default(max_line_length, None, name='sequence_length')\n",
    "# Find the shape of the input data for sequence_loss\n",
    "input_shape = tf.shape(input_data)\n",
    "\n",
    "# Create the training and inference logits\n",
    "train_logits, inference_logits = seq2seq_model(\n",
    "    tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, sequence_length, len(answers_vocab_to_int), \n",
    "    len(questions_vocab_to_int), encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers, \n",
    "    questions_vocab_to_int)\n",
    "\n",
    "# Create a tensor for the inference logits, needed if loading a checkpoint version of the model\n",
    "tf.identity(inference_logits, 'logits')\n",
    "\n",
    "with tf.name_scope(\"optimization\"):\n",
    "    # Loss function\n",
    "    cost = tf.contrib.seq2seq.sequence_loss(\n",
    "        train_logits,\n",
    "        targets,\n",
    "        tf.ones([input_shape[0], sequence_length]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = random.sample(range(len(conversation)), 50)\n",
    "sample_context_list = []\n",
    "sample_response_list = []\n",
    "\n",
    "for index in indices:\n",
    "    \n",
    "    response = clean_text(conversation[index][-1])\n",
    "        \n",
    "    context = clean_text(conversation[index][0]) + \"\\n\"\n",
    "    for i in range(1, len(conversation[index]) - 1):\n",
    "        \n",
    "        if i % 2 == 0:\n",
    "            prefix = \"FS: \"\n",
    "        else:\n",
    "            prefix = \"SS: \"\n",
    "            \n",
    "        context += clean_text(conversation[index][i]) + \"\\n\"\n",
    "        \n",
    "    sample_context_list.append(context)\n",
    "    sample_response_list.append(response)\n",
    "\n",
    "#with open(\"cornell_movie_dialogue_sample.csv\", \"w\") as handle:\n",
    "#    for c, r in zip(sample_context_list, sample_response_list):\n",
    "#        handle.write('\"' + c + '\"' + \"#\" + r + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_response_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
