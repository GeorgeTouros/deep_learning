{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a seq2seq model for a chatbot with Keras\n",
    "## By George Touros\n",
    "\n",
    "For a detailed walkthrough of the process please consult the accompanying docx. The commented-out code can be ommitted if one is running this notebook for demostration purposes. The pre-processed dataset and trained model weights are also attached in the deliverable, and should run fastly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas\n",
    "import pickle as pkl\n",
    "from tensorflow.keras import layers , activations , models , preprocessing, utils\n",
    "import wget\n",
    "import zipfile\n",
    "import os, fnmatch\n",
    "import random\n",
    "random.seed(2020)\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_line_length = 1\n",
    "max_line_length = 12\n",
    "HIDDEN_DIM=400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract the dataset\n",
    "#def fetch_data(web_file, local_dir='.'):\n",
    "#    \"\"\"Download the `web_file`, assuming it is a web resource into the local_dir. \n",
    "#    If a file with the same filename already exists in the local directory, do not \n",
    "#    download it but return its path instead.\n",
    "#    Arguments:\n",
    "#        web_file: a web resource identifiable by a url (str)\n",
    "#        local_dir: a local directory to download the web_file into (str)\n",
    "#    Return: The local path to the file (str)\n",
    "#    \"\"\"\n",
    "#    file_name = local_dir + \"/\" + web_file.rsplit(\"/\",1)[-1]\n",
    "#    if os.path.exists(file_name):\n",
    "#        return file_name\n",
    "#    else:\n",
    "#        file_name = wget.download(web_file, out=local_dir)\n",
    "#        return file_name\n",
    "#data_filename = fetch_data('https://s3.amazonaws.com/pytorch-tutorial-assets/cornell_movie_dialogs_corpus.zip')\n",
    "#with zipfile.ZipFile(data_filename, 'r') as zip_ref:\n",
    "#    zip_ref.extractall('.\\data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the movie lines\n",
    "#movie_lines_features = [\"LineID\", \"Character\", \"Movie\", \"Name\", \"Line\"]\n",
    "#movie_lines = pd.read_csv('.\\\\data\\\\cornell movie-dialogs corpus\\\\movie_lines.txt', \n",
    "#                          engine = \"python\", \n",
    "#                          index_col = False,\n",
    "#                          sep=' \\+\\+\\+\\$\\+\\+\\+ ',\n",
    "#                          names = movie_lines_features)\n",
    "#\n",
    "## Using only the required columns, namely, \"LineID\" and \"Line\"\n",
    "#movie_lines = movie_lines[[\"LineID\", \"Line\"]]\n",
    "#\n",
    "## Strip the space from \"LineID\" for further usage and change the datatype of \"Line\"\n",
    "#movie_lines[\"LineID\"] = movie_lines[\"LineID\"].apply(str.strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the conversations file\n",
    "#movie_conversations_features = [\"Character1\", \"Character2\", \"Movie\", \"Conversation\"]\n",
    "#movie_conversations = pd.read_csv('.\\\\data\\\\cornell movie-dialogs corpus\\\\movie_conversations.txt',\n",
    "#                                  sep = \"\\+\\+\\+\\$\\+\\+\\+\", \n",
    "#                                  engine = \"python\", \n",
    "#                                  index_col = False, \n",
    "#                                  names = movie_conversations_features)\n",
    "#\n",
    "# Again using the required feature, \"Conversation\"\n",
    "# movie_conversations = movie_conversations[\"Conversation\"]\n",
    "\n",
    "# Preprocessing and storing the conversation data. This takes too long to run, so we saved the result as a pickle\n",
    "# conversation = [[str(list(movie_lines.loc[movie_lines[\"LineID\"] == u.strip().strip(\"'\"), \"Line\"])[0]).strip() for u in c.strip().strip('[').strip(']').split(',')] for c in movie_conversations]\n",
    "\n",
    "#with open(\".\\\\data\\\\conversations.pkl\", \"wb\") as handle:\n",
    "    #pkl.dump(conversation, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "## Sampling - Cleaning - Trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"data\\conversations.pkl\", \"rb\") as handle:\n",
    "    conversation = pkl.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a random sample of conversations\n",
    "\n",
    "sample_size = 30000\n",
    "\n",
    "indices = random.sample(range(len(conversation)), sample_size)\n",
    "\n",
    "conv_sample = []\n",
    "\n",
    "for i in indices:\n",
    "    conv = conversation[i]\n",
    "    conv_sample.append(conv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the sentences into questions (inputs) and answers (targets)\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for conv in conv_sample:\n",
    "    for i in range(len(conv)-1):\n",
    "        questions.append(conv[i])\n",
    "        answers.append(conv[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    text = re.sub(r\"  \", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "    \n",
    "clean_answers = []    \n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the questions that are too short/long\n",
    "short_questions_temp = []\n",
    "short_answers_temp = []\n",
    "\n",
    "i = 0\n",
    "for question in clean_questions:\n",
    "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
    "        short_questions_temp.append(question)\n",
    "        short_answers_temp.append(clean_answers[i])\n",
    "    i += 1\n",
    "\n",
    "# Filter out the answers that are too short/long\n",
    "short_questions = []\n",
    "short_answers = []\n",
    "\n",
    "i = 0\n",
    "for answer in short_answers_temp:\n",
    "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
    "        short_answers.append(answer)\n",
    "        short_questions.append(short_questions_temp[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of questions: 41971\n",
      "# of answers: 41971\n",
      "% of data used: 52.449999999999996%\n"
     ]
    }
   ],
   "source": [
    "# Compare the number of lines we will use with the total number of lines.\n",
    "print(\"# of questions:\", len(short_questions))\n",
    "print(\"# of answers:\", len(short_answers))\n",
    "print(\"% of data used: {}%\".format(round(len(short_questions)/len(questions),4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 19633\n"
     ]
    }
   ],
   "source": [
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts( short_questions + short_answers )\n",
    "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final number of samples: 28229\n"
     ]
    }
   ],
   "source": [
    "def trim_rare_words(word_counts, list_of_questions, list_of_answers, min_word_freq=1):\n",
    "    \"\"\"\n",
    "    take a dict of word wounts, a list of questions and a list of answers \n",
    "    and return these lists without those that have rare words. \n",
    "    Optional argument sets the minimum frequency of the rare words. \n",
    "    \"\"\"\n",
    "    keep_words = {}\n",
    "    for key, value in word_counts:\n",
    "        if value > min_word_freq:\n",
    "            keep_words[key] = value\n",
    "    \n",
    "    keep_questions_indeces = []\n",
    "    for i, question in enumerate(list_of_questions):\n",
    "        q_words = question.split(' ')\n",
    "        if all(word in keep_words.keys() for word in q_words):\n",
    "            keep_questions_indeces.append(i)\n",
    "    keep_answer_indeces = []\n",
    "    for i, answer in enumerate(list_of_answers):\n",
    "        a_words = answer.split(' ')\n",
    "        if all(word in keep_words.keys() for word in a_words):\n",
    "            keep_answer_indeces.append(i)\n",
    "    \n",
    "    total_keep = set(keep_questions_indeces).intersection(keep_answer_indeces)\n",
    "    \n",
    "    keep_questions = [question for i, question in enumerate(list_of_questions) if i in total_keep ]\n",
    "    keep_answers = [answer for i, answer in enumerate(list_of_answers) if i in total_keep ]\n",
    "    \n",
    "    \n",
    "    return keep_questions, keep_answers\n",
    "\n",
    "k_questions, k_answers = trim_rare_words(tokenizer.word_counts.items(), short_questions, short_answers, 2)  \n",
    "\n",
    "print('final number of samples: {}'.format(len(k_questions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger(decoder_input_sentence):\n",
    "    bos = \"<BOS> \"\n",
    "    eos = \" <EOS>\"\n",
    "    final_target = [bos + text + eos for text in decoder_input_sentence] \n",
    "    return final_target\n",
    "\n",
    "tagged_answers = tagger(k_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New VOCAB SIZE : 7057\n"
     ]
    }
   ],
   "source": [
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts( k_questions + tagged_answers )\n",
    "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "print( 'New VOCAB SIZE : {}'.format( VOCAB_SIZE ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation to Inputs & Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28229, 12) 12\n",
      "(28229, 14) 14\n",
      "(28229, 14, 7057)\n"
     ]
    }
   ],
   "source": [
    "# transform text to tokenized padded sequences, for the encoder and decoder inputs\n",
    "\n",
    "# encoder_input_data\n",
    "tokenized_questions = tokenizer.texts_to_sequences( k_questions )\n",
    "maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\n",
    "encoder_input_data = np.array( padded_questions )\n",
    "print( encoder_input_data.shape , maxlen_questions )\n",
    "\n",
    "# decoder_input_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( tagged_answers )\n",
    "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "decoder_input_data = np.array( padded_answers )\n",
    "print( decoder_input_data.shape , maxlen_answers )\n",
    "\n",
    "# Do the same but with one-hot-encoding for the decoder outputs. \n",
    "\n",
    "# decoder_output_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( tagged_answers )\n",
    "for i in range(len(tokenized_answers)) :\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
    "decoder_output_data = np.array( onehot_answers )\n",
    "print( decoder_output_data.shape )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 400)    2822800     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 400)    2822800     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 400), (None, 1281600     embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 400),  1281600     embedding_3[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 7057)   2829857     lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 11,038,657\n",
      "Trainable params: 11,038,657\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The encoding layer of the model\n",
    "\n",
    "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, HIDDEN_DIM , mask_zero=True ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( HIDDEN_DIM , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "# The decoding layer, initialised with the encoder's output state\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, HIDDEN_DIM , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( HIDDEN_DIM , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "\n",
    "# A dense output layer, with a softmax activation\n",
    "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "# Compile the model using the RMSprop Optimiser\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22583 samples, validate on 5646 samples\n",
      "Epoch 1/45\n",
      "22583/22583 [==============================] - 222s 10ms/sample - loss: 2.3265 - acc: 0.3003 - val_loss: 2.1372 - val_acc: 0.3343\n",
      "Epoch 2/45\n",
      "22583/22583 [==============================] - 221s 10ms/sample - loss: 2.0348 - acc: 0.3500 - val_loss: 2.0308 - val_acc: 0.3583\n",
      "Epoch 3/45\n",
      "22583/22583 [==============================] - 216s 10ms/sample - loss: 1.9396 - acc: 0.3654 - val_loss: 1.9849 - val_acc: 0.3651\n",
      "Epoch 4/45\n",
      "22583/22583 [==============================] - 214s 9ms/sample - loss: 1.8777 - acc: 0.3734 - val_loss: 1.9636 - val_acc: 0.3702\n",
      "Epoch 5/45\n",
      "22583/22583 [==============================] - 218s 10ms/sample - loss: 1.8255 - acc: 0.3812 - val_loss: 1.9546 - val_acc: 0.3734\n",
      "Epoch 6/45\n",
      "22583/22583 [==============================] - 211s 9ms/sample - loss: 1.7773 - acc: 0.3887 - val_loss: 1.9438 - val_acc: 0.3732\n",
      "Epoch 7/45\n",
      "22583/22583 [==============================] - 213s 9ms/sample - loss: 1.7300 - acc: 0.3965 - val_loss: 1.9434 - val_acc: 0.3763\n",
      "Epoch 8/45\n",
      "22583/22583 [==============================] - 215s 10ms/sample - loss: 1.6832 - acc: 0.4053 - val_loss: 1.9443 - val_acc: 0.3774\n",
      "Epoch 9/45\n",
      "22583/22583 [==============================] - 213s 9ms/sample - loss: 1.6350 - acc: 0.4149 - val_loss: 1.9545 - val_acc: 0.3756\n",
      "Epoch 10/45\n",
      "22583/22583 [==============================] - 220s 10ms/sample - loss: 1.5849 - acc: 0.4247 - val_loss: 1.9746 - val_acc: 0.3683\n",
      "Epoch 11/45\n",
      "22583/22583 [==============================] - 213s 9ms/sample - loss: 1.5328 - acc: 0.4358 - val_loss: 1.9865 - val_acc: 0.3735\n",
      "Epoch 12/45\n",
      "22583/22583 [==============================] - 218s 10ms/sample - loss: 1.4789 - acc: 0.4485 - val_loss: 2.0064 - val_acc: 0.3655\n",
      "Epoch 13/45\n",
      "22583/22583 [==============================] - 217s 10ms/sample - loss: 1.4234 - acc: 0.4622 - val_loss: 2.0267 - val_acc: 0.3617\n",
      "Epoch 14/45\n",
      "22583/22583 [==============================] - 216s 10ms/sample - loss: 1.3663 - acc: 0.4799 - val_loss: 2.0563 - val_acc: 0.3562\n",
      "Epoch 15/45\n",
      "22583/22583 [==============================] - 208s 9ms/sample - loss: 1.3095 - acc: 0.4971 - val_loss: 2.0851 - val_acc: 0.3507\n",
      "Epoch 16/45\n",
      "22583/22583 [==============================] - 211s 9ms/sample - loss: 1.2514 - acc: 0.5174 - val_loss: 2.1195 - val_acc: 0.3444\n",
      "Epoch 17/45\n",
      "22583/22583 [==============================] - 205s 9ms/sample - loss: 1.1930 - acc: 0.5379 - val_loss: 2.1557 - val_acc: 0.3422\n",
      "Epoch 18/45\n",
      "22583/22583 [==============================] - 209s 9ms/sample - loss: 1.1352 - acc: 0.5601 - val_loss: 2.1952 - val_acc: 0.3384\n",
      "Epoch 19/45\n",
      "22583/22583 [==============================] - 206s 9ms/sample - loss: 1.0777 - acc: 0.5837 - val_loss: 2.2328 - val_acc: 0.3323\n",
      "Epoch 20/45\n",
      "22583/22583 [==============================] - 203s 9ms/sample - loss: 1.0210 - acc: 0.6062 - val_loss: 2.2697 - val_acc: 0.3315\n",
      "Epoch 21/45\n",
      "22583/22583 [==============================] - 203s 9ms/sample - loss: 0.9659 - acc: 0.6291 - val_loss: 2.3159 - val_acc: 0.3266\n",
      "Epoch 22/45\n",
      "22583/22583 [==============================] - 203s 9ms/sample - loss: 0.9124 - acc: 0.6548 - val_loss: 2.3589 - val_acc: 0.3221\n",
      "Epoch 23/45\n",
      "22583/22583 [==============================] - 208s 9ms/sample - loss: 0.8613 - acc: 0.6760 - val_loss: 2.3984 - val_acc: 0.3201\n",
      "Epoch 24/45\n",
      "22583/22583 [==============================] - 201s 9ms/sample - loss: 0.8117 - acc: 0.6984 - val_loss: 2.4438 - val_acc: 0.3157\n",
      "Epoch 25/45\n",
      "22583/22583 [==============================] - 201s 9ms/sample - loss: 0.7638 - acc: 0.7195 - val_loss: 2.4820 - val_acc: 0.3142\n",
      "Epoch 26/45\n",
      "22583/22583 [==============================] - 202s 9ms/sample - loss: 0.7192 - acc: 0.7396 - val_loss: 2.5278 - val_acc: 0.3104\n",
      "Epoch 27/45\n",
      "22583/22583 [==============================] - 205s 9ms/sample - loss: 0.6764 - acc: 0.7574 - val_loss: 2.5666 - val_acc: 0.3124\n",
      "Epoch 28/45\n",
      "22583/22583 [==============================] - 203s 9ms/sample - loss: 0.6367 - acc: 0.7760 - val_loss: 2.6061 - val_acc: 0.3099\n",
      "Epoch 29/45\n",
      "22583/22583 [==============================] - 203s 9ms/sample - loss: 0.5992 - acc: 0.7918 - val_loss: 2.6376 - val_acc: 0.3120\n",
      "Epoch 30/45\n",
      "22583/22583 [==============================] - 203s 9ms/sample - loss: 0.5641 - acc: 0.8073 - val_loss: 2.6768 - val_acc: 0.3059\n",
      "Epoch 31/45\n",
      "22583/22583 [==============================] - 202s 9ms/sample - loss: 0.5318 - acc: 0.8201 - val_loss: 2.7211 - val_acc: 0.3041\n",
      "Epoch 32/45\n",
      "22583/22583 [==============================] - 208s 9ms/sample - loss: 0.5016 - acc: 0.8329 - val_loss: 2.7468 - val_acc: 0.3053\n",
      "Epoch 33/45\n",
      "22583/22583 [==============================] - 202s 9ms/sample - loss: 0.4736 - acc: 0.8446 - val_loss: 2.7889 - val_acc: 0.3015\n",
      "Epoch 34/45\n",
      "22583/22583 [==============================] - 203s 9ms/sample - loss: 0.4466 - acc: 0.8561 - val_loss: 2.8219 - val_acc: 0.3011\n",
      "Epoch 35/45\n",
      "22583/22583 [==============================] - 205s 9ms/sample - loss: 0.4234 - acc: 0.8644 - val_loss: 2.8458 - val_acc: 0.3031\n",
      "Epoch 36/45\n",
      "22583/22583 [==============================] - 206s 9ms/sample - loss: 0.4022 - acc: 0.8724 - val_loss: 2.8727 - val_acc: 0.3023\n",
      "Epoch 37/45\n",
      "22583/22583 [==============================] - 204s 9ms/sample - loss: 0.3833 - acc: 0.8792 - val_loss: 2.9090 - val_acc: 0.3005\n",
      "Epoch 38/45\n",
      "22583/22583 [==============================] - 202s 9ms/sample - loss: 0.3651 - acc: 0.8864 - val_loss: 2.9394 - val_acc: 0.2992\n",
      "Epoch 39/45\n",
      "22583/22583 [==============================] - 202s 9ms/sample - loss: 0.3496 - acc: 0.8912 - val_loss: 2.9597 - val_acc: 0.2981\n",
      "Epoch 40/45\n",
      "22583/22583 [==============================] - 201s 9ms/sample - loss: 0.3361 - acc: 0.8959 - val_loss: 2.9842 - val_acc: 0.2961\n",
      "Epoch 41/45\n",
      "22583/22583 [==============================] - 205s 9ms/sample - loss: 0.3228 - acc: 0.9004 - val_loss: 3.0060 - val_acc: 0.2980\n",
      "Epoch 42/45\n",
      "22583/22583 [==============================] - 201s 9ms/sample - loss: 0.3118 - acc: 0.9037 - val_loss: 3.0239 - val_acc: 0.2948\n",
      "Epoch 43/45\n",
      "22583/22583 [==============================] - 200s 9ms/sample - loss: 0.2985 - acc: 0.9082 - val_loss: 3.0442 - val_acc: 0.2962\n",
      "Epoch 44/45\n",
      "22583/22583 [==============================] - 202s 9ms/sample - loss: 0.2868 - acc: 0.9112 - val_loss: 3.0633 - val_acc: 0.2937\n",
      "Epoch 45/45\n",
      "22583/22583 [==============================] - 204s 9ms/sample - loss: 0.2774 - acc: 0.9143 - val_loss: 3.0721 - val_acc: 0.2979\n",
      "Wall time: 2h 35min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 100\n",
    "epochs = 45\n",
    "\n",
    "#history = model.fit([encoder_input_data, decoder_input_data],decoder_output_data,\n",
    "#                    batch_size=batch_size,\n",
    "#                    epochs=epochs,\n",
    "#                    verbose=1,\n",
    "#                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#model_json = model.to_json()\n",
    "#with open(\".\\model\\LSTM_D_s2s_model_keras.json\", \"w\") as json_file:\n",
    "#    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "#model.save_weights(\".\\model\\LSTM_D_s2s_model_keras_weights.h5\")\n",
    "#print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('.\\model\\LSTM_D_s2s_model_keras_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to make inference, we need to tweek the process a bit. \n",
    "# We initialise the encoder's and decoder's states using the trained model.\n",
    "\n",
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( HIDDEN_DIM ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( HIDDEN_DIM ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter question : hello\n",
      " i am sorry did i wake you eos\n",
      "Enter question : not at all\n",
      " good for you eos\n",
      "Enter question : how are you\n",
      " fine eos\n",
      "Enter question : any news\n",
      " you look married eos\n",
      "Enter question : i am not\n",
      " you are not what supposed to sleep with me eos\n",
      "Enter question : i would not want to\n",
      " i know eos\n",
      "Enter question : so what are you proposing\n",
      " you know about that eos\n",
      "Enter question : do I\n",
      " well you read this eos\n",
      "Enter question : maybe i missed something\n",
      " good eos\n",
      "Enter question : anything else\n",
      " what are we going to do about this eos\n"
     ]
    }
   ],
   "source": [
    "enc_model , dec_model = make_inference_models()\n",
    "\n",
    "for _ in range(10):\n",
    "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['bos']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'eos' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( decoded_translation )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgements\n",
    "\n",
    "This work would have been impossible without the open source tutorials, widely available in the Web. We mention the most important here: \n",
    "https://colab.research.google.com/drive/1FKhOYhOz8d6BKLVVwL1YMlmoFQ2ML1DS#scrollTo=2zBmN8qB3O-e&forceEdit=true&sandboxMode=true\n",
    "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "https://github.com/shreyans29/thesemicolon/blob/master/chatbotPreprocessing.py\n",
    "https://github.com/shreyans29/thesemicolon/blob/master/chatbotlstmtrain.py\n",
    "https://github.com/shreyans29/thesemicolon/blob/master/chat.py\n",
    "https://github.com/oswaldoludwig/Seq2seq-Chatbot-for-Keras/blob/master/conversation_discriminator.py\n",
    "https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639\n",
    "https://github.com/samurainote/Automatic-Encoder-Decoder_Seq2Seq_Chatbot/blob/master/seq2seq_model.py\n",
    "https://medium.com/analytics-vidhya/intuitive-understanding-of-seq2seq-model-attention-mechanism-in-deep-learning-1c1c24aace1e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
