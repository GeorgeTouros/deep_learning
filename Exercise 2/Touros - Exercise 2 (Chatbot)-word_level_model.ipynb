{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import nltk\n",
    "import pandas\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding, Dense\n",
    "from keras.layers import TimeDistributed, LSTM\n",
    "from keras.engine.input_layer import Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wget\n",
    "import zipfile\n",
    "import os, fnmatch\n",
    "import seaborn as sns\n",
    "import pickle as pkl\n",
    "import random\n",
    "import re\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract the dataset\n",
    "#def fetch_data(web_file, local_dir='.'):\n",
    "#    \"\"\"Download the `web_file`, assuming it is a web resource into the local_dir. \n",
    "#    If a file with the same filename already exists in the local directory, do not \n",
    "#    download it but return its path instead.\n",
    "#    Arguments:\n",
    "#        web_file: a web resource identifiable by a url (str)\n",
    "#        local_dir: a local directory to download the web_file into (str)\n",
    "#    Return: The local path to the file (str)\n",
    "#    \"\"\"\n",
    "#    file_name = local_dir + \"/\" + web_file.rsplit(\"/\",1)[-1]\n",
    "#    if os.path.exists(file_name):\n",
    "#        return file_name\n",
    "#    else:\n",
    "#        file_name = wget.download(web_file, out=local_dir)\n",
    "#        return file_name\n",
    "#data_filename = fetch_data('https://s3.amazonaws.com/pytorch-tutorial-assets/cornell_movie_dialogs_corpus.zip')\n",
    "#with zipfile.ZipFile(data_filename, 'r') as zip_ref:\n",
    "#    zip_ref.extractall('.\\data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the movie lines\n",
    "#movie_lines_features = [\"LineID\", \"Character\", \"Movie\", \"Name\", \"Line\"]\n",
    "#movie_lines = pd.read_csv('.\\\\data\\\\cornell movie-dialogs corpus\\\\movie_lines.txt', \n",
    "#                          engine = \"python\", \n",
    "#                          index_col = False,\n",
    "#                          sep=' \\+\\+\\+\\$\\+\\+\\+ ',\n",
    "#                          names = movie_lines_features)\n",
    "#\n",
    "## Using only the required columns, namely, \"LineID\" and \"Line\"\n",
    "#movie_lines = movie_lines[[\"LineID\", \"Line\"]]\n",
    "#\n",
    "## Strip the space from \"LineID\" for further usage and change the datatype of \"Line\"\n",
    "#movie_lines[\"LineID\"] = movie_lines[\"LineID\"].apply(str.strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movie_lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the conversations file\n",
    "#movie_conversations_features = [\"Character1\", \"Character2\", \"Movie\", \"Conversation\"]\n",
    "#movie_conversations = pd.read_csv('.\\\\data\\\\cornell movie-dialogs corpus\\\\movie_conversations.txt',\n",
    "#                                  sep = \"\\+\\+\\+\\$\\+\\+\\+\", \n",
    "#                                  engine = \"python\", \n",
    "#                                  index_col = False, \n",
    "#                                  names = movie_conversations_features)\n",
    "#\n",
    "# Again using the required feature, \"Conversation\"\n",
    "# movie_conversations = movie_conversations[\"Conversation\"]\n",
    "\n",
    "# Preprocessing and storing the conversation data. This takes too long to run, so we saved the result as a pickle\n",
    "# conversation = [[str(list(movie_lines.loc[movie_lines[\"LineID\"] == u.strip().strip(\"'\"), \"Line\"])[0]).strip() for u in c.strip().strip('[').strip(']').split(',')] for c in movie_conversations]\n",
    "\n",
    "#with open(\".\\\\data\\\\conversations.pkl\", \"wb\") as handle:\n",
    "    #pkl.dump(conversation, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"data\\conversations.pkl\", \"rb\") as handle:\n",
    "    conversation = pkl.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 30000\n",
    "\n",
    "indices = random.sample(range(len(conversation)), sample_size)\n",
    "\n",
    "conv_sample = []\n",
    "\n",
    "for i in indices:\n",
    "    conv = conversation[i]\n",
    "    conv_sample.append(conv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list of conversations, ready to use. These are the tasks we need to do: \n",
    "* Create pairs of questions and answers\n",
    "* Text Cleaning\n",
    "* Remove too large and too small utterances\n",
    "* Put \\<*BOS*> tag and \\<*EOS*> tag for decoder input\n",
    "* Make Vocabulary (VOCAB_SIZE)\n",
    "* Tokenize Bag of words to Bag of IDs\n",
    "* Padding (MAX_LEN)\n",
    "* Word Embedding (EMBEDDING_DIM)\n",
    "* Reshape the Data depends on neural network shape\n",
    "* Split Data for training and validation, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "min_line_length = 1\n",
    "max_line_length = 18\n",
    "VOCAB_SIZE= 10000\n",
    "HIDDEN_DIM=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the sentences into questions (inputs) and answers (targets)\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for conv in conv_sample:\n",
    "    for i in range(len(conv)-1):\n",
    "        questions.append(conv[i])\n",
    "        answers.append(conv[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here now, what are you fixin' to do?\n",
      "Have a look.  At nothing.\n",
      "\n",
      "You've got to tell Craig what's going on. He must never leave Malkovich.\n",
      "I'm glad you learned sign language, Elijah, but I'm tired of your nagging. I'm tired of this conversation. I'm tired period. What has the world ever done for me that I should feel personally responsible for saving it?\n",
      "\n",
      "I'm glad you learned sign language, Elijah, but I'm tired of your nagging. I'm tired of this conversation. I'm tired period. What has the world ever done for me that I should feel personally responsible for saving it?\n",
      "It is better to light one candle than curse the darkness. I learned that from you.\n",
      "\n",
      "Whoever hired us.\n",
      "I got this gig through a contractor. And he most definitely didn't know who was doing the hiring, only that they were paying a lot of money...\n",
      "\n",
      "It's all my fault, I'm so sorry Doug.\n",
      "I had it coming from someone.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if we have loaded the data correctly\n",
    "limit = 0\n",
    "for i in range(limit, limit+5):\n",
    "    print(questions[i])\n",
    "    print(answers[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79766\n",
      "79766\n"
     ]
    }
   ],
   "source": [
    "# Compare lengths of questions and answers\n",
    "print(len(questions))\n",
    "print(len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    text = re.sub(r\"  \", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "    \n",
    "clean_answers = []    \n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here now what are you fixing to do\n",
      "have a look at nothing\n",
      "\n",
      "you have got to tell craig that is going on he must never leave malkovich\n",
      "i am glad you learned sign language elijah but i am tired of your nagging i am tired of this conversation i am tired period what has the world ever done for me that i should feel personally responsible for saving it\n",
      "\n",
      "i am glad you learned sign language elijah but i am tired of your nagging i am tired of this conversation i am tired period what has the world ever done for me that i should feel personally responsible for saving it\n",
      "it is better to light one candle than curse the darkness i learned that from you\n",
      "\n",
      "whoever hired us\n",
      "i got this gig through a contractor and he most definitely did not know who was doing the hiring only that they were paying a lot of money\n",
      "\n",
      "it is all my fault i am so sorry doug\n",
      "i had it coming from someone\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at some of the data to ensure that it has been cleaned well.\n",
    "limit = 0\n",
    "for i in range(limit, limit+5):\n",
    "    print(clean_questions[i])\n",
    "    print(clean_answers[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the length of sentences\n",
    "lengths = []\n",
    "for question in clean_questions:\n",
    "    lengths.append(len(question.split()))\n",
    "for answer in clean_answers:\n",
    "    lengths.append(len(answer.split()))\n",
    "\n",
    "# Create a dataframe so that the values can be inspected\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159532.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.902245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.190124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>365.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              counts\n",
       "count  159532.000000\n",
       "mean       10.902245\n",
       "std        12.190124\n",
       "min         1.000000\n",
       "25%         4.000000\n",
       "50%         7.000000\n",
       "75%        14.000000\n",
       "max       365.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove questions and answers that are shorter than <min_line_length> words and longer than <max_line_length> words.\n",
    "\n",
    "# Filter out the questions that are too short/long\n",
    "short_questions_temp = []\n",
    "short_answers_temp = []\n",
    "\n",
    "i = 0\n",
    "for question in clean_questions:\n",
    "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
    "        short_questions_temp.append(question)\n",
    "        short_answers_temp.append(clean_answers[i])\n",
    "    i += 1\n",
    "\n",
    "# Filter out the answers that are too short/long\n",
    "short_questions = []\n",
    "short_answers = []\n",
    "\n",
    "i = 0\n",
    "for answer in short_answers_temp:\n",
    "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
    "        short_answers.append(answer)\n",
    "        short_questions.append(short_questions_temp[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of questions: 56571\n",
      "# of answers: 56571\n",
      "% of data used: 70.92%\n"
     ]
    }
   ],
   "source": [
    "# Compare the number of lines we will use with the total number of lines.\n",
    "print(\"# of questions:\", len(short_questions))\n",
    "print(\"# of answers:\", len(short_answers))\n",
    "print(\"% of data used: {}%\".format(round(len(short_questions)/len(questions),4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put <BOS> and <EOS> tags on what will be the decoder input\n",
    "\n",
    "def tagger(decoder_input_sentence):\n",
    "    bos = \"<BOS> \"\n",
    "    eos = \" <EOS>\"\n",
    "    final_target = [bos + text + eos for text in decoder_input_sentence] \n",
    "    return final_target\n",
    "\n",
    "tagged_answers = tagger(short_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make vocabulary of arbitrary size\n",
    "\n",
    "def vocab_creator(text_lists, VOCAB_SIZE):\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "    tokenizer.fit_on_texts(text_lists)\n",
    "    dictionary = tokenizer.word_index\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for k, v in dictionary.items():\n",
    "        if v < VOCAB_SIZE:\n",
    "            word2idx[k] = v\n",
    "            idx2word[v] = k\n",
    "        if v >= VOCAB_SIZE-1:\n",
    "            continue\n",
    "          \n",
    "    return word2idx, idx2word\n",
    "\n",
    "\n",
    "word2idx, idx2word = vocab_creator(text_lists=short_questions+tagged_answers, VOCAB_SIZE=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos': 1,\n",
       " 'eos': 2,\n",
       " 'you': 3,\n",
       " 'i': 4,\n",
       " 'is': 5,\n",
       " 'the': 6,\n",
       " 'not': 7,\n",
       " 'to': 8,\n",
       " 'it': 9,\n",
       " 'a': 10,\n",
       " 'do': 11,\n",
       " 'that': 12,\n",
       " 'are': 13,\n",
       " 'what': 14,\n",
       " 'me': 15,\n",
       " 'have': 16,\n",
       " 'of': 17,\n",
       " 'and': 18,\n",
       " 'we': 19,\n",
       " 'in': 20,\n",
       " 'am': 21,\n",
       " 'he': 22,\n",
       " 'no': 23,\n",
       " 'this': 24,\n",
       " 'will': 25,\n",
       " 'your': 26,\n",
       " 'know': 27,\n",
       " 'for': 28,\n",
       " 'was': 29,\n",
       " 'my': 30,\n",
       " 'be': 31,\n",
       " 'on': 32,\n",
       " 'did': 33,\n",
       " 'just': 34,\n",
       " 'about': 35,\n",
       " 'they': 36,\n",
       " 'like': 37,\n",
       " 'would': 38,\n",
       " 'with': 39,\n",
       " 'how': 40,\n",
       " 'get': 41,\n",
       " 'all': 42,\n",
       " 'here': 43,\n",
       " 'but': 44,\n",
       " 'she': 45,\n",
       " 'so': 46,\n",
       " 'yes': 47,\n",
       " 'got': 48,\n",
       " 'want': 49,\n",
       " 'out': 50,\n",
       " 'well': 51,\n",
       " 'him': 52,\n",
       " 'why': 53,\n",
       " 'think': 54,\n",
       " 'yeah': 55,\n",
       " 'oh': 56,\n",
       " 'can': 57,\n",
       " 'right': 58,\n",
       " 'up': 59,\n",
       " 'go': 60,\n",
       " 'there': 61,\n",
       " 'if': 62,\n",
       " 'going': 63,\n",
       " 'at': 64,\n",
       " 'one': 65,\n",
       " 'where': 66,\n",
       " 'her': 67,\n",
       " 'now': 68,\n",
       " 'see': 69,\n",
       " 'good': 70,\n",
       " 'cannot': 71,\n",
       " 'who': 72,\n",
       " 'come': 73,\n",
       " 'tell': 74,\n",
       " 'say': 75,\n",
       " 'were': 76,\n",
       " 'could': 77,\n",
       " 'then': 78,\n",
       " 'time': 79,\n",
       " 'okay': 80,\n",
       " 'back': 81,\n",
       " 'from': 82,\n",
       " 'been': 83,\n",
       " 'some': 84,\n",
       " 'mean': 85,\n",
       " 'when': 86,\n",
       " 'look': 87,\n",
       " 'an': 88,\n",
       " 'take': 89,\n",
       " 'something': 90,\n",
       " 'them': 91,\n",
       " 'sure': 92,\n",
       " 'his': 93,\n",
       " 'us': 94,\n",
       " 'does': 95,\n",
       " 'really': 96,\n",
       " 'too': 97,\n",
       " 'or': 98,\n",
       " 'as': 99,\n",
       " 'never': 100,\n",
       " 'man': 101,\n",
       " 'had': 102,\n",
       " 'should': 103,\n",
       " 'said': 104,\n",
       " 'doing': 105,\n",
       " 'need': 106,\n",
       " 'way': 107,\n",
       " 'maybe': 108,\n",
       " 'mr': 109,\n",
       " 'sorry': 110,\n",
       " 'very': 111,\n",
       " 'little': 112,\n",
       " 'any': 113,\n",
       " 'talk': 114,\n",
       " 'over': 115,\n",
       " 'make': 116,\n",
       " 'gonna': 117,\n",
       " 'down': 118,\n",
       " 'sir': 119,\n",
       " 'more': 120,\n",
       " 'nothing': 121,\n",
       " 'let': 122,\n",
       " \"there's\": 123,\n",
       " 'hey': 124,\n",
       " 'much': 125,\n",
       " 'off': 126,\n",
       " 'anything': 127,\n",
       " 'two': 128,\n",
       " 'thought': 129,\n",
       " 'give': 130,\n",
       " 'call': 131,\n",
       " 'has': 132,\n",
       " 'please': 133,\n",
       " 'still': 134,\n",
       " 'love': 135,\n",
       " 'thing': 136,\n",
       " 'only': 137,\n",
       " 'our': 138,\n",
       " 'by': 139,\n",
       " \"let's\": 140,\n",
       " 'work': 141,\n",
       " 'told': 142,\n",
       " 'people': 143,\n",
       " 'night': 144,\n",
       " 'must': 145,\n",
       " 'long': 146,\n",
       " 'because': 147,\n",
       " 'find': 148,\n",
       " 'ever': 149,\n",
       " 'name': 150,\n",
       " 'thank': 151,\n",
       " 'better': 152,\n",
       " 'help': 153,\n",
       " 'last': 154,\n",
       " 'hell': 155,\n",
       " 'fine': 156,\n",
       " 'again': 157,\n",
       " 'shit': 158,\n",
       " 'god': 159,\n",
       " 'around': 160,\n",
       " 'great': 161,\n",
       " 'money': 162,\n",
       " 'believe': 163,\n",
       " 'before': 164,\n",
       " 'huh': 165,\n",
       " 'talking': 166,\n",
       " 'guess': 167,\n",
       " 'fucking': 168,\n",
       " 'wait': 169,\n",
       " 'thanks': 170,\n",
       " 'fuck': 171,\n",
       " 'even': 172,\n",
       " 'course': 173,\n",
       " 'home': 174,\n",
       " 'dead': 175,\n",
       " 'things': 176,\n",
       " 'other': 177,\n",
       " 'wrong': 178,\n",
       " 'always': 179,\n",
       " 'uh': 180,\n",
       " 'nice': 181,\n",
       " 'first': 182,\n",
       " 'leave': 183,\n",
       " 'put': 184,\n",
       " 'old': 185,\n",
       " 'those': 186,\n",
       " 'day': 187,\n",
       " 'happened': 188,\n",
       " 'these': 189,\n",
       " 'bad': 190,\n",
       " 'keep': 191,\n",
       " 'gotta': 192,\n",
       " 'ask': 193,\n",
       " 'kind': 194,\n",
       " 'away': 195,\n",
       " 'place': 196,\n",
       " 'than': 197,\n",
       " 'remember': 198,\n",
       " 'hear': 199,\n",
       " 'new': 200,\n",
       " 'after': 201,\n",
       " 'guy': 202,\n",
       " 'into': 203,\n",
       " 'ai': 204,\n",
       " 'kill': 205,\n",
       " 'feel': 206,\n",
       " 'stop': 207,\n",
       " 'girl': 208,\n",
       " 'big': 209,\n",
       " 'years': 210,\n",
       " 'coming': 211,\n",
       " 'three': 212,\n",
       " 'real': 213,\n",
       " 'mind': 214,\n",
       " 'life': 215,\n",
       " 'might': 216,\n",
       " 'getting': 217,\n",
       " 'stay': 218,\n",
       " 'else': 219,\n",
       " 'hi': 220,\n",
       " 'lot': 221,\n",
       " 'enough': 222,\n",
       " 'hello': 223,\n",
       " 'another': 224,\n",
       " 'everything': 225,\n",
       " 'miss': 226,\n",
       " 'understand': 227,\n",
       " 'friend': 228,\n",
       " 'made': 229,\n",
       " 'tonight': 230,\n",
       " 'looking': 231,\n",
       " 'try': 232,\n",
       " 'mother': 233,\n",
       " 'left': 234,\n",
       " 'saw': 235,\n",
       " 'done': 236,\n",
       " 'ya': 237,\n",
       " 'trying': 238,\n",
       " 'heard': 239,\n",
       " 'yourself': 240,\n",
       " 'may': 241,\n",
       " 'house': 242,\n",
       " 'car': 243,\n",
       " 'wanted': 244,\n",
       " 'care': 245,\n",
       " 'someone': 246,\n",
       " 'boy': 247,\n",
       " 'morning': 248,\n",
       " 'room': 249,\n",
       " 'pretty': 250,\n",
       " 'listen': 251,\n",
       " 'yet': 252,\n",
       " 'idea': 253,\n",
       " 'baby': 254,\n",
       " 'father': 255,\n",
       " 'tomorrow': 256,\n",
       " \"who's\": 257,\n",
       " 'came': 258,\n",
       " 'show': 259,\n",
       " 'seen': 260,\n",
       " 'their': 261,\n",
       " 'own': 262,\n",
       " 'dad': 263,\n",
       " 'knew': 264,\n",
       " 'went': 265,\n",
       " 'jesus': 266,\n",
       " 'job': 267,\n",
       " 'play': 268,\n",
       " 'called': 269,\n",
       " 'through': 270,\n",
       " 'many': 271,\n",
       " 'five': 272,\n",
       " 'killed': 273,\n",
       " 'shut': 274,\n",
       " 'business': 275,\n",
       " 'live': 276,\n",
       " 'already': 277,\n",
       " 'found': 278,\n",
       " 'best': 279,\n",
       " 'today': 280,\n",
       " 'use': 281,\n",
       " 'being': 282,\n",
       " 'together': 283,\n",
       " \"'em\": 284,\n",
       " 'matter': 285,\n",
       " 'looks': 286,\n",
       " 'alright': 287,\n",
       " 'meet': 288,\n",
       " 'afraid': 289,\n",
       " 'same': 290,\n",
       " 'probably': 291,\n",
       " 'damn': 292,\n",
       " 'problem': 293,\n",
       " 'next': 294,\n",
       " 'guys': 295,\n",
       " 'gone': 296,\n",
       " 'saying': 297,\n",
       " 'late': 298,\n",
       " 'minute': 299,\n",
       " 'supposed': 300,\n",
       " 'mrs': 301,\n",
       " 'used': 302,\n",
       " 'start': 303,\n",
       " 'alone': 304,\n",
       " 'stuff': 305,\n",
       " 'wife': 306,\n",
       " 'took': 307,\n",
       " 'myself': 308,\n",
       " 'son': 309,\n",
       " 'every': 310,\n",
       " 'worry': 311,\n",
       " 'friends': 312,\n",
       " 'which': 313,\n",
       " 'mom': 314,\n",
       " 'wanna': 315,\n",
       " 'four': 316,\n",
       " 'forget': 317,\n",
       " 'few': 318,\n",
       " 'minutes': 319,\n",
       " 'school': 320,\n",
       " 'phone': 321,\n",
       " 'drink': 322,\n",
       " 'wants': 323,\n",
       " 'hundred': 324,\n",
       " 'exactly': 325,\n",
       " 'hope': 326,\n",
       " 'hurt': 327,\n",
       " 'die': 328,\n",
       " 'woman': 329,\n",
       " 'run': 330,\n",
       " 'beautiful': 331,\n",
       " 'until': 332,\n",
       " 'somebody': 333,\n",
       " 'without': 334,\n",
       " 'ready': 335,\n",
       " 'ten': 336,\n",
       " 'watch': 337,\n",
       " 'ago': 338,\n",
       " 'anyway': 339,\n",
       " 'men': 340,\n",
       " 'thinking': 341,\n",
       " 'world': 342,\n",
       " 'open': 343,\n",
       " 'crazy': 344,\n",
       " 'married': 345,\n",
       " 'hate': 346,\n",
       " 'once': 347,\n",
       " 'sit': 348,\n",
       " 'check': 349,\n",
       " 'whole': 350,\n",
       " 'funny': 351,\n",
       " 'working': 352,\n",
       " 'says': 353,\n",
       " 'word': 354,\n",
       " 'lost': 355,\n",
       " 'later': 356,\n",
       " 'none': 357,\n",
       " 'while': 358,\n",
       " 'brother': 359,\n",
       " 'hard': 360,\n",
       " 'bring': 361,\n",
       " 'happy': 362,\n",
       " 'quite': 363,\n",
       " 'honey': 364,\n",
       " 'sleep': 365,\n",
       " 'hold': 366,\n",
       " 'head': 367,\n",
       " 'hit': 368,\n",
       " 'true': 369,\n",
       " 'doctor': 370,\n",
       " 'mine': 371,\n",
       " 'kid': 372,\n",
       " 'gun': 373,\n",
       " 'deal': 374,\n",
       " 'part': 375,\n",
       " 'least': 376,\n",
       " 'jack': 377,\n",
       " 'taking': 378,\n",
       " 'week': 379,\n",
       " 'anyone': 380,\n",
       " 'actually': 381,\n",
       " 'point': 382,\n",
       " 'such': 383,\n",
       " 'question': 384,\n",
       " 'door': 385,\n",
       " 'john': 386,\n",
       " 'read': 387,\n",
       " 'move': 388,\n",
       " 'turn': 389,\n",
       " 'captain': 390,\n",
       " 'half': 391,\n",
       " 'ok': 392,\n",
       " 'kids': 393,\n",
       " 'nobody': 394,\n",
       " 'whatever': 395,\n",
       " 'dr': 396,\n",
       " 'days': 397,\n",
       " 'ah': 398,\n",
       " 'both': 399,\n",
       " 'party': 400,\n",
       " 'anymore': 401,\n",
       " 'thousand': 402,\n",
       " 'suppose': 403,\n",
       " 'having': 404,\n",
       " 'year': 405,\n",
       " 'wish': 406,\n",
       " 'soon': 407,\n",
       " 'bet': 408,\n",
       " 'makes': 409,\n",
       " 'sick': 410,\n",
       " 'eat': 411,\n",
       " 'second': 412,\n",
       " 'under': 413,\n",
       " 'knows': 414,\n",
       " 'important': 415,\n",
       " 'drive': 416,\n",
       " 'yours': 417,\n",
       " 'since': 418,\n",
       " 'telling': 419,\n",
       " 'most': 420,\n",
       " 'easy': 421,\n",
       " 'asked': 422,\n",
       " 'case': 423,\n",
       " 'close': 424,\n",
       " 'far': 425,\n",
       " 'serious': 426,\n",
       " 'shot': 427,\n",
       " 'dollars': 428,\n",
       " 'police': 429,\n",
       " 'cut': 430,\n",
       " 'trust': 431,\n",
       " 'times': 432,\n",
       " 'family': 433,\n",
       " 'frank': 434,\n",
       " 'excuse': 435,\n",
       " 'story': 436,\n",
       " 'making': 437,\n",
       " 'chance': 438,\n",
       " 'anybody': 439,\n",
       " 'happen': 440,\n",
       " 'six': 441,\n",
       " 'ass': 442,\n",
       " 'met': 443,\n",
       " 'pay': 444,\n",
       " 'answer': 445,\n",
       " 'walk': 446,\n",
       " 'change': 447,\n",
       " 'rest': 448,\n",
       " 'bed': 449,\n",
       " 'young': 450,\n",
       " 'end': 451,\n",
       " 'send': 452,\n",
       " 'bit': 453,\n",
       " 'town': 454,\n",
       " 'number': 455,\n",
       " 'gave': 456,\n",
       " 'sometimes': 457,\n",
       " 'waiting': 458,\n",
       " 'died': 459,\n",
       " 'asking': 460,\n",
       " 'kidding': 461,\n",
       " 'water': 462,\n",
       " 'different': 463,\n",
       " 'though': 464,\n",
       " 'everyone': 465,\n",
       " 'truth': 466,\n",
       " 'office': 467,\n",
       " 'almost': 468,\n",
       " 'set': 469,\n",
       " 'sounds': 470,\n",
       " 'game': 471,\n",
       " 'cool': 472,\n",
       " 'george': 473,\n",
       " 'everybody': 474,\n",
       " 'couple': 475,\n",
       " 'daddy': 476,\n",
       " 'shoot': 477,\n",
       " 'hour': 478,\n",
       " 'feeling': 479,\n",
       " 'inside': 480,\n",
       " 'twenty': 481,\n",
       " 'either': 482,\n",
       " 'pick': 483,\n",
       " 'christ': 484,\n",
       " 'heart': 485,\n",
       " 'tried': 486,\n",
       " 'face': 487,\n",
       " 'each': 488,\n",
       " 'promise': 489,\n",
       " 'trouble': 490,\n",
       " 'blood': 491,\n",
       " 'dinner': 492,\n",
       " 'gets': 493,\n",
       " 'harry': 494,\n",
       " 'alive': 495,\n",
       " 'sex': 496,\n",
       " \"c'mon\": 497,\n",
       " 'high': 498,\n",
       " 'means': 499,\n",
       " 'stupid': 500,\n",
       " 'eight': 501,\n",
       " 'eyes': 502,\n",
       " 'power': 503,\n",
       " 'book': 504,\n",
       " 'running': 505,\n",
       " 'luck': 506,\n",
       " 'hand': 507,\n",
       " 'war': 508,\n",
       " 'lucky': 509,\n",
       " 'stand': 510,\n",
       " 'reason': 511,\n",
       " 'death': 512,\n",
       " 'perhaps': 513,\n",
       " 'scared': 514,\n",
       " 'glad': 515,\n",
       " 'shall': 516,\n",
       " 'calling': 517,\n",
       " 'full': 518,\n",
       " 'playing': 519,\n",
       " 'goddamn': 520,\n",
       " 'leaving': 521,\n",
       " 'mary': 522,\n",
       " 'husband': 523,\n",
       " 'uhhuh': 524,\n",
       " 'break': 525,\n",
       " 'months': 526,\n",
       " 'outside': 527,\n",
       " 'dear': 528,\n",
       " 'hours': 529,\n",
       " 'safe': 530,\n",
       " 'hot': 531,\n",
       " 'buy': 532,\n",
       " 'against': 533,\n",
       " 'seem': 534,\n",
       " 'sound': 535,\n",
       " 'speak': 536,\n",
       " 'boys': 537,\n",
       " 'special': 538,\n",
       " 'behind': 539,\n",
       " 'lie': 540,\n",
       " 'white': 541,\n",
       " 'person': 542,\n",
       " 'black': 543,\n",
       " 'goes': 544,\n",
       " 'goodbye': 545,\n",
       " 'sam': 546,\n",
       " 'dream': 547,\n",
       " 'side': 548,\n",
       " 'fire': 549,\n",
       " 'hang': 550,\n",
       " 'fun': 551,\n",
       " 'york': 552,\n",
       " 'light': 553,\n",
       " 'needs': 554,\n",
       " 'possible': 555,\n",
       " 'eh': 556,\n",
       " 'questions': 557,\n",
       " 'lose': 558,\n",
       " 'rather': 559,\n",
       " 'hair': 560,\n",
       " 'along': 561,\n",
       " 'early': 562,\n",
       " 'sort': 563,\n",
       " 'figure': 564,\n",
       " 'save': 565,\n",
       " 'million': 566,\n",
       " 'sent': 567,\n",
       " 'movie': 568,\n",
       " 'ride': 569,\n",
       " 'street': 570,\n",
       " 'mouth': 571,\n",
       " 'dog': 572,\n",
       " 'free': 573,\n",
       " 'started': 574,\n",
       " 'hands': 575,\n",
       " 'thirty': 576,\n",
       " 'darling': 577,\n",
       " 'ahead': 578,\n",
       " 'drop': 579,\n",
       " 'moving': 580,\n",
       " 'living': 581,\n",
       " 'worse': 582,\n",
       " 'nick': 583,\n",
       " 'wonder': 584,\n",
       " \"'cause\": 585,\n",
       " 'line': 586,\n",
       " 'hurry': 587,\n",
       " 'somewhere': 588,\n",
       " 'brought': 589,\n",
       " 'wonderful': 590,\n",
       " 'body': 591,\n",
       " 'bullshit': 592,\n",
       " 'women': 593,\n",
       " 'food': 594,\n",
       " 'worth': 595,\n",
       " 'girls': 596,\n",
       " 'tired': 597,\n",
       " 'miles': 598,\n",
       " 'certainly': 599,\n",
       " 'date': 600,\n",
       " 'coffee': 601,\n",
       " 'buddy': 602,\n",
       " 'sweet': 603,\n",
       " 'eddie': 604,\n",
       " 'charlie': 605,\n",
       " 'worried': 606,\n",
       " 'moment': 607,\n",
       " 'interested': 608,\n",
       " 'bitch': 609,\n",
       " 'news': 610,\n",
       " \"here's\": 611,\n",
       " 'city': 612,\n",
       " 'seven': 613,\n",
       " 'president': 614,\n",
       " 'seems': 615,\n",
       " 'comes': 616,\n",
       " 'touch': 617,\n",
       " 'its': 618,\n",
       " 'front': 619,\n",
       " 'mister': 620,\n",
       " 'also': 621,\n",
       " 'lady': 622,\n",
       " 'sister': 623,\n",
       " 'ha': 624,\n",
       " 'air': 625,\n",
       " 'fast': 626,\n",
       " 'perfect': 627,\n",
       " 'expect': 628,\n",
       " 'till': 629,\n",
       " 'walter': 630,\n",
       " 'himself': 631,\n",
       " 'pull': 632,\n",
       " 'happening': 633,\n",
       " 'wear': 634,\n",
       " 'plan': 635,\n",
       " 'fifty': 636,\n",
       " 'weeks': 637,\n",
       " 'meeting': 638,\n",
       " 'cold': 639,\n",
       " 'relax': 640,\n",
       " 'king': 641,\n",
       " 'parents': 642,\n",
       " 'hospital': 643,\n",
       " 'general': 644,\n",
       " 'seeing': 645,\n",
       " 'sense': 646,\n",
       " 'act': 647,\n",
       " 'dude': 648,\n",
       " 'finish': 649,\n",
       " 'lunch': 650,\n",
       " \"ma'am\": 651,\n",
       " 'child': 652,\n",
       " 'absolutely': 653,\n",
       " 'poor': 654,\n",
       " 'joe': 655,\n",
       " 'tom': 656,\n",
       " 'bill': 657,\n",
       " 'clean': 658,\n",
       " 'fight': 659,\n",
       " 'report': 660,\n",
       " 'beer': 661,\n",
       " 'earth': 662,\n",
       " \"name's\": 663,\n",
       " 'follow': 664,\n",
       " 'choice': 665,\n",
       " 'looked': 666,\n",
       " 'accident': 667,\n",
       " 'order': 668,\n",
       " 'picture': 669,\n",
       " '�': 670,\n",
       " 'explain': 671,\n",
       " 'nine': 672,\n",
       " 'words': 673,\n",
       " 'quit': 674,\n",
       " \"o'clock\": 675,\n",
       " 'la': 676,\n",
       " 'cop': 677,\n",
       " 'daughter': 678,\n",
       " 'piece': 679,\n",
       " 'terrible': 680,\n",
       " 'between': 681,\n",
       " 'learn': 682,\n",
       " 'key': 683,\n",
       " 'paul': 684,\n",
       " 'catch': 685,\n",
       " 'unless': 686,\n",
       " 'write': 687,\n",
       " 'ship': 688,\n",
       " 'cause': 689,\n",
       " 'handle': 690,\n",
       " 'fact': 691,\n",
       " 'peter': 692,\n",
       " 'feet': 693,\n",
       " 'happens': 694,\n",
       " 'chief': 695,\n",
       " 'twelve': 696,\n",
       " 'clothes': 697,\n",
       " 'evening': 698,\n",
       " 'lives': 699,\n",
       " 'small': 700,\n",
       " 'able': 701,\n",
       " 'ice': 702,\n",
       " 'wow': 703,\n",
       " 'major': 704,\n",
       " 'suit': 705,\n",
       " 'jimmy': 706,\n",
       " 'outta': 707,\n",
       " 'music': 708,\n",
       " 'straight': 709,\n",
       " 'nope': 710,\n",
       " 'missed': 711,\n",
       " 'ones': 712,\n",
       " 'yah': 713,\n",
       " 'anywhere': 714,\n",
       " 'throw': 715,\n",
       " 'favor': 716,\n",
       " 'wrote': 717,\n",
       " 'control': 718,\n",
       " 'david': 719,\n",
       " 'max': 720,\n",
       " 'except': 721,\n",
       " 'clear': 722,\n",
       " 'children': 723,\n",
       " 'doc': 724,\n",
       " 'sake': 725,\n",
       " 'plane': 726,\n",
       " 'tv': 727,\n",
       " 'kiss': 728,\n",
       " 'hotel': 729,\n",
       " 'bob': 730,\n",
       " 'pardon': 731,\n",
       " 'finished': 732,\n",
       " 'marry': 733,\n",
       " 'um': 734,\n",
       " 'asshole': 735,\n",
       " 'boss': 736,\n",
       " 'ring': 737,\n",
       " 'top': 738,\n",
       " 'quiet': 739,\n",
       " 'nervous': 740,\n",
       " 'store': 741,\n",
       " 'others': 742,\n",
       " 'james': 743,\n",
       " 'worked': 744,\n",
       " 'bye': 745,\n",
       " 'english': 746,\n",
       " 'ben': 747,\n",
       " 'afternoon': 748,\n",
       " 'yesterday': 749,\n",
       " 'known': 750,\n",
       " 'red': 751,\n",
       " 'less': 752,\n",
       " 'lord': 753,\n",
       " 'johnny': 754,\n",
       " 'hungry': 755,\n",
       " 'country': 756,\n",
       " 'drunk': 757,\n",
       " 'officer': 758,\n",
       " 'giving': 759,\n",
       " 'tough': 760,\n",
       " 'fucked': 761,\n",
       " 'interesting': 762,\n",
       " 'smart': 763,\n",
       " 'blue': 764,\n",
       " 'watching': 765,\n",
       " 'neither': 766,\n",
       " 'meant': 767,\n",
       " 'card': 768,\n",
       " 'michael': 769,\n",
       " 'christmas': 770,\n",
       " 'strange': 771,\n",
       " 'thinks': 772,\n",
       " 'goodnight': 773,\n",
       " 'welcome': 774,\n",
       " 'surprise': 775,\n",
       " 'road': 776,\n",
       " 'swear': 777,\n",
       " 'company': 778,\n",
       " 'fifteen': 779,\n",
       " 'felt': 780,\n",
       " 'doubt': 781,\n",
       " 'ray': 782,\n",
       " 'mistake': 783,\n",
       " 'smell': 784,\n",
       " 'dance': 785,\n",
       " 'liked': 786,\n",
       " 'state': 787,\n",
       " 'fair': 788,\n",
       " 'broke': 789,\n",
       " 'asleep': 790,\n",
       " 'talked': 791,\n",
       " 'mike': 792,\n",
       " 'likes': 793,\n",
       " 'taken': 794,\n",
       " 'boat': 795,\n",
       " 'bucks': 796,\n",
       " 'win': 797,\n",
       " 'works': 798,\n",
       " 'fall': 799,\n",
       " 'lying': 800,\n",
       " 'honor': 801,\n",
       " 'dress': 802,\n",
       " 'don�t': 803,\n",
       " 'problems': 804,\n",
       " 'past': 805,\n",
       " 'law': 806,\n",
       " 'fault': 807,\n",
       " 'personal': 808,\n",
       " 'american': 809,\n",
       " 'sign': 810,\n",
       " 'gimme': 811,\n",
       " 'bother': 812,\n",
       " 'aye': 813,\n",
       " 'wake': 814,\n",
       " 'honest': 815,\n",
       " 'human': 816,\n",
       " 'apartment': 817,\n",
       " 'figured': 818,\n",
       " 'takes': 819,\n",
       " 'blow': 820,\n",
       " 'beat': 821,\n",
       " 'owe': 822,\n",
       " 'birthday': 823,\n",
       " 'dick': 824,\n",
       " 'dunno': 825,\n",
       " 'mama': 826,\n",
       " 'island': 827,\n",
       " 'calls': 828,\n",
       " 'mad': 829,\n",
       " 'stick': 830,\n",
       " 'dark': 831,\n",
       " 'changed': 832,\n",
       " 'message': 833,\n",
       " 'charge': 834,\n",
       " 'busy': 835,\n",
       " 'tape': 836,\n",
       " 'uncle': 837,\n",
       " 'hide': 838,\n",
       " 'ran': 839,\n",
       " 'jim': 840,\n",
       " 'agent': 841,\n",
       " 'voice': 842,\n",
       " 'bastard': 843,\n",
       " 'named': 844,\n",
       " 'address': 845,\n",
       " 'short': 846,\n",
       " 'stopped': 847,\n",
       " 'missing': 848,\n",
       " 'sell': 849,\n",
       " 'totally': 850,\n",
       " 'listening': 851,\n",
       " 'ii': 852,\n",
       " 'sitting': 853,\n",
       " 'lots': 854,\n",
       " 'smoke': 855,\n",
       " 'class': 856,\n",
       " 'books': 857,\n",
       " 'private': 858,\n",
       " 'count': 859,\n",
       " 'bank': 860,\n",
       " 'notice': 861,\n",
       " 'staying': 862,\n",
       " 'south': 863,\n",
       " 'deep': 864,\n",
       " 'fool': 865,\n",
       " 'killer': 866,\n",
       " 'involved': 867,\n",
       " 'sid': 868,\n",
       " 'trip': 869,\n",
       " 'betty': 870,\n",
       " 'holy': 871,\n",
       " 'pictures': 872,\n",
       " 'rich': 873,\n",
       " 'lovely': 874,\n",
       " 'pass': 875,\n",
       " 'secret': 876,\n",
       " 'cops': 877,\n",
       " 'bathroom': 878,\n",
       " 'lawyer': 879,\n",
       " 'besides': 880,\n",
       " 'favorite': 881,\n",
       " 'forgot': 882,\n",
       " 'ought': 883,\n",
       " 'star': 884,\n",
       " 'bobby': 885,\n",
       " 'radio': 886,\n",
       " 'letter': 887,\n",
       " 'ted': 888,\n",
       " 'wearing': 889,\n",
       " 'window': 890,\n",
       " 'longer': 891,\n",
       " 'present': 892,\n",
       " 'turned': 893,\n",
       " 'caught': 894,\n",
       " 'list': 895,\n",
       " 'table': 896,\n",
       " 'ma': 897,\n",
       " 'information': 898,\n",
       " 'louis': 899,\n",
       " 'month': 900,\n",
       " 'jail': 901,\n",
       " 'college': 902,\n",
       " 'enjoy': 903,\n",
       " 'movies': 904,\n",
       " 'hardly': 905,\n",
       " 'rose': 906,\n",
       " \"everything's\": 907,\n",
       " 'lieutenant': 908,\n",
       " 'green': 909,\n",
       " 'de': 910,\n",
       " 'dangerous': 911,\n",
       " 'sweetheart': 912,\n",
       " 'crime': 913,\n",
       " 'difference': 914,\n",
       " 'plenty': 915,\n",
       " 'given': 916,\n",
       " 'completely': 917,\n",
       " 'cash': 918,\n",
       " 'silly': 919,\n",
       " 'truck': 920,\n",
       " 'fix': 921,\n",
       " 'weird': 922,\n",
       " 'needed': 923,\n",
       " 'history': 924,\n",
       " 'loved': 925,\n",
       " 'closed': 926,\n",
       " 'careful': 927,\n",
       " 'picked': 928,\n",
       " 'murder': 929,\n",
       " 'french': 930,\n",
       " 'forgive': 931,\n",
       " 'bought': 932,\n",
       " 'upstairs': 933,\n",
       " 'near': 934,\n",
       " 'usually': 935,\n",
       " 'reach': 936,\n",
       " 'security': 937,\n",
       " 'join': 938,\n",
       " 'grand': 939,\n",
       " 'joke': 940,\n",
       " 'bomb': 941,\n",
       " 'normal': 942,\n",
       " 'dying': 943,\n",
       " 'plans': 944,\n",
       " 'congratulations': 945,\n",
       " 'lived': 946,\n",
       " 'standing': 947,\n",
       " 'station': 948,\n",
       " 'bus': 949,\n",
       " 'girlfriend': 950,\n",
       " 'jerry': 951,\n",
       " 'nah': 952,\n",
       " 'boyfriend': 953,\n",
       " 'reading': 954,\n",
       " 'breakfast': 955,\n",
       " 'upset': 956,\n",
       " 'floor': 957,\n",
       " 'awful': 958,\n",
       " 'alex': 959,\n",
       " 'cost': 960,\n",
       " 'team': 961,\n",
       " 'beg': 962,\n",
       " 'heaven': 963,\n",
       " 'kinda': 964,\n",
       " 'record': 965,\n",
       " 'space': 966,\n",
       " 'finally': 967,\n",
       " 'eve': 968,\n",
       " 'bag': 969,\n",
       " 'taste': 970,\n",
       " 'colonel': 971,\n",
       " 'seconds': 972,\n",
       " 'wedding': 973,\n",
       " 'memory': 974,\n",
       " 'paid': 975,\n",
       " 'pleasure': 976,\n",
       " 'land': 977,\n",
       " 'amazing': 978,\n",
       " 'writing': 979,\n",
       " 'won': 980,\n",
       " 'killing': 981,\n",
       " 'type': 982,\n",
       " 'keys': 983,\n",
       " 'sleeping': 984,\n",
       " 'mention': 985,\n",
       " 'detective': 986,\n",
       " 'calm': 987,\n",
       " 'fat': 988,\n",
       " 'horse': 989,\n",
       " 'hmm': 990,\n",
       " 'driving': 991,\n",
       " 'fly': 992,\n",
       " 'ed': 993,\n",
       " 'mark': 994,\n",
       " 'paper': 995,\n",
       " 'tight': 996,\n",
       " 'department': 997,\n",
       " 'pain': 998,\n",
       " 'future': 999,\n",
       " 'billy': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'bos',\n",
       " 2: 'eos',\n",
       " 3: 'you',\n",
       " 4: 'i',\n",
       " 5: 'is',\n",
       " 6: 'the',\n",
       " 7: 'not',\n",
       " 8: 'to',\n",
       " 9: 'it',\n",
       " 10: 'a',\n",
       " 11: 'do',\n",
       " 12: 'that',\n",
       " 13: 'are',\n",
       " 14: 'what',\n",
       " 15: 'me',\n",
       " 16: 'have',\n",
       " 17: 'of',\n",
       " 18: 'and',\n",
       " 19: 'we',\n",
       " 20: 'in',\n",
       " 21: 'am',\n",
       " 22: 'he',\n",
       " 23: 'no',\n",
       " 24: 'this',\n",
       " 25: 'will',\n",
       " 26: 'your',\n",
       " 27: 'know',\n",
       " 28: 'for',\n",
       " 29: 'was',\n",
       " 30: 'my',\n",
       " 31: 'be',\n",
       " 32: 'on',\n",
       " 33: 'did',\n",
       " 34: 'just',\n",
       " 35: 'about',\n",
       " 36: 'they',\n",
       " 37: 'like',\n",
       " 38: 'would',\n",
       " 39: 'with',\n",
       " 40: 'how',\n",
       " 41: 'get',\n",
       " 42: 'all',\n",
       " 43: 'here',\n",
       " 44: 'but',\n",
       " 45: 'she',\n",
       " 46: 'so',\n",
       " 47: 'yes',\n",
       " 48: 'got',\n",
       " 49: 'want',\n",
       " 50: 'out',\n",
       " 51: 'well',\n",
       " 52: 'him',\n",
       " 53: 'why',\n",
       " 54: 'think',\n",
       " 55: 'yeah',\n",
       " 56: 'oh',\n",
       " 57: 'can',\n",
       " 58: 'right',\n",
       " 59: 'up',\n",
       " 60: 'go',\n",
       " 61: 'there',\n",
       " 62: 'if',\n",
       " 63: 'going',\n",
       " 64: 'at',\n",
       " 65: 'one',\n",
       " 66: 'where',\n",
       " 67: 'her',\n",
       " 68: 'now',\n",
       " 69: 'see',\n",
       " 70: 'good',\n",
       " 71: 'cannot',\n",
       " 72: 'who',\n",
       " 73: 'come',\n",
       " 74: 'tell',\n",
       " 75: 'say',\n",
       " 76: 'were',\n",
       " 77: 'could',\n",
       " 78: 'then',\n",
       " 79: 'time',\n",
       " 80: 'okay',\n",
       " 81: 'back',\n",
       " 82: 'from',\n",
       " 83: 'been',\n",
       " 84: 'some',\n",
       " 85: 'mean',\n",
       " 86: 'when',\n",
       " 87: 'look',\n",
       " 88: 'an',\n",
       " 89: 'take',\n",
       " 90: 'something',\n",
       " 91: 'them',\n",
       " 92: 'sure',\n",
       " 93: 'his',\n",
       " 94: 'us',\n",
       " 95: 'does',\n",
       " 96: 'really',\n",
       " 97: 'too',\n",
       " 98: 'or',\n",
       " 99: 'as',\n",
       " 100: 'never',\n",
       " 101: 'man',\n",
       " 102: 'had',\n",
       " 103: 'should',\n",
       " 104: 'said',\n",
       " 105: 'doing',\n",
       " 106: 'need',\n",
       " 107: 'way',\n",
       " 108: 'maybe',\n",
       " 109: 'mr',\n",
       " 110: 'sorry',\n",
       " 111: 'very',\n",
       " 112: 'little',\n",
       " 113: 'any',\n",
       " 114: 'talk',\n",
       " 115: 'over',\n",
       " 116: 'make',\n",
       " 117: 'gonna',\n",
       " 118: 'down',\n",
       " 119: 'sir',\n",
       " 120: 'more',\n",
       " 121: 'nothing',\n",
       " 122: 'let',\n",
       " 123: \"there's\",\n",
       " 124: 'hey',\n",
       " 125: 'much',\n",
       " 126: 'off',\n",
       " 127: 'anything',\n",
       " 128: 'two',\n",
       " 129: 'thought',\n",
       " 130: 'give',\n",
       " 131: 'call',\n",
       " 132: 'has',\n",
       " 133: 'please',\n",
       " 134: 'still',\n",
       " 135: 'love',\n",
       " 136: 'thing',\n",
       " 137: 'only',\n",
       " 138: 'our',\n",
       " 139: 'by',\n",
       " 140: \"let's\",\n",
       " 141: 'work',\n",
       " 142: 'told',\n",
       " 143: 'people',\n",
       " 144: 'night',\n",
       " 145: 'must',\n",
       " 146: 'long',\n",
       " 147: 'because',\n",
       " 148: 'find',\n",
       " 149: 'ever',\n",
       " 150: 'name',\n",
       " 151: 'thank',\n",
       " 152: 'better',\n",
       " 153: 'help',\n",
       " 154: 'last',\n",
       " 155: 'hell',\n",
       " 156: 'fine',\n",
       " 157: 'again',\n",
       " 158: 'shit',\n",
       " 159: 'god',\n",
       " 160: 'around',\n",
       " 161: 'great',\n",
       " 162: 'money',\n",
       " 163: 'believe',\n",
       " 164: 'before',\n",
       " 165: 'huh',\n",
       " 166: 'talking',\n",
       " 167: 'guess',\n",
       " 168: 'fucking',\n",
       " 169: 'wait',\n",
       " 170: 'thanks',\n",
       " 171: 'fuck',\n",
       " 172: 'even',\n",
       " 173: 'course',\n",
       " 174: 'home',\n",
       " 175: 'dead',\n",
       " 176: 'things',\n",
       " 177: 'other',\n",
       " 178: 'wrong',\n",
       " 179: 'always',\n",
       " 180: 'uh',\n",
       " 181: 'nice',\n",
       " 182: 'first',\n",
       " 183: 'leave',\n",
       " 184: 'put',\n",
       " 185: 'old',\n",
       " 186: 'those',\n",
       " 187: 'day',\n",
       " 188: 'happened',\n",
       " 189: 'these',\n",
       " 190: 'bad',\n",
       " 191: 'keep',\n",
       " 192: 'gotta',\n",
       " 193: 'ask',\n",
       " 194: 'kind',\n",
       " 195: 'away',\n",
       " 196: 'place',\n",
       " 197: 'than',\n",
       " 198: 'remember',\n",
       " 199: 'hear',\n",
       " 200: 'new',\n",
       " 201: 'after',\n",
       " 202: 'guy',\n",
       " 203: 'into',\n",
       " 204: 'ai',\n",
       " 205: 'kill',\n",
       " 206: 'feel',\n",
       " 207: 'stop',\n",
       " 208: 'girl',\n",
       " 209: 'big',\n",
       " 210: 'years',\n",
       " 211: 'coming',\n",
       " 212: 'three',\n",
       " 213: 'real',\n",
       " 214: 'mind',\n",
       " 215: 'life',\n",
       " 216: 'might',\n",
       " 217: 'getting',\n",
       " 218: 'stay',\n",
       " 219: 'else',\n",
       " 220: 'hi',\n",
       " 221: 'lot',\n",
       " 222: 'enough',\n",
       " 223: 'hello',\n",
       " 224: 'another',\n",
       " 225: 'everything',\n",
       " 226: 'miss',\n",
       " 227: 'understand',\n",
       " 228: 'friend',\n",
       " 229: 'made',\n",
       " 230: 'tonight',\n",
       " 231: 'looking',\n",
       " 232: 'try',\n",
       " 233: 'mother',\n",
       " 234: 'left',\n",
       " 235: 'saw',\n",
       " 236: 'done',\n",
       " 237: 'ya',\n",
       " 238: 'trying',\n",
       " 239: 'heard',\n",
       " 240: 'yourself',\n",
       " 241: 'may',\n",
       " 242: 'house',\n",
       " 243: 'car',\n",
       " 244: 'wanted',\n",
       " 245: 'care',\n",
       " 246: 'someone',\n",
       " 247: 'boy',\n",
       " 248: 'morning',\n",
       " 249: 'room',\n",
       " 250: 'pretty',\n",
       " 251: 'listen',\n",
       " 252: 'yet',\n",
       " 253: 'idea',\n",
       " 254: 'baby',\n",
       " 255: 'father',\n",
       " 256: 'tomorrow',\n",
       " 257: \"who's\",\n",
       " 258: 'came',\n",
       " 259: 'show',\n",
       " 260: 'seen',\n",
       " 261: 'their',\n",
       " 262: 'own',\n",
       " 263: 'dad',\n",
       " 264: 'knew',\n",
       " 265: 'went',\n",
       " 266: 'jesus',\n",
       " 267: 'job',\n",
       " 268: 'play',\n",
       " 269: 'called',\n",
       " 270: 'through',\n",
       " 271: 'many',\n",
       " 272: 'five',\n",
       " 273: 'killed',\n",
       " 274: 'shut',\n",
       " 275: 'business',\n",
       " 276: 'live',\n",
       " 277: 'already',\n",
       " 278: 'found',\n",
       " 279: 'best',\n",
       " 280: 'today',\n",
       " 281: 'use',\n",
       " 282: 'being',\n",
       " 283: 'together',\n",
       " 284: \"'em\",\n",
       " 285: 'matter',\n",
       " 286: 'looks',\n",
       " 287: 'alright',\n",
       " 288: 'meet',\n",
       " 289: 'afraid',\n",
       " 290: 'same',\n",
       " 291: 'probably',\n",
       " 292: 'damn',\n",
       " 293: 'problem',\n",
       " 294: 'next',\n",
       " 295: 'guys',\n",
       " 296: 'gone',\n",
       " 297: 'saying',\n",
       " 298: 'late',\n",
       " 299: 'minute',\n",
       " 300: 'supposed',\n",
       " 301: 'mrs',\n",
       " 302: 'used',\n",
       " 303: 'start',\n",
       " 304: 'alone',\n",
       " 305: 'stuff',\n",
       " 306: 'wife',\n",
       " 307: 'took',\n",
       " 308: 'myself',\n",
       " 309: 'son',\n",
       " 310: 'every',\n",
       " 311: 'worry',\n",
       " 312: 'friends',\n",
       " 313: 'which',\n",
       " 314: 'mom',\n",
       " 315: 'wanna',\n",
       " 316: 'four',\n",
       " 317: 'forget',\n",
       " 318: 'few',\n",
       " 319: 'minutes',\n",
       " 320: 'school',\n",
       " 321: 'phone',\n",
       " 322: 'drink',\n",
       " 323: 'wants',\n",
       " 324: 'hundred',\n",
       " 325: 'exactly',\n",
       " 326: 'hope',\n",
       " 327: 'hurt',\n",
       " 328: 'die',\n",
       " 329: 'woman',\n",
       " 330: 'run',\n",
       " 331: 'beautiful',\n",
       " 332: 'until',\n",
       " 333: 'somebody',\n",
       " 334: 'without',\n",
       " 335: 'ready',\n",
       " 336: 'ten',\n",
       " 337: 'watch',\n",
       " 338: 'ago',\n",
       " 339: 'anyway',\n",
       " 340: 'men',\n",
       " 341: 'thinking',\n",
       " 342: 'world',\n",
       " 343: 'open',\n",
       " 344: 'crazy',\n",
       " 345: 'married',\n",
       " 346: 'hate',\n",
       " 347: 'once',\n",
       " 348: 'sit',\n",
       " 349: 'check',\n",
       " 350: 'whole',\n",
       " 351: 'funny',\n",
       " 352: 'working',\n",
       " 353: 'says',\n",
       " 354: 'word',\n",
       " 355: 'lost',\n",
       " 356: 'later',\n",
       " 357: 'none',\n",
       " 358: 'while',\n",
       " 359: 'brother',\n",
       " 360: 'hard',\n",
       " 361: 'bring',\n",
       " 362: 'happy',\n",
       " 363: 'quite',\n",
       " 364: 'honey',\n",
       " 365: 'sleep',\n",
       " 366: 'hold',\n",
       " 367: 'head',\n",
       " 368: 'hit',\n",
       " 369: 'true',\n",
       " 370: 'doctor',\n",
       " 371: 'mine',\n",
       " 372: 'kid',\n",
       " 373: 'gun',\n",
       " 374: 'deal',\n",
       " 375: 'part',\n",
       " 376: 'least',\n",
       " 377: 'jack',\n",
       " 378: 'taking',\n",
       " 379: 'week',\n",
       " 380: 'anyone',\n",
       " 381: 'actually',\n",
       " 382: 'point',\n",
       " 383: 'such',\n",
       " 384: 'question',\n",
       " 385: 'door',\n",
       " 386: 'john',\n",
       " 387: 'read',\n",
       " 388: 'move',\n",
       " 389: 'turn',\n",
       " 390: 'captain',\n",
       " 391: 'half',\n",
       " 392: 'ok',\n",
       " 393: 'kids',\n",
       " 394: 'nobody',\n",
       " 395: 'whatever',\n",
       " 396: 'dr',\n",
       " 397: 'days',\n",
       " 398: 'ah',\n",
       " 399: 'both',\n",
       " 400: 'party',\n",
       " 401: 'anymore',\n",
       " 402: 'thousand',\n",
       " 403: 'suppose',\n",
       " 404: 'having',\n",
       " 405: 'year',\n",
       " 406: 'wish',\n",
       " 407: 'soon',\n",
       " 408: 'bet',\n",
       " 409: 'makes',\n",
       " 410: 'sick',\n",
       " 411: 'eat',\n",
       " 412: 'second',\n",
       " 413: 'under',\n",
       " 414: 'knows',\n",
       " 415: 'important',\n",
       " 416: 'drive',\n",
       " 417: 'yours',\n",
       " 418: 'since',\n",
       " 419: 'telling',\n",
       " 420: 'most',\n",
       " 421: 'easy',\n",
       " 422: 'asked',\n",
       " 423: 'case',\n",
       " 424: 'close',\n",
       " 425: 'far',\n",
       " 426: 'serious',\n",
       " 427: 'shot',\n",
       " 428: 'dollars',\n",
       " 429: 'police',\n",
       " 430: 'cut',\n",
       " 431: 'trust',\n",
       " 432: 'times',\n",
       " 433: 'family',\n",
       " 434: 'frank',\n",
       " 435: 'excuse',\n",
       " 436: 'story',\n",
       " 437: 'making',\n",
       " 438: 'chance',\n",
       " 439: 'anybody',\n",
       " 440: 'happen',\n",
       " 441: 'six',\n",
       " 442: 'ass',\n",
       " 443: 'met',\n",
       " 444: 'pay',\n",
       " 445: 'answer',\n",
       " 446: 'walk',\n",
       " 447: 'change',\n",
       " 448: 'rest',\n",
       " 449: 'bed',\n",
       " 450: 'young',\n",
       " 451: 'end',\n",
       " 452: 'send',\n",
       " 453: 'bit',\n",
       " 454: 'town',\n",
       " 455: 'number',\n",
       " 456: 'gave',\n",
       " 457: 'sometimes',\n",
       " 458: 'waiting',\n",
       " 459: 'died',\n",
       " 460: 'asking',\n",
       " 461: 'kidding',\n",
       " 462: 'water',\n",
       " 463: 'different',\n",
       " 464: 'though',\n",
       " 465: 'everyone',\n",
       " 466: 'truth',\n",
       " 467: 'office',\n",
       " 468: 'almost',\n",
       " 469: 'set',\n",
       " 470: 'sounds',\n",
       " 471: 'game',\n",
       " 472: 'cool',\n",
       " 473: 'george',\n",
       " 474: 'everybody',\n",
       " 475: 'couple',\n",
       " 476: 'daddy',\n",
       " 477: 'shoot',\n",
       " 478: 'hour',\n",
       " 479: 'feeling',\n",
       " 480: 'inside',\n",
       " 481: 'twenty',\n",
       " 482: 'either',\n",
       " 483: 'pick',\n",
       " 484: 'christ',\n",
       " 485: 'heart',\n",
       " 486: 'tried',\n",
       " 487: 'face',\n",
       " 488: 'each',\n",
       " 489: 'promise',\n",
       " 490: 'trouble',\n",
       " 491: 'blood',\n",
       " 492: 'dinner',\n",
       " 493: 'gets',\n",
       " 494: 'harry',\n",
       " 495: 'alive',\n",
       " 496: 'sex',\n",
       " 497: \"c'mon\",\n",
       " 498: 'high',\n",
       " 499: 'means',\n",
       " 500: 'stupid',\n",
       " 501: 'eight',\n",
       " 502: 'eyes',\n",
       " 503: 'power',\n",
       " 504: 'book',\n",
       " 505: 'running',\n",
       " 506: 'luck',\n",
       " 507: 'hand',\n",
       " 508: 'war',\n",
       " 509: 'lucky',\n",
       " 510: 'stand',\n",
       " 511: 'reason',\n",
       " 512: 'death',\n",
       " 513: 'perhaps',\n",
       " 514: 'scared',\n",
       " 515: 'glad',\n",
       " 516: 'shall',\n",
       " 517: 'calling',\n",
       " 518: 'full',\n",
       " 519: 'playing',\n",
       " 520: 'goddamn',\n",
       " 521: 'leaving',\n",
       " 522: 'mary',\n",
       " 523: 'husband',\n",
       " 524: 'uhhuh',\n",
       " 525: 'break',\n",
       " 526: 'months',\n",
       " 527: 'outside',\n",
       " 528: 'dear',\n",
       " 529: 'hours',\n",
       " 530: 'safe',\n",
       " 531: 'hot',\n",
       " 532: 'buy',\n",
       " 533: 'against',\n",
       " 534: 'seem',\n",
       " 535: 'sound',\n",
       " 536: 'speak',\n",
       " 537: 'boys',\n",
       " 538: 'special',\n",
       " 539: 'behind',\n",
       " 540: 'lie',\n",
       " 541: 'white',\n",
       " 542: 'person',\n",
       " 543: 'black',\n",
       " 544: 'goes',\n",
       " 545: 'goodbye',\n",
       " 546: 'sam',\n",
       " 547: 'dream',\n",
       " 548: 'side',\n",
       " 549: 'fire',\n",
       " 550: 'hang',\n",
       " 551: 'fun',\n",
       " 552: 'york',\n",
       " 553: 'light',\n",
       " 554: 'needs',\n",
       " 555: 'possible',\n",
       " 556: 'eh',\n",
       " 557: 'questions',\n",
       " 558: 'lose',\n",
       " 559: 'rather',\n",
       " 560: 'hair',\n",
       " 561: 'along',\n",
       " 562: 'early',\n",
       " 563: 'sort',\n",
       " 564: 'figure',\n",
       " 565: 'save',\n",
       " 566: 'million',\n",
       " 567: 'sent',\n",
       " 568: 'movie',\n",
       " 569: 'ride',\n",
       " 570: 'street',\n",
       " 571: 'mouth',\n",
       " 572: 'dog',\n",
       " 573: 'free',\n",
       " 574: 'started',\n",
       " 575: 'hands',\n",
       " 576: 'thirty',\n",
       " 577: 'darling',\n",
       " 578: 'ahead',\n",
       " 579: 'drop',\n",
       " 580: 'moving',\n",
       " 581: 'living',\n",
       " 582: 'worse',\n",
       " 583: 'nick',\n",
       " 584: 'wonder',\n",
       " 585: \"'cause\",\n",
       " 586: 'line',\n",
       " 587: 'hurry',\n",
       " 588: 'somewhere',\n",
       " 589: 'brought',\n",
       " 590: 'wonderful',\n",
       " 591: 'body',\n",
       " 592: 'bullshit',\n",
       " 593: 'women',\n",
       " 594: 'food',\n",
       " 595: 'worth',\n",
       " 596: 'girls',\n",
       " 597: 'tired',\n",
       " 598: 'miles',\n",
       " 599: 'certainly',\n",
       " 600: 'date',\n",
       " 601: 'coffee',\n",
       " 602: 'buddy',\n",
       " 603: 'sweet',\n",
       " 604: 'eddie',\n",
       " 605: 'charlie',\n",
       " 606: 'worried',\n",
       " 607: 'moment',\n",
       " 608: 'interested',\n",
       " 609: 'bitch',\n",
       " 610: 'news',\n",
       " 611: \"here's\",\n",
       " 612: 'city',\n",
       " 613: 'seven',\n",
       " 614: 'president',\n",
       " 615: 'seems',\n",
       " 616: 'comes',\n",
       " 617: 'touch',\n",
       " 618: 'its',\n",
       " 619: 'front',\n",
       " 620: 'mister',\n",
       " 621: 'also',\n",
       " 622: 'lady',\n",
       " 623: 'sister',\n",
       " 624: 'ha',\n",
       " 625: 'air',\n",
       " 626: 'fast',\n",
       " 627: 'perfect',\n",
       " 628: 'expect',\n",
       " 629: 'till',\n",
       " 630: 'walter',\n",
       " 631: 'himself',\n",
       " 632: 'pull',\n",
       " 633: 'happening',\n",
       " 634: 'wear',\n",
       " 635: 'plan',\n",
       " 636: 'fifty',\n",
       " 637: 'weeks',\n",
       " 638: 'meeting',\n",
       " 639: 'cold',\n",
       " 640: 'relax',\n",
       " 641: 'king',\n",
       " 642: 'parents',\n",
       " 643: 'hospital',\n",
       " 644: 'general',\n",
       " 645: 'seeing',\n",
       " 646: 'sense',\n",
       " 647: 'act',\n",
       " 648: 'dude',\n",
       " 649: 'finish',\n",
       " 650: 'lunch',\n",
       " 651: \"ma'am\",\n",
       " 652: 'child',\n",
       " 653: 'absolutely',\n",
       " 654: 'poor',\n",
       " 655: 'joe',\n",
       " 656: 'tom',\n",
       " 657: 'bill',\n",
       " 658: 'clean',\n",
       " 659: 'fight',\n",
       " 660: 'report',\n",
       " 661: 'beer',\n",
       " 662: 'earth',\n",
       " 663: \"name's\",\n",
       " 664: 'follow',\n",
       " 665: 'choice',\n",
       " 666: 'looked',\n",
       " 667: 'accident',\n",
       " 668: 'order',\n",
       " 669: 'picture',\n",
       " 670: '�',\n",
       " 671: 'explain',\n",
       " 672: 'nine',\n",
       " 673: 'words',\n",
       " 674: 'quit',\n",
       " 675: \"o'clock\",\n",
       " 676: 'la',\n",
       " 677: 'cop',\n",
       " 678: 'daughter',\n",
       " 679: 'piece',\n",
       " 680: 'terrible',\n",
       " 681: 'between',\n",
       " 682: 'learn',\n",
       " 683: 'key',\n",
       " 684: 'paul',\n",
       " 685: 'catch',\n",
       " 686: 'unless',\n",
       " 687: 'write',\n",
       " 688: 'ship',\n",
       " 689: 'cause',\n",
       " 690: 'handle',\n",
       " 691: 'fact',\n",
       " 692: 'peter',\n",
       " 693: 'feet',\n",
       " 694: 'happens',\n",
       " 695: 'chief',\n",
       " 696: 'twelve',\n",
       " 697: 'clothes',\n",
       " 698: 'evening',\n",
       " 699: 'lives',\n",
       " 700: 'small',\n",
       " 701: 'able',\n",
       " 702: 'ice',\n",
       " 703: 'wow',\n",
       " 704: 'major',\n",
       " 705: 'suit',\n",
       " 706: 'jimmy',\n",
       " 707: 'outta',\n",
       " 708: 'music',\n",
       " 709: 'straight',\n",
       " 710: 'nope',\n",
       " 711: 'missed',\n",
       " 712: 'ones',\n",
       " 713: 'yah',\n",
       " 714: 'anywhere',\n",
       " 715: 'throw',\n",
       " 716: 'favor',\n",
       " 717: 'wrote',\n",
       " 718: 'control',\n",
       " 719: 'david',\n",
       " 720: 'max',\n",
       " 721: 'except',\n",
       " 722: 'clear',\n",
       " 723: 'children',\n",
       " 724: 'doc',\n",
       " 725: 'sake',\n",
       " 726: 'plane',\n",
       " 727: 'tv',\n",
       " 728: 'kiss',\n",
       " 729: 'hotel',\n",
       " 730: 'bob',\n",
       " 731: 'pardon',\n",
       " 732: 'finished',\n",
       " 733: 'marry',\n",
       " 734: 'um',\n",
       " 735: 'asshole',\n",
       " 736: 'boss',\n",
       " 737: 'ring',\n",
       " 738: 'top',\n",
       " 739: 'quiet',\n",
       " 740: 'nervous',\n",
       " 741: 'store',\n",
       " 742: 'others',\n",
       " 743: 'james',\n",
       " 744: 'worked',\n",
       " 745: 'bye',\n",
       " 746: 'english',\n",
       " 747: 'ben',\n",
       " 748: 'afternoon',\n",
       " 749: 'yesterday',\n",
       " 750: 'known',\n",
       " 751: 'red',\n",
       " 752: 'less',\n",
       " 753: 'lord',\n",
       " 754: 'johnny',\n",
       " 755: 'hungry',\n",
       " 756: 'country',\n",
       " 757: 'drunk',\n",
       " 758: 'officer',\n",
       " 759: 'giving',\n",
       " 760: 'tough',\n",
       " 761: 'fucked',\n",
       " 762: 'interesting',\n",
       " 763: 'smart',\n",
       " 764: 'blue',\n",
       " 765: 'watching',\n",
       " 766: 'neither',\n",
       " 767: 'meant',\n",
       " 768: 'card',\n",
       " 769: 'michael',\n",
       " 770: 'christmas',\n",
       " 771: 'strange',\n",
       " 772: 'thinks',\n",
       " 773: 'goodnight',\n",
       " 774: 'welcome',\n",
       " 775: 'surprise',\n",
       " 776: 'road',\n",
       " 777: 'swear',\n",
       " 778: 'company',\n",
       " 779: 'fifteen',\n",
       " 780: 'felt',\n",
       " 781: 'doubt',\n",
       " 782: 'ray',\n",
       " 783: 'mistake',\n",
       " 784: 'smell',\n",
       " 785: 'dance',\n",
       " 786: 'liked',\n",
       " 787: 'state',\n",
       " 788: 'fair',\n",
       " 789: 'broke',\n",
       " 790: 'asleep',\n",
       " 791: 'talked',\n",
       " 792: 'mike',\n",
       " 793: 'likes',\n",
       " 794: 'taken',\n",
       " 795: 'boat',\n",
       " 796: 'bucks',\n",
       " 797: 'win',\n",
       " 798: 'works',\n",
       " 799: 'fall',\n",
       " 800: 'lying',\n",
       " 801: 'honor',\n",
       " 802: 'dress',\n",
       " 803: 'don�t',\n",
       " 804: 'problems',\n",
       " 805: 'past',\n",
       " 806: 'law',\n",
       " 807: 'fault',\n",
       " 808: 'personal',\n",
       " 809: 'american',\n",
       " 810: 'sign',\n",
       " 811: 'gimme',\n",
       " 812: 'bother',\n",
       " 813: 'aye',\n",
       " 814: 'wake',\n",
       " 815: 'honest',\n",
       " 816: 'human',\n",
       " 817: 'apartment',\n",
       " 818: 'figured',\n",
       " 819: 'takes',\n",
       " 820: 'blow',\n",
       " 821: 'beat',\n",
       " 822: 'owe',\n",
       " 823: 'birthday',\n",
       " 824: 'dick',\n",
       " 825: 'dunno',\n",
       " 826: 'mama',\n",
       " 827: 'island',\n",
       " 828: 'calls',\n",
       " 829: 'mad',\n",
       " 830: 'stick',\n",
       " 831: 'dark',\n",
       " 832: 'changed',\n",
       " 833: 'message',\n",
       " 834: 'charge',\n",
       " 835: 'busy',\n",
       " 836: 'tape',\n",
       " 837: 'uncle',\n",
       " 838: 'hide',\n",
       " 839: 'ran',\n",
       " 840: 'jim',\n",
       " 841: 'agent',\n",
       " 842: 'voice',\n",
       " 843: 'bastard',\n",
       " 844: 'named',\n",
       " 845: 'address',\n",
       " 846: 'short',\n",
       " 847: 'stopped',\n",
       " 848: 'missing',\n",
       " 849: 'sell',\n",
       " 850: 'totally',\n",
       " 851: 'listening',\n",
       " 852: 'ii',\n",
       " 853: 'sitting',\n",
       " 854: 'lots',\n",
       " 855: 'smoke',\n",
       " 856: 'class',\n",
       " 857: 'books',\n",
       " 858: 'private',\n",
       " 859: 'count',\n",
       " 860: 'bank',\n",
       " 861: 'notice',\n",
       " 862: 'staying',\n",
       " 863: 'south',\n",
       " 864: 'deep',\n",
       " 865: 'fool',\n",
       " 866: 'killer',\n",
       " 867: 'involved',\n",
       " 868: 'sid',\n",
       " 869: 'trip',\n",
       " 870: 'betty',\n",
       " 871: 'holy',\n",
       " 872: 'pictures',\n",
       " 873: 'rich',\n",
       " 874: 'lovely',\n",
       " 875: 'pass',\n",
       " 876: 'secret',\n",
       " 877: 'cops',\n",
       " 878: 'bathroom',\n",
       " 879: 'lawyer',\n",
       " 880: 'besides',\n",
       " 881: 'favorite',\n",
       " 882: 'forgot',\n",
       " 883: 'ought',\n",
       " 884: 'star',\n",
       " 885: 'bobby',\n",
       " 886: 'radio',\n",
       " 887: 'letter',\n",
       " 888: 'ted',\n",
       " 889: 'wearing',\n",
       " 890: 'window',\n",
       " 891: 'longer',\n",
       " 892: 'present',\n",
       " 893: 'turned',\n",
       " 894: 'caught',\n",
       " 895: 'list',\n",
       " 896: 'table',\n",
       " 897: 'ma',\n",
       " 898: 'information',\n",
       " 899: 'louis',\n",
       " 900: 'month',\n",
       " 901: 'jail',\n",
       " 902: 'college',\n",
       " 903: 'enjoy',\n",
       " 904: 'movies',\n",
       " 905: 'hardly',\n",
       " 906: 'rose',\n",
       " 907: \"everything's\",\n",
       " 908: 'lieutenant',\n",
       " 909: 'green',\n",
       " 910: 'de',\n",
       " 911: 'dangerous',\n",
       " 912: 'sweetheart',\n",
       " 913: 'crime',\n",
       " 914: 'difference',\n",
       " 915: 'plenty',\n",
       " 916: 'given',\n",
       " 917: 'completely',\n",
       " 918: 'cash',\n",
       " 919: 'silly',\n",
       " 920: 'truck',\n",
       " 921: 'fix',\n",
       " 922: 'weird',\n",
       " 923: 'needed',\n",
       " 924: 'history',\n",
       " 925: 'loved',\n",
       " 926: 'closed',\n",
       " 927: 'careful',\n",
       " 928: 'picked',\n",
       " 929: 'murder',\n",
       " 930: 'french',\n",
       " 931: 'forgive',\n",
       " 932: 'bought',\n",
       " 933: 'upstairs',\n",
       " 934: 'near',\n",
       " 935: 'usually',\n",
       " 936: 'reach',\n",
       " 937: 'security',\n",
       " 938: 'join',\n",
       " 939: 'grand',\n",
       " 940: 'joke',\n",
       " 941: 'bomb',\n",
       " 942: 'normal',\n",
       " 943: 'dying',\n",
       " 944: 'plans',\n",
       " 945: 'congratulations',\n",
       " 946: 'lived',\n",
       " 947: 'standing',\n",
       " 948: 'station',\n",
       " 949: 'bus',\n",
       " 950: 'girlfriend',\n",
       " 951: 'jerry',\n",
       " 952: 'nah',\n",
       " 953: 'boyfriend',\n",
       " 954: 'reading',\n",
       " 955: 'breakfast',\n",
       " 956: 'upset',\n",
       " 957: 'floor',\n",
       " 958: 'awful',\n",
       " 959: 'alex',\n",
       " 960: 'cost',\n",
       " 961: 'team',\n",
       " 962: 'beg',\n",
       " 963: 'heaven',\n",
       " 964: 'kinda',\n",
       " 965: 'record',\n",
       " 966: 'space',\n",
       " 967: 'finally',\n",
       " 968: 'eve',\n",
       " 969: 'bag',\n",
       " 970: 'taste',\n",
       " 971: 'colonel',\n",
       " 972: 'seconds',\n",
       " 973: 'wedding',\n",
       " 974: 'memory',\n",
       " 975: 'paid',\n",
       " 976: 'pleasure',\n",
       " 977: 'land',\n",
       " 978: 'amazing',\n",
       " 979: 'writing',\n",
       " 980: 'won',\n",
       " 981: 'killing',\n",
       " 982: 'type',\n",
       " 983: 'keys',\n",
       " 984: 'sleeping',\n",
       " 985: 'mention',\n",
       " 986: 'detective',\n",
       " 987: 'calm',\n",
       " 988: 'fat',\n",
       " 989: 'horse',\n",
       " 990: 'hmm',\n",
       " 991: 'driving',\n",
       " 992: 'fly',\n",
       " 993: 'ed',\n",
       " 994: 'mark',\n",
       " 995: 'paper',\n",
       " 996: 'tight',\n",
       " 997: 'department',\n",
       " 998: 'pain',\n",
       " 999: 'future',\n",
       " 1000: 'billy',\n",
       " ...}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an index for the padder \n",
    "idx2word[0]=' '\n",
    "word2idx[' ']= 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2seq(encoder_text, decoder_text, VOCAB_SIZE):\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "    tokenizer.fit_on_texts(encoder_text+decoder_text)\n",
    "    encoder_sequences = tokenizer.texts_to_sequences(encoder_text)\n",
    "    decoder_sequences = tokenizer.texts_to_sequences(decoder_text)\n",
    "  \n",
    "    return encoder_sequences, decoder_sequences\n",
    "\n",
    "encoder_sequences, decoder_sequences = text2seq(short_questions, tagged_answers, VOCAB_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[43, 68, 14, 13, 3, 2145, 8, 11],\n",
       " [9, 5, 42, 30, 807, 4, 21, 46, 110, 1999],\n",
       " [4, 102, 9, 211, 82, 246],\n",
       " [192, 60, 39, 6, 2333, 61],\n",
       " [4, 302, 8, 60, 39, 6, 4682],\n",
       " [6, 4682, 53],\n",
       " [347, 19, 2611, 1329, 12, 5, 9, 3584, 5, 1247],\n",
       " [30, 442],\n",
       " [66, 11, 19, 288],\n",
       " [9, 5, 10, 181, 187, 40, 35, 6, 1302]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_sequences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 16, 10, 87, 64, 121, 2],\n",
       " [1, 4, 102, 9, 211, 82, 246, 2],\n",
       " [1, 13, 3, 92, 4, 103, 7, 41, 10, 370, 2],\n",
       " [1, 4, 302, 8, 60, 39, 6, 4682, 2],\n",
       " [1, 6, 4682, 53, 2],\n",
       " [1, 689, 171, 52, 12, 5, 53, 2],\n",
       " [1, 24, 5, 378, 1132, 4, 21, 2591, 2],\n",
       " [1, 55, 12, 5, 988, 97, 2],\n",
       " [1, 9, 5, 10, 181, 187, 40, 35, 6, 1302, 2],\n",
       " [1, 875, 2]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_sequences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the padding is 2 characters longer than max_line_length to account for the <BOS> and <EOS> tags\n",
    "\n",
    "def padding(encoder_sequences, decoder_sequences, MAX_LEN):\n",
    "  \n",
    "    encoder_input_data = pad_sequences(encoder_sequences, maxlen=MAX_LEN, dtype='int32', padding='post', truncating='post')\n",
    "    decoder_input_data = pad_sequences(decoder_sequences, maxlen=MAX_LEN, dtype='int32', padding='post', truncating='post')\n",
    "    decoder_output_data = pad_sequences(decoder_sequences, maxlen=MAX_LEN+1, dtype='int32', padding='post', truncating='post')\n",
    "    return encoder_input_data, decoder_input_data, decoder_output_data\n",
    "\n",
    "encoder_padded_seq, decoder_padded_seq, decoder_padded_seq_1 = padding(encoder_sequences, decoder_sequences, MAX_LEN=max_line_length+2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56571, 20)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_padded_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56571, 20)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_padded_seq_1[:,1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56571, 20)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_padded_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = encoder_padded_seq\n",
    "decoder_input_data = decoder_padded_seq\n",
    "decoder_output_data = to_categorical(decoder_padded_seq_1[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-665e446816f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0men_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0men_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0men_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mde_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mde_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mde_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0men_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0men_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0men_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mde_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mde_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mde_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_splitter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_input_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_input_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_output_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-665e446816f2>\u001b[0m in \u001b[0;36mdata_splitter\u001b[1;34m(encoder_input_data, decoder_input_data, decoder_output_data, test_size1, test_size2)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdata_splitter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_input_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_input_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_output_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0men_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0men_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mde_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mde_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_input_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_input_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_output_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_size1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0men_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0men_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mde_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mde_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mde_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_size2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2145\u001b[0m     return list(chain.from_iterable((_safe_indexing(a, train),\n\u001b[1;32m-> 2146\u001b[1;33m                                      _safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[0;32m   2147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2145\u001b[0m     return list(chain.from_iterable((_safe_indexing(a, train),\n\u001b[1;32m-> 2146\u001b[1;33m                                      _safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[0;32m   2147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_pandas_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_list_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_array_indexing\u001b[1;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def data_splitter(encoder_input_data, decoder_input_data, decoder_output_data, test_size1=0.2, test_size2=0.3):\n",
    "  \n",
    "    en_train, en_test, de_train, de_test, out_train, out_test = train_test_split(encoder_input_data, decoder_input_data, decoder_output_data, test_size=test_size1)\n",
    "    en_train, en_val, de_train, de_val, out_train, out_val = train_test_split(en_train, de_train, out_train, test_size=test_size2)\n",
    "  \n",
    "    return en_train, en_val, en_test, de_train, de_val, de_test, out_train, out_val, out_test\n",
    "\n",
    "en_train, en_val, en_test, de_train, de_val, de_test, out_train, out_val, out_test = data_splitter(encoder_input_data, decoder_input_data, decoder_output_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model_builder(HIDDEN_DIM=300):\n",
    "    \n",
    "    encoder_inputs = Input(shape=(None,))\n",
    "    x = Embedding(max_line_length+2, HIDDEN_DIM)(encoder_inputs)\n",
    "    x, state_h, state_c = LSTM(HIDDEN_DIM, return_state=True)(x)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    x = Embedding(max_line_length+2, HIDDEN_DIM)(decoder_inputs)\n",
    "    x = LSTM(HIDDEN_DIM, return_sequences=True)(x, initial_state=encoder_states)\n",
    "    \n",
    "    \n",
    "    # dense_layer = Dense(VOCAB_SIZE, activation='softmax')\n",
    "    outputs = TimeDistributed(Dense(max_line_length+2, activation='softmax'))(x)\n",
    "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\georg\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    6000        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 300)    6000        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 300), (None, 721200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, None, 300)    721200      embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 20)     6020        lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,460,420\n",
      "Trainable params: 1,460,420\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = seq2seq_model_builder(HIDDEN_DIM=300)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected time_distributed_1 to have 3 dimensions, but got array with shape (31679, 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    133\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    136\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected time_distributed_1 to have 3 dimensions, but got array with shape (31679, 20)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 128\n",
    "epochs = 40\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit([en_train, de_train],out_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=([en_val, de_val],out_val))\n",
    "score = model.evaluate([en_test, de_test],out_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1.])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['eos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(HIDDEN_DIM,))\n",
    "decoder_state_input_c = Input(shape=(HIDDEN_DIM,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for the frequency of the vocabulary\n",
    "vocab = {}\n",
    "for question in short_questions:\n",
    "    for word in question.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "            \n",
    "for answer in short_answers:\n",
    "    for word in answer.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "            \n",
    "# Remove rare words from the vocabulary.\n",
    "# We will aim to replace fewer than 5% of words with <UNK>\n",
    "# You will see this ratio soon.\n",
    "threshold = 10\n",
    "count = 0\n",
    "for k,v in vocab.items():\n",
    "    if v >= threshold:\n",
    "        count += 1\n",
    "\n",
    "print(\"Size of total vocab:\", len(vocab))\n",
    "print(\"Size of vocab we will use:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we want to use a different vocabulary sizes for the source and target text, \n",
    "# we can set different threshold values.\n",
    "# Nonetheless, we will create dictionaries to provide a unique integer for each word.\n",
    "questions_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        questions_vocab_to_int[word] = word_num\n",
    "        word_num += 1\n",
    "        \n",
    "answers_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        answers_vocab_to_int[word] = word_num\n",
    "        word_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the unique tokens to the vocabulary dictionaries.\n",
    "codes = ['<PAD>','<EOS>','<UNK>','<GO>']\n",
    "\n",
    "for code in codes:\n",
    "    questions_vocab_to_int[code] = len(questions_vocab_to_int)+1\n",
    "    \n",
    "for code in codes:\n",
    "    answers_vocab_to_int[code] = len(answers_vocab_to_int)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries to map the unique integers to their respective words.\n",
    "# i.e. an inverse dictionary for vocab_to_int.\n",
    "questions_int_to_vocab = {v_i: v for v, v_i in questions_vocab_to_int.items()}\n",
    "answers_int_to_vocab = {v_i: v for v, v_i in answers_vocab_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the length of the dictionaries.\n",
    "print(len(questions_vocab_to_int))\n",
    "print(len(questions_int_to_vocab))\n",
    "print(len(answers_vocab_to_int))\n",
    "print(len(answers_int_to_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the end of sentence token to the end of every answer.\n",
    "for i in range(len(short_answers)):\n",
    "    short_answers[i] += ' <EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text to integers. \n",
    "# Replace any words that are not in the respective vocabulary with <UNK> \n",
    "questions_int = []\n",
    "for question in short_questions:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in questions_vocab_to_int:\n",
    "            ints.append(questions_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(questions_vocab_to_int[word])\n",
    "    questions_int.append(ints)\n",
    "    \n",
    "answers_int = []\n",
    "for answer in short_answers:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in answers_vocab_to_int:\n",
    "            ints.append(answers_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(answers_vocab_to_int[word])\n",
    "    answers_int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the lengths\n",
    "print(len(questions_int))\n",
    "print(len(answers_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate what percentage of all words have been replaced with <UNK>\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "for question in questions_int:\n",
    "    for word in question:\n",
    "        if word == questions_vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "        word_count += 1\n",
    "    \n",
    "for answer in answers_int:\n",
    "    for word in answer:\n",
    "        if word == answers_vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "        word_count += 1\n",
    "    \n",
    "unk_ratio = round(unk_count/word_count,4)*100\n",
    "    \n",
    "print(\"Total number of words:\", word_count)\n",
    "print(\"Number of times <UNK> is used:\", unk_count)\n",
    "print(\"Percent of words that are <UNK>: {}%\".format(round(unk_ratio,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort questions and answers by the length of questions.\n",
    "# This will reduce the amount of padding during training\n",
    "# Which should speed up training and help to reduce the loss\n",
    "\n",
    "sorted_questions = []\n",
    "sorted_answers = []\n",
    "\n",
    "for length in range(1, max_line_length+1):\n",
    "    for i in enumerate(questions_int):\n",
    "        if len(i[1]) == length:\n",
    "            sorted_questions.append(questions_int[i[0]])\n",
    "            sorted_answers.append(answers_int[i[0]])\n",
    "\n",
    "print(len(sorted_questions))\n",
    "print(len(sorted_answers))\n",
    "print()\n",
    "for i in range(3):\n",
    "    print(sorted_questions[i])\n",
    "    print(sorted_answers[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    '''Create palceholders for inputs to the model'''\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "    '''Create the encoding layer'''\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    _, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = enc_cell,\n",
    "                                                   cell_bw = enc_cell,\n",
    "                                                   sequence_length = sequence_length,\n",
    "                                                   inputs = rnn_inputs, \n",
    "                                                   dtype=tf.float32)\n",
    "    return enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope,\n",
    "                         output_fn, keep_prob, batch_size):\n",
    "    '''Decode the training data'''\n",
    "    \n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = \\\n",
    "            tf.contrib.seq2seq.prepare_attention(attention_states,\n",
    "                                                 attention_option=\"bahdanau\",\n",
    "                                                 num_units=dec_cell.output_size)\n",
    "    \n",
    "    train_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
    "                                                                     att_keys,\n",
    "                                                                     att_vals,\n",
    "                                                                     att_score_fn,\n",
    "                                                                     att_construct_fn,\n",
    "                                                                     name = \"attn_dec_train\")\n",
    "    train_pred, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, \n",
    "                                                              train_decoder_fn, \n",
    "                                                              dec_embed_input, \n",
    "                                                              sequence_length, \n",
    "                                                              scope=decoding_scope)\n",
    "    train_pred_drop = tf.nn.dropout(train_pred, keep_prob)\n",
    "    return output_fn(train_pred_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id,\n",
    "                         maximum_length, vocab_size, decoding_scope, output_fn, keep_prob, batch_size):\n",
    "    '''Decode the prediction data'''\n",
    "    \n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = \\\n",
    "            tf.contrib.seq2seq.prepare_attention(attention_states,\n",
    "                                                 attention_option=\"bahdanau\",\n",
    "                                                 num_units=dec_cell.output_size)\n",
    "    \n",
    "    infer_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_inference(output_fn, \n",
    "                                                                         encoder_state[0], \n",
    "                                                                         att_keys, \n",
    "                                                                         att_vals, \n",
    "                                                                         att_score_fn, \n",
    "                                                                         att_construct_fn, \n",
    "                                                                         dec_embeddings,\n",
    "                                                                         start_of_sequence_id, \n",
    "                                                                         end_of_sequence_id, \n",
    "                                                                         maximum_length, \n",
    "                                                                         vocab_size, \n",
    "                                                                         name = \"attn_dec_inf\")\n",
    "    infer_logits, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, \n",
    "                                                                infer_decoder_fn, \n",
    "                                                                scope=decoding_scope)\n",
    "    \n",
    "    return infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size,\n",
    "                   num_layers, vocab_to_int, keep_prob, batch_size):\n",
    "    '''Create the decoding cell and input the parameters for the training and inference decoding layers'''\n",
    "    \n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "        dec_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "        \n",
    "        weights = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        output_fn = lambda x: tf.contrib.layers.fully_connected(x, \n",
    "                                                                vocab_size, \n",
    "                                                                None, \n",
    "                                                                scope=decoding_scope,\n",
    "                                                                weights_initializer = weights,\n",
    "                                                                biases_initializer = biases)\n",
    "\n",
    "        train_logits = decoding_layer_train(encoder_state, \n",
    "                                            dec_cell, \n",
    "                                            dec_embed_input, \n",
    "                                            sequence_length, \n",
    "                                            decoding_scope, \n",
    "                                            output_fn, \n",
    "                                            keep_prob, \n",
    "                                            batch_size)\n",
    "        decoding_scope.reuse_variables()\n",
    "        infer_logits = decoding_layer_infer(encoder_state, \n",
    "                                            dec_cell, \n",
    "                                            dec_embeddings, \n",
    "                                            vocab_to_int['<GO>'],\n",
    "                                            vocab_to_int['<EOS>'], \n",
    "                                            sequence_length - 1, \n",
    "                                            vocab_size,\n",
    "                                            decoding_scope, \n",
    "                                            output_fn, keep_prob, \n",
    "                                            batch_size)\n",
    "\n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size, sequence_length, answers_vocab_size, \n",
    "                  questions_vocab_size, enc_embedding_size, dec_embedding_size, rnn_size, num_layers, \n",
    "                  questions_vocab_to_int):\n",
    "    \n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, \n",
    "                                                       answers_vocab_size+1, \n",
    "                                                       enc_embedding_size,\n",
    "                                                       initializer = tf.random_uniform_initializer(0,1))\n",
    "    enc_state = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "\n",
    "    dec_input = process_encoding_input(target_data, questions_vocab_to_int, batch_size)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([questions_vocab_size+1, dec_embedding_size], 0, 1))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    train_logits, infer_logits = decoding_layer(dec_embed_input, \n",
    "                                                dec_embeddings, \n",
    "                                                enc_state, \n",
    "                                                questions_vocab_size, \n",
    "                                                sequence_length, \n",
    "                                                rnn_size, \n",
    "                                                num_layers, \n",
    "                                                questions_vocab_to_int, \n",
    "                                                keep_prob, \n",
    "                                                batch_size)\n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "rnn_size = 512\n",
    "num_layers = 2\n",
    "encoding_embedding_size = 512\n",
    "decoding_embedding_size = 512\n",
    "learning_rate = 0.005\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.0001\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph to ensure that it is ready for training\n",
    "tf.reset_default_graph()\n",
    "# Start the session\n",
    "sess = tf.InteractiveSession()\n",
    "    \n",
    "# Load the model inputs    \n",
    "input_data, targets, lr, keep_prob = model_inputs()\n",
    "# Sequence length will be the max line length for each batch\n",
    "sequence_length = tf.placeholder_with_default(max_line_length, None, name='sequence_length')\n",
    "# Find the shape of the input data for sequence_loss\n",
    "input_shape = tf.shape(input_data)\n",
    "\n",
    "# Create the training and inference logits\n",
    "train_logits, inference_logits = seq2seq_model(\n",
    "    tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, sequence_length, len(answers_vocab_to_int), \n",
    "    len(questions_vocab_to_int), encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers, \n",
    "    questions_vocab_to_int)\n",
    "\n",
    "# Create a tensor for the inference logits, needed if loading a checkpoint version of the model\n",
    "tf.identity(inference_logits, 'logits')\n",
    "\n",
    "with tf.name_scope(\"optimization\"):\n",
    "    # Loss function\n",
    "    cost = tf.contrib.seq2seq.sequence_loss(\n",
    "        train_logits,\n",
    "        targets,\n",
    "        tf.ones([input_shape[0], sequence_length]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = random.sample(range(len(conversation)), 50)\n",
    "sample_context_list = []\n",
    "sample_response_list = []\n",
    "\n",
    "for index in indices:\n",
    "    \n",
    "    response = clean_text(conversation[index][-1])\n",
    "        \n",
    "    context = clean_text(conversation[index][0]) + \"\\n\"\n",
    "    for i in range(1, len(conversation[index]) - 1):\n",
    "        \n",
    "        if i % 2 == 0:\n",
    "            prefix = \"FS: \"\n",
    "        else:\n",
    "            prefix = \"SS: \"\n",
    "            \n",
    "        context += clean_text(conversation[index][i]) + \"\\n\"\n",
    "        \n",
    "    sample_context_list.append(context)\n",
    "    sample_response_list.append(response)\n",
    "\n",
    "#with open(\"cornell_movie_dialogue_sample.csv\", \"w\") as handle:\n",
    "#    for c, r in zip(sample_context_list, sample_response_list):\n",
    "#        handle.write('\"' + c + '\"' + \"#\" + r + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_response_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
